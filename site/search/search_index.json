{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ETLX","text":"<p>Configuration-driven ETL for Python</p> <p>Build data pipelines in YAML or Python with support for 20+ compute backends.</p> <p>Get Started View on GitHub</p>"},{"location":"#why-etlx","title":"Why ETLX?","text":"<p>ETLX provides a unified API for data transformation across multiple compute engines. Write your pipeline once, run it anywhere - from local DuckDB to distributed Spark to cloud warehouses like Snowflake and BigQuery.</p> <ul> <li> <p> Configuration-Driven</p> <p>Define pipelines in YAML with variable substitution. No code required for common ETL patterns.</p> </li> <li> <p> Multi-Backend</p> <p>Run the same pipeline on DuckDB, Polars, Spark, Snowflake, BigQuery, and more via Ibis.</p> </li> <li> <p> Quality Checks</p> <p>Built-in data quality validation with not_null, unique, row_count, and custom expressions.</p> </li> <li> <p> CLI &amp; API</p> <p>Use the <code>etlx</code> CLI for quick runs or the Python API for programmatic control.</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"YAMLPython pipeline.yml<pre><code>name: sales_summary\nengine: duckdb\n\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n  - op: aggregate\n    group_by: [region]\n    aggs:\n      total_sales: sum(amount)\n      order_count: count(*)\n\nchecks:\n  - type: not_null\n    columns: [region, total_sales]\n\nsink:\n  type: file\n  path: output/sales_by_region.parquet\n  format: parquet\n</code></pre> <pre><code>etlx run pipeline.yml\n</code></pre> <pre><code>from etlx import Pipeline\nfrom etlx.config.models import FileSource, FileSink\nfrom etlx.config.transforms import FilterTransform, AggregateTransform\n\npipeline = (\n    Pipeline(\"sales_summary\", engine=\"duckdb\")\n    .source(FileSource(path=\"data/sales.csv\", format=\"csv\"))\n    .transform(FilterTransform(predicate=\"amount &gt; 0\"))\n    .transform(AggregateTransform(\n        group_by=[\"region\"],\n        aggs={\"total_sales\": \"sum(amount)\", \"order_count\": \"count(*)\"}\n    ))\n    .sink(FileSink(path=\"output/sales_by_region.parquet\"))\n)\n\nresult = pipeline.run()\nprint(f\"Processed {result.rows_processed} rows\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation (includes DuckDB + Polars)\npip install etlx\n\n# With cloud storage support\npip install etlx[aws]      # S3\npip install etlx[gcp]      # GCS + BigQuery\npip install etlx[azure]    # Azure ADLS\n\n# With additional backends\npip install etlx[spark]\npip install etlx[snowflake]\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#12-transform-operations","title":"12 Transform Operations","text":"Transform Description <code>select</code> Choose and reorder columns <code>rename</code> Rename columns <code>filter</code> Filter rows with SQL predicates <code>derive_column</code> Create computed columns <code>cast</code> Convert column types <code>fill_null</code> Replace null values <code>dedup</code> Remove duplicates <code>sort</code> Order rows <code>join</code> Join datasets <code>aggregate</code> Group and aggregate <code>union</code> Combine datasets vertically <code>limit</code> Limit row count"},{"location":"#5-quality-checks","title":"5 Quality Checks","text":"Check Description <code>not_null</code> Ensure columns have no null values <code>unique</code> Verify uniqueness constraints <code>row_count</code> Validate row count bounds <code>accepted_values</code> Check values against whitelist <code>expression</code> Custom SQL predicate validation"},{"location":"#supported-backends","title":"Supported Backends","text":"Backend Type Included DuckDB Local Yes Polars Local Yes DataFusion Local Optional Spark Distributed Optional Snowflake Cloud DW Optional BigQuery Cloud DW Optional PostgreSQL Database Optional MySQL Database Optional"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Getting Started</p> <p>Install ETLX and run your first pipeline in 5 minutes.</p> <p> Quick Start</p> </li> <li> <p> User Guide</p> <p>Learn about transforms, quality checks, and configuration.</p> <p> User Guide</p> </li> <li> <p> API Reference</p> <p>Explore the Python API for programmatic pipeline building.</p> <p> Python API</p> </li> <li> <p> Examples</p> <p>See complete examples for common ETL patterns.</p> <p> Examples</p> </li> </ul>"},{"location":"api/","title":"Python API Reference","text":"<p>ETLX provides a Python API for programmatic pipeline creation and execution. Use the API when you need:</p> <ul> <li>Dynamic pipeline generation</li> <li>Integration with Python applications</li> <li>Programmatic control over execution</li> <li>Custom preprocessing or postprocessing</li> </ul>"},{"location":"api/#quick-start","title":"Quick Start","text":"<pre><code>from etlx import Pipeline\n\n# Load and run a pipeline\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run()\n\nprint(f\"Processed {result.rows_processed} rows\")\n</code></pre>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#pipeline","title":"Pipeline","text":"<p>The main entry point for working with ETLX pipelines.</p> <pre><code>from etlx import Pipeline\n\n# From YAML file\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\n\n# From configuration dict\npipeline = Pipeline.from_config({\n    \"name\": \"my_pipeline\",\n    \"engine\": \"duckdb\",\n    \"source\": {\"type\": \"file\", \"path\": \"data.csv\", \"format\": \"csv\"},\n    \"sink\": {\"type\": \"file\", \"path\": \"output.parquet\", \"format\": \"parquet\"}\n})\n</code></pre> <p>Learn more about Pipeline \u2192</p>"},{"location":"api/#etlxengine","title":"ETLXEngine","text":"<p>Direct access to the execution engine for advanced use cases.</p> <pre><code>from etlx import ETLXEngine\n\nengine = ETLXEngine(backend=\"duckdb\")\nresult = engine.execute(config)\n</code></pre> <p>Learn more about ETLXEngine \u2192</p>"},{"location":"api/#configuration-models","title":"Configuration Models","text":"<p>Pydantic models for type-safe pipeline configuration.</p> <pre><code>from etlx.config import PipelineConfig, FileSource, FileSink\n\nconfig = PipelineConfig(\n    name=\"typed_pipeline\",\n    engine=\"duckdb\",\n    source=FileSource(type=\"file\", path=\"input.csv\", format=\"csv\"),\n    sink=FileSink(type=\"file\", path=\"output.parquet\", format=\"parquet\")\n)\n</code></pre> <p>Learn more about Configuration \u2192</p>"},{"location":"api/#quality-checks","title":"Quality Checks","text":"<p>Programmatic data quality validation.</p> <pre><code>from etlx.quality import NotNullCheck, UniqueCheck\n\nchecks = [\n    NotNullCheck(columns=[\"id\", \"name\"]),\n    UniqueCheck(columns=[\"id\"])\n]\n</code></pre> <p>Learn more about Quality Checks \u2192</p>"},{"location":"api/#common-patterns","title":"Common Patterns","text":""},{"location":"api/#run-pipeline-with-variables","title":"Run Pipeline with Variables","text":"<pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run(variables={\n    \"DATE\": \"2025-01-15\",\n    \"REGION\": \"north\"\n})\n</code></pre>"},{"location":"api/#validate-before-running","title":"Validate Before Running","text":"<pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\n\n# Validate configuration\nerrors = pipeline.validate()\nif errors:\n    for error in errors:\n        print(f\"Error: {error}\")\nelse:\n    result = pipeline.run()\n</code></pre>"},{"location":"api/#dry-run","title":"Dry Run","text":"<pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run(dry_run=True)\n\nprint(f\"Would process {result.rows_processed} rows\")\n</code></pre>"},{"location":"api/#access-results","title":"Access Results","text":"<pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run()\n\nprint(f\"Pipeline: {result.pipeline_name}\")\nprint(f\"Status: {result.status}\")\nprint(f\"Duration: {result.duration_ms}ms\")\nprint(f\"Rows processed: {result.rows_processed}\")\nprint(f\"Rows written: {result.rows_written}\")\nprint(f\"Checks passed: {result.checks_passed}\")\nprint(f\"Checks failed: {result.checks_failed}\")\n</code></pre>"},{"location":"api/#get-result-dataframe","title":"Get Result DataFrame","text":"<pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run()\n\n# Access the result as a DataFrame\ndf = result.to_dataframe()\nprint(df.head())\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<pre><code>from etlx import Pipeline\nfrom etlx.exceptions import (\n    ETLXError,\n    ConfigurationError,\n    ExecutionError,\n    QualityCheckError\n)\n\ntry:\n    pipeline = Pipeline.from_yaml(\"pipeline.yml\")\n    result = pipeline.run()\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\nexcept QualityCheckError as e:\n    print(f\"Quality check failed: {e}\")\n    print(f\"Failed checks: {e.failed_checks}\")\nexcept ExecutionError as e:\n    print(f\"Execution error: {e}\")\nexcept ETLXError as e:\n    print(f\"ETLX error: {e}\")\n</code></pre>"},{"location":"api/#integration-examples","title":"Integration Examples","text":""},{"location":"api/#with-airflow","title":"With Airflow","text":"<pre><code>from airflow.decorators import task\n\n@task\ndef run_etlx_pipeline(config_path: str, **kwargs):\n    from etlx import Pipeline\n\n    pipeline = Pipeline.from_yaml(config_path)\n    result = pipeline.run(variables=kwargs)\n\n    return {\n        \"status\": result.status,\n        \"rows\": result.rows_written\n    }\n</code></pre>"},{"location":"api/#with-fastapi","title":"With FastAPI","text":"<pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom etlx import Pipeline\n\napp = FastAPI()\n\n@app.post(\"/pipelines/{name}/run\")\nasync def run_pipeline(name: str, background_tasks: BackgroundTasks):\n    def execute():\n        pipeline = Pipeline.from_yaml(f\"pipelines/{name}.yml\")\n        return pipeline.run()\n\n    background_tasks.add_task(execute)\n    return {\"status\": \"started\"}\n</code></pre>"},{"location":"api/#with-prefect","title":"With Prefect","text":"<pre><code>from prefect import flow, task\nfrom etlx import Pipeline\n\n@task\ndef run_etl(config_path: str):\n    pipeline = Pipeline.from_yaml(config_path)\n    return pipeline.run()\n\n@flow\ndef etl_flow():\n    result = run_etl(\"pipeline.yml\")\n    print(f\"Processed {result.rows_written} rows\")\n</code></pre>"},{"location":"api/#api-reference","title":"API Reference","text":"Module Description Pipeline Main pipeline class ETLXEngine Execution engine Config Configuration models Quality Quality check classes"},{"location":"api/#related","title":"Related","text":"<ul> <li>CLI Reference - Command-line interface</li> <li>Pipeline YAML - YAML configuration</li> <li>Airflow Integration - Airflow integration</li> </ul>"},{"location":"api/config/","title":"Configuration Models","text":"<p>ETLX uses Pydantic models for type-safe pipeline configuration. These models provide validation, IDE autocomplete, and documentation.</p>"},{"location":"api/config/#import","title":"Import","text":"<pre><code>from etlx.config import (\n    PipelineConfig,\n    FileSource,\n    DatabaseSource,\n    FileSink,\n    DatabaseSink,\n    # Transforms\n    SelectTransform,\n    FilterTransform,\n    DeriveColumnTransform,\n    AggregateTransform,\n    JoinTransform,\n    # ... and more\n)\n</code></pre>"},{"location":"api/config/#pipelineconfig","title":"PipelineConfig","text":"<p>The root configuration model for a complete pipeline.</p> <pre><code>from etlx.config import PipelineConfig\n\nclass PipelineConfig(BaseModel):\n    name: str\n    description: str | None = None\n    engine: str = \"duckdb\"\n    source: SourceConfig\n    transforms: list[TransformConfig] = []\n    checks: list[CheckConfig] = []\n    sink: SinkConfig\n</code></pre>"},{"location":"api/config/#example","title":"Example","text":"<pre><code>from etlx.config import (\n    PipelineConfig,\n    FileSource,\n    FileSink,\n    FilterTransform,\n    SelectTransform\n)\n\nconfig = PipelineConfig(\n    name=\"sales_etl\",\n    description=\"Process daily sales data\",\n    engine=\"duckdb\",\n    source=FileSource(\n        type=\"file\",\n        path=\"data/sales.csv\",\n        format=\"csv\"\n    ),\n    transforms=[\n        FilterTransform(op=\"filter\", predicate=\"amount &gt; 0\"),\n        SelectTransform(op=\"select\", columns=[\"id\", \"name\", \"amount\"])\n    ],\n    sink=FileSink(\n        type=\"file\",\n        path=\"output/results.parquet\",\n        format=\"parquet\"\n    )\n)\n</code></pre>"},{"location":"api/config/#source-models","title":"Source Models","text":""},{"location":"api/config/#filesource","title":"FileSource","text":"<p>Read from local or cloud files.</p> <pre><code>class FileSource(BaseModel):\n    type: Literal[\"file\"]\n    path: str\n    format: Literal[\"csv\", \"parquet\", \"json\", \"excel\"]\n    options: dict | None = None\n</code></pre> <p>Example:</p> <pre><code>source = FileSource(\n    type=\"file\",\n    path=\"data/*.parquet\",\n    format=\"parquet\"\n)\n\n# With options\nsource = FileSource(\n    type=\"file\",\n    path=\"data/input.csv\",\n    format=\"csv\",\n    options={\n        \"delimiter\": \";\",\n        \"encoding\": \"utf-8\",\n        \"has_header\": True\n    }\n)\n</code></pre>"},{"location":"api/config/#databasesource","title":"DatabaseSource","text":"<p>Read from databases.</p> <pre><code>class DatabaseSource(BaseModel):\n    type: Literal[\"database\"]\n    connection: str\n    table: str | None = None\n    query: str | None = None\n</code></pre> <p>Example:</p> <pre><code># From table\nsource = DatabaseSource(\n    type=\"database\",\n    connection=\"postgres\",\n    table=\"public.orders\"\n)\n\n# From query\nsource = DatabaseSource(\n    type=\"database\",\n    connection=\"postgres\",\n    query=\"\"\"\n        SELECT * FROM orders\n        WHERE created_at &gt;= CURRENT_DATE - INTERVAL '7 days'\n    \"\"\"\n)\n</code></pre>"},{"location":"api/config/#sink-models","title":"Sink Models","text":""},{"location":"api/config/#filesink","title":"FileSink","text":"<p>Write to files.</p> <pre><code>class FileSink(BaseModel):\n    type: Literal[\"file\"]\n    path: str\n    format: Literal[\"csv\", \"parquet\", \"json\"]\n    mode: Literal[\"replace\", \"append\"] = \"replace\"\n    options: dict | None = None\n</code></pre> <p>Example:</p> <pre><code>sink = FileSink(\n    type=\"file\",\n    path=\"output/results.parquet\",\n    format=\"parquet\",\n    mode=\"replace\"\n)\n</code></pre>"},{"location":"api/config/#databasesink","title":"DatabaseSink","text":"<p>Write to databases.</p> <pre><code>class DatabaseSink(BaseModel):\n    type: Literal[\"database\"]\n    connection: str\n    table: str\n    mode: Literal[\"replace\", \"append\", \"upsert\"] = \"replace\"\n    upsert_keys: list[str] | None = None\n</code></pre> <p>Example:</p> <pre><code># Replace mode\nsink = DatabaseSink(\n    type=\"database\",\n    connection=\"postgres\",\n    table=\"analytics.summary\",\n    mode=\"replace\"\n)\n\n# Upsert mode\nsink = DatabaseSink(\n    type=\"database\",\n    connection=\"postgres\",\n    table=\"analytics.summary\",\n    mode=\"upsert\",\n    upsert_keys=[\"id\"]\n)\n</code></pre>"},{"location":"api/config/#transform-models","title":"Transform Models","text":"<p>All transforms use a discriminated union based on the <code>op</code> field.</p>"},{"location":"api/config/#selecttransform","title":"SelectTransform","text":"<pre><code>class SelectTransform(BaseModel):\n    op: Literal[\"select\"]\n    columns: list[str]\n</code></pre>"},{"location":"api/config/#renametransform","title":"RenameTransform","text":"<pre><code>class RenameTransform(BaseModel):\n    op: Literal[\"rename\"]\n    columns: dict[str, str]  # old_name: new_name\n</code></pre>"},{"location":"api/config/#filtertransform","title":"FilterTransform","text":"<pre><code>class FilterTransform(BaseModel):\n    op: Literal[\"filter\"]\n    predicate: str\n</code></pre>"},{"location":"api/config/#derivecolumntransform","title":"DeriveColumnTransform","text":"<pre><code>class DeriveColumnTransform(BaseModel):\n    op: Literal[\"derive_column\"]\n    name: str\n    expr: str\n</code></pre>"},{"location":"api/config/#casttransform","title":"CastTransform","text":"<pre><code>class CastTransform(BaseModel):\n    op: Literal[\"cast\"]\n    columns: dict[str, str]  # column: type\n</code></pre>"},{"location":"api/config/#fillnulltransform","title":"FillNullTransform","text":"<pre><code>class FillNullTransform(BaseModel):\n    op: Literal[\"fill_null\"]\n    columns: dict[str, Any]  # column: value\n</code></pre>"},{"location":"api/config/#deduptransform","title":"DedupTransform","text":"<pre><code>class DedupTransform(BaseModel):\n    op: Literal[\"dedup\"]\n    columns: list[str] | None = None  # None = all columns\n    keep: Literal[\"first\", \"last\"] = \"first\"\n</code></pre>"},{"location":"api/config/#sorttransform","title":"SortTransform","text":"<pre><code>class SortColumn(BaseModel):\n    column: str\n    order: Literal[\"asc\", \"desc\"] = \"asc\"\n\nclass SortTransform(BaseModel):\n    op: Literal[\"sort\"]\n    by: list[SortColumn]\n</code></pre>"},{"location":"api/config/#jointransform","title":"JoinTransform","text":"<pre><code>class JoinTransform(BaseModel):\n    op: Literal[\"join\"]\n    right: SourceConfig\n    on: list[str]\n    how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\"\n</code></pre>"},{"location":"api/config/#aggregatetransform","title":"AggregateTransform","text":"<pre><code>class AggregateTransform(BaseModel):\n    op: Literal[\"aggregate\"]\n    group_by: list[str]\n    aggregations: dict[str, str]  # output_name: expression\n</code></pre>"},{"location":"api/config/#uniontransform","title":"UnionTransform","text":"<pre><code>class UnionTransform(BaseModel):\n    op: Literal[\"union\"]\n    sources: list[SourceConfig]\n</code></pre>"},{"location":"api/config/#limittransform","title":"LimitTransform","text":"<pre><code>class LimitTransform(BaseModel):\n    op: Literal[\"limit\"]\n    n: int\n    offset: int = 0\n</code></pre>"},{"location":"api/config/#check-models","title":"Check Models","text":""},{"location":"api/config/#notnullcheck","title":"NotNullCheck","text":"<pre><code>class NotNullCheck(BaseModel):\n    check: Literal[\"not_null\"]\n    columns: list[str]\n</code></pre>"},{"location":"api/config/#uniquecheck","title":"UniqueCheck","text":"<pre><code>class UniqueCheck(BaseModel):\n    check: Literal[\"unique\"]\n    columns: list[str]\n</code></pre>"},{"location":"api/config/#rowcountcheck","title":"RowCountCheck","text":"<pre><code>class RowCountCheck(BaseModel):\n    check: Literal[\"row_count\"]\n    min: int | None = None\n    max: int | None = None\n    exact: int | None = None\n</code></pre>"},{"location":"api/config/#acceptedvaluescheck","title":"AcceptedValuesCheck","text":"<pre><code>class AcceptedValuesCheck(BaseModel):\n    check: Literal[\"accepted_values\"]\n    column: str\n    values: list[Any]\n</code></pre>"},{"location":"api/config/#expressioncheck","title":"ExpressionCheck","text":"<pre><code>class ExpressionCheck(BaseModel):\n    check: Literal[\"expression\"]\n    expr: str\n    threshold: float = 1.0  # Fraction that must pass\n</code></pre>"},{"location":"api/config/#complete-example","title":"Complete Example","text":"<pre><code>from etlx import Pipeline\nfrom etlx.config import (\n    PipelineConfig,\n    FileSource,\n    FileSink,\n    FilterTransform,\n    DeriveColumnTransform,\n    AggregateTransform,\n    SortTransform,\n    SortColumn,\n    NotNullCheck,\n    RowCountCheck\n)\n\nconfig = PipelineConfig(\n    name=\"sales_analytics\",\n    description=\"Compute sales metrics by category\",\n    engine=\"duckdb\",\n\n    source=FileSource(\n        type=\"file\",\n        path=\"data/sales.parquet\",\n        format=\"parquet\"\n    ),\n\n    transforms=[\n        FilterTransform(\n            op=\"filter\",\n            predicate=\"status = 'completed' AND amount &gt; 0\"\n        ),\n        DeriveColumnTransform(\n            op=\"derive_column\",\n            name=\"net_amount\",\n            expr=\"amount - discount\"\n        ),\n        AggregateTransform(\n            op=\"aggregate\",\n            group_by=[\"category\"],\n            aggregations={\n                \"total_revenue\": \"sum(net_amount)\",\n                \"order_count\": \"count(*)\",\n                \"avg_order\": \"avg(net_amount)\"\n            }\n        ),\n        SortTransform(\n            op=\"sort\",\n            by=[SortColumn(column=\"total_revenue\", order=\"desc\")]\n        )\n    ],\n\n    checks=[\n        NotNullCheck(check=\"not_null\", columns=[\"category\", \"total_revenue\"]),\n        RowCountCheck(check=\"row_count\", min=1)\n    ],\n\n    sink=FileSink(\n        type=\"file\",\n        path=\"output/category_metrics.parquet\",\n        format=\"parquet\"\n    )\n)\n\n# Create and run pipeline\npipeline = Pipeline.from_model(config)\nresult = pipeline.run()\n</code></pre>"},{"location":"api/config/#validation","title":"Validation","text":"<p>Pydantic models provide automatic validation:</p> <pre><code>from pydantic import ValidationError\nfrom etlx.config import PipelineConfig\n\ntry:\n    config = PipelineConfig(\n        name=\"test\",\n        source={\"type\": \"invalid\"},  # Invalid!\n        sink={\"type\": \"file\", \"path\": \"out.csv\", \"format\": \"csv\"}\n    )\nexcept ValidationError as e:\n    print(e)\n    # source -&gt; type\n    #   Input should be 'file' or 'database'\n</code></pre>"},{"location":"api/config/#serialization","title":"Serialization","text":""},{"location":"api/config/#to-dictionary","title":"To Dictionary","text":"<pre><code>config_dict = config.model_dump()\n</code></pre>"},{"location":"api/config/#to-json","title":"To JSON","text":"<pre><code>config_json = config.model_dump_json(indent=2)\n</code></pre>"},{"location":"api/config/#to-yaml","title":"To YAML","text":"<pre><code>import yaml\n\nconfig_dict = config.model_dump()\nconfig_yaml = yaml.dump(config_dict)\n</code></pre>"},{"location":"api/config/#related","title":"Related","text":"<ul> <li>Pipeline YAML - YAML format reference</li> <li>Transforms - Transform documentation</li> <li>Quality Checks - Check documentation</li> </ul>"},{"location":"api/engine/","title":"ETLXEngine Class","text":"<p>The <code>ETLXEngine</code> class provides low-level access to the ETLX execution engine. Use this for advanced use cases where you need direct control over execution.</p> <p>Use Pipeline Instead</p> <p>For most use cases, the Pipeline class is recommended. Use <code>ETLXEngine</code> only when you need low-level control.</p>"},{"location":"api/engine/#import","title":"Import","text":"<pre><code>from etlx import ETLXEngine\n</code></pre>"},{"location":"api/engine/#constructor","title":"Constructor","text":"<pre><code>ETLXEngine(\n    backend: str = \"duckdb\",\n    **options\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>backend</code> <code>str</code> <code>\"duckdb\"</code> Backend engine name <code>**options</code> Backend-specific options <p>Supported Backends:</p> <ul> <li><code>duckdb</code> - DuckDB (default)</li> <li><code>polars</code> - Polars</li> <li><code>pandas</code> - Pandas</li> <li><code>spark</code> - Apache Spark</li> <li><code>datafusion</code> - Apache DataFusion</li> <li><code>snowflake</code> - Snowflake</li> <li><code>bigquery</code> - Google BigQuery</li> <li><code>postgres</code> - PostgreSQL</li> <li><code>mysql</code> - MySQL</li> <li><code>clickhouse</code> - ClickHouse</li> </ul> <p>Example:</p> <pre><code># Default (DuckDB)\nengine = ETLXEngine()\n\n# Specific backend\nengine = ETLXEngine(backend=\"polars\")\n\n# With backend options\nengine = ETLXEngine(\n    backend=\"spark\",\n    master=\"local[*]\",\n    executor_memory=\"4g\"\n)\n</code></pre>"},{"location":"api/engine/#methods","title":"Methods","text":""},{"location":"api/engine/#execute","title":"<code>execute</code>","text":"<p>Execute a pipeline configuration.</p> <pre><code>ETLXEngine.execute(\n    config: PipelineConfig | dict,\n    variables: dict[str, str] | None = None,\n    dry_run: bool = False\n) -&gt; ExecutionResult\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>PipelineConfig \\| dict</code> Pipeline configuration <code>variables</code> <code>dict[str, str] \\| None</code> Variable substitutions <code>dry_run</code> <code>bool</code> Execute without writing <p>Returns: <code>ExecutionResult</code></p> <p>Example:</p> <pre><code>from etlx import ETLXEngine\nfrom etlx.config import PipelineConfig\n\nengine = ETLXEngine(backend=\"duckdb\")\n\nconfig = {\n    \"name\": \"test\",\n    \"source\": {\"type\": \"file\", \"path\": \"data.csv\", \"format\": \"csv\"},\n    \"sink\": {\"type\": \"file\", \"path\": \"out.parquet\", \"format\": \"parquet\"}\n}\n\nresult = engine.execute(config)\n</code></pre>"},{"location":"api/engine/#read_source","title":"<code>read_source</code>","text":"<p>Read data from a source configuration.</p> <pre><code>ETLXEngine.read_source(\n    source_config: SourceConfig | dict\n) -&gt; Table\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>source_config</code> <code>SourceConfig \\| dict</code> Source configuration <p>Returns: Ibis table expression</p> <p>Example:</p> <pre><code>engine = ETLXEngine()\n\ntable = engine.read_source({\n    \"type\": \"file\",\n    \"path\": \"data/sales.csv\",\n    \"format\": \"csv\"\n})\n\n# Now you can use Ibis operations\nfiltered = table.filter(table.amount &gt; 100)\nresult = filtered.execute()\n</code></pre>"},{"location":"api/engine/#write_sink","title":"<code>write_sink</code>","text":"<p>Write data to a sink configuration.</p> <pre><code>ETLXEngine.write_sink(\n    table: Table,\n    sink_config: SinkConfig | dict,\n    mode: str = \"replace\"\n) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>table</code> <code>Table</code> Ibis table expression <code>sink_config</code> <code>SinkConfig \\| dict</code> Sink configuration <code>mode</code> <code>str</code> Write mode: \"replace\", \"append\" <p>Example:</p> <pre><code>engine = ETLXEngine()\n\n# Read and transform\ntable = engine.read_source({\"type\": \"file\", \"path\": \"in.csv\", \"format\": \"csv\"})\nfiltered = table.filter(table.status == \"active\")\n\n# Write\nengine.write_sink(\n    filtered,\n    {\"type\": \"file\", \"path\": \"out.parquet\", \"format\": \"parquet\"},\n    mode=\"replace\"\n)\n</code></pre>"},{"location":"api/engine/#apply_transform","title":"<code>apply_transform</code>","text":"<p>Apply a single transform to a table.</p> <pre><code>ETLXEngine.apply_transform(\n    table: Table,\n    transform: TransformConfig | dict\n) -&gt; Table\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>table</code> <code>Table</code> Input Ibis table <code>transform</code> <code>TransformConfig \\| dict</code> Transform configuration <p>Returns: Transformed Ibis table</p> <p>Example:</p> <pre><code>engine = ETLXEngine()\n\ntable = engine.read_source(source_config)\n\n# Apply transforms one by one\ntable = engine.apply_transform(table, {\"op\": \"filter\", \"predicate\": \"amount &gt; 0\"})\ntable = engine.apply_transform(table, {\"op\": \"select\", \"columns\": [\"id\", \"amount\"]})\n</code></pre>"},{"location":"api/engine/#apply_transforms","title":"<code>apply_transforms</code>","text":"<p>Apply multiple transforms to a table.</p> <pre><code>ETLXEngine.apply_transforms(\n    table: Table,\n    transforms: list[TransformConfig | dict]\n) -&gt; Table\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>table</code> <code>Table</code> Input Ibis table <code>transforms</code> <code>list</code> List of transform configurations <p>Returns: Transformed Ibis table</p> <p>Example:</p> <pre><code>engine = ETLXEngine()\n\ntable = engine.read_source(source_config)\n\ntransforms = [\n    {\"op\": \"filter\", \"predicate\": \"amount &gt; 0\"},\n    {\"op\": \"select\", \"columns\": [\"id\", \"name\", \"amount\"]},\n    {\"op\": \"sort\", \"by\": [{\"column\": \"amount\", \"order\": \"desc\"}]}\n]\n\nresult = engine.apply_transforms(table, transforms)\n</code></pre>"},{"location":"api/engine/#run_checks","title":"<code>run_checks</code>","text":"<p>Execute quality checks on a table.</p> <pre><code>ETLXEngine.run_checks(\n    table: Table,\n    checks: list[CheckConfig | dict]\n) -&gt; CheckResults\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>table</code> <code>Table</code> Table to validate <code>checks</code> <code>list</code> List of check configurations <p>Returns: <code>CheckResults</code> with pass/fail details</p> <p>Example:</p> <pre><code>engine = ETLXEngine()\ntable = engine.read_source(source_config)\n\nchecks = [\n    {\"check\": \"not_null\", \"columns\": [\"id\", \"name\"]},\n    {\"check\": \"unique\", \"columns\": [\"id\"]},\n    {\"check\": \"row_count\", \"min\": 1}\n]\n\nresults = engine.run_checks(table, checks)\nprint(f\"Passed: {results.passed}, Failed: {results.failed}\")\n\nfor check in results.details:\n    print(f\"  {check.name}: {check.status}\")\n</code></pre>"},{"location":"api/engine/#get_connection","title":"<code>get_connection</code>","text":"<p>Get the underlying Ibis connection.</p> <pre><code>ETLXEngine.get_connection() -&gt; Connection\n</code></pre> <p>Returns: Ibis backend connection</p> <p>Example:</p> <pre><code>engine = ETLXEngine(backend=\"duckdb\")\nconn = engine.get_connection()\n\n# Execute raw SQL\nresult = conn.raw_sql(\"SELECT * FROM read_csv('data.csv') LIMIT 10\")\n</code></pre>"},{"location":"api/engine/#properties","title":"Properties","text":""},{"location":"api/engine/#backend","title":"<code>backend</code>","text":"<p>The configured backend name.</p> <pre><code>engine.backend  # -&gt; str\n</code></pre>"},{"location":"api/engine/#is_connected","title":"<code>is_connected</code>","text":"<p>Whether the engine has an active connection.</p> <pre><code>engine.is_connected  # -&gt; bool\n</code></pre>"},{"location":"api/engine/#executionresult","title":"ExecutionResult","text":"<p>Result returned by <code>execute()</code>.</p> Attribute Type Description <code>success</code> <code>bool</code> Whether execution succeeded <code>duration_ms</code> <code>float</code> Execution time <code>rows_processed</code> <code>int</code> Rows read <code>rows_written</code> <code>int</code> Rows written <code>check_results</code> <code>CheckResults \\| None</code> Quality check results <code>error</code> <code>Exception \\| None</code> Error if failed <code>table</code> <code>Table \\| None</code> Result table (if dry_run)"},{"location":"api/engine/#checkresults","title":"CheckResults","text":"<p>Result returned by <code>run_checks()</code>.</p> Attribute Type Description <code>passed</code> <code>int</code> Number passed <code>failed</code> <code>int</code> Number failed <code>total</code> <code>int</code> Total checks <code>details</code> <code>list[CheckResult]</code> Individual results"},{"location":"api/engine/#complete-example","title":"Complete Example","text":"<pre><code>from etlx import ETLXEngine\n\n# Initialize engine\nengine = ETLXEngine(backend=\"duckdb\")\n\n# Read source\ntable = engine.read_source({\n    \"type\": \"file\",\n    \"path\": \"data/sales.csv\",\n    \"format\": \"csv\"\n})\n\n# Apply transforms\ntransforms = [\n    {\"op\": \"filter\", \"predicate\": \"status = 'completed'\"},\n    {\"op\": \"derive_column\", \"name\": \"total\", \"expr\": \"quantity * price\"},\n    {\"op\": \"aggregate\", \"group_by\": [\"category\"], \"aggregations\": {\"revenue\": \"sum(total)\"}}\n]\ntable = engine.apply_transforms(table, transforms)\n\n# Run quality checks\nchecks = [\n    {\"check\": \"not_null\", \"columns\": [\"category\", \"revenue\"]},\n    {\"check\": \"expression\", \"expr\": \"revenue &gt;= 0\"}\n]\ncheck_results = engine.run_checks(table, checks)\n\nif check_results.failed &gt; 0:\n    print(\"Quality checks failed!\")\n    for detail in check_results.details:\n        if not detail.passed:\n            print(f\"  - {detail.name}: {detail.message}\")\nelse:\n    # Write output\n    engine.write_sink(\n        table,\n        {\"type\": \"file\", \"path\": \"output/revenue.parquet\", \"format\": \"parquet\"}\n    )\n    print(\"Pipeline completed successfully\")\n</code></pre>"},{"location":"api/engine/#related","title":"Related","text":"<ul> <li>Pipeline - High-level pipeline API</li> <li>Configuration Models - Configuration types</li> <li>Backend Selection - Choosing backends</li> </ul>"},{"location":"api/pipeline/","title":"Pipeline Class","text":"<p>The <code>Pipeline</code> class is the main entry point for working with ETLX pipelines programmatically.</p>"},{"location":"api/pipeline/#import","title":"Import","text":"<pre><code>from etlx import Pipeline\n</code></pre>"},{"location":"api/pipeline/#class-methods","title":"Class Methods","text":""},{"location":"api/pipeline/#from_yaml","title":"<code>from_yaml</code>","text":"<p>Load a pipeline from a YAML configuration file.</p> <pre><code>Pipeline.from_yaml(path: str | Path) -&gt; Pipeline\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>path</code> <code>str \\| Path</code> Path to YAML configuration file <p>Returns: <code>Pipeline</code> instance</p> <p>Example:</p> <pre><code>pipeline = Pipeline.from_yaml(\"pipelines/sales_etl.yml\")\n</code></pre>"},{"location":"api/pipeline/#from_config","title":"<code>from_config</code>","text":"<p>Create a pipeline from a configuration dictionary.</p> <pre><code>Pipeline.from_config(config: dict) -&gt; Pipeline\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>dict</code> Pipeline configuration dictionary <p>Returns: <code>Pipeline</code> instance</p> <p>Example:</p> <pre><code>pipeline = Pipeline.from_config({\n    \"name\": \"sales_etl\",\n    \"engine\": \"duckdb\",\n    \"source\": {\n        \"type\": \"file\",\n        \"path\": \"data/sales.csv\",\n        \"format\": \"csv\"\n    },\n    \"transforms\": [\n        {\"op\": \"filter\", \"predicate\": \"amount &gt; 0\"},\n        {\"op\": \"select\", \"columns\": [\"id\", \"name\", \"amount\"]}\n    ],\n    \"sink\": {\n        \"type\": \"file\",\n        \"path\": \"output/results.parquet\",\n        \"format\": \"parquet\"\n    }\n})\n</code></pre>"},{"location":"api/pipeline/#from_model","title":"<code>from_model</code>","text":"<p>Create a pipeline from a Pydantic configuration model.</p> <pre><code>Pipeline.from_model(config: PipelineConfig) -&gt; Pipeline\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>PipelineConfig</code> Pydantic configuration model <p>Returns: <code>Pipeline</code> instance</p> <p>Example:</p> <pre><code>from etlx.config import PipelineConfig, FileSource, FileSink\n\nconfig = PipelineConfig(\n    name=\"sales_etl\",\n    engine=\"duckdb\",\n    source=FileSource(type=\"file\", path=\"data.csv\", format=\"csv\"),\n    sink=FileSink(type=\"file\", path=\"output.parquet\", format=\"parquet\")\n)\n\npipeline = Pipeline.from_model(config)\n</code></pre>"},{"location":"api/pipeline/#instance-methods","title":"Instance Methods","text":""},{"location":"api/pipeline/#run","title":"<code>run</code>","text":"<p>Execute the pipeline.</p> <pre><code>Pipeline.run(\n    variables: dict[str, str] | None = None,\n    engine: str | None = None,\n    dry_run: bool = False,\n    fail_on_checks: bool = True\n) -&gt; PipelineResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>variables</code> <code>dict[str, str] \\| None</code> <code>None</code> Variable substitutions <code>engine</code> <code>str \\| None</code> <code>None</code> Override engine from config <code>dry_run</code> <code>bool</code> <code>False</code> Execute without writing output <code>fail_on_checks</code> <code>bool</code> <code>True</code> Raise exception on check failure <p>Returns: <code>PipelineResult</code> with execution details</p> <p>Raises:</p> <ul> <li><code>ConfigurationError</code> - Invalid configuration</li> <li><code>ExecutionError</code> - Execution failure</li> <li><code>QualityCheckError</code> - Quality check failure (if <code>fail_on_checks=True</code>)</li> </ul> <p>Examples:</p> <pre><code># Basic run\nresult = pipeline.run()\n\n# With variables\nresult = pipeline.run(variables={\n    \"DATE\": \"2025-01-15\",\n    \"REGION\": \"north\"\n})\n\n# Override engine\nresult = pipeline.run(engine=\"polars\")\n\n# Dry run (no output written)\nresult = pipeline.run(dry_run=True)\n\n# Continue on check failures\nresult = pipeline.run(fail_on_checks=False)\nif result.checks_failed &gt; 0:\n    print(f\"Warning: {result.checks_failed} checks failed\")\n</code></pre>"},{"location":"api/pipeline/#validate","title":"<code>validate</code>","text":"<p>Validate pipeline configuration without executing.</p> <pre><code>Pipeline.validate() -&gt; list[str]\n</code></pre> <p>Returns: List of validation error messages (empty if valid)</p> <p>Example:</p> <pre><code>errors = pipeline.validate()\nif errors:\n    for error in errors:\n        print(f\"Validation error: {error}\")\nelse:\n    print(\"Configuration is valid\")\n    result = pipeline.run()\n</code></pre>"},{"location":"api/pipeline/#explain","title":"<code>explain</code>","text":"<p>Get an execution plan explanation.</p> <pre><code>Pipeline.explain() -&gt; str\n</code></pre> <p>Returns: Human-readable execution plan</p> <p>Example:</p> <pre><code>print(pipeline.explain())\n</code></pre> <p>Output:</p> <pre><code>Pipeline: sales_etl\nEngine: duckdb\n\nSteps:\n1. Read from: data/sales.csv (csv)\n2. Filter: amount &gt; 0\n3. Select: id, name, amount\n4. Quality checks: 2 checks\n5. Write to: output/results.parquet (parquet)\n</code></pre>"},{"location":"api/pipeline/#properties","title":"Properties","text":""},{"location":"api/pipeline/#name","title":"<code>name</code>","text":"<p>Pipeline name from configuration.</p> <pre><code>pipeline.name  # -&gt; str\n</code></pre>"},{"location":"api/pipeline/#config","title":"<code>config</code>","text":"<p>Access the underlying configuration model.</p> <pre><code>pipeline.config  # -&gt; PipelineConfig\n</code></pre>"},{"location":"api/pipeline/#engine","title":"<code>engine</code>","text":"<p>Configured engine name.</p> <pre><code>pipeline.engine  # -&gt; str\n</code></pre>"},{"location":"api/pipeline/#pipelineresult","title":"PipelineResult","text":"<p>The result returned by <code>Pipeline.run()</code>.</p>"},{"location":"api/pipeline/#attributes","title":"Attributes","text":"Attribute Type Description <code>pipeline_name</code> <code>str</code> Name of the pipeline <code>status</code> <code>str</code> \"SUCCESS\" or \"FAILED\" <code>duration_ms</code> <code>float</code> Execution time in milliseconds <code>rows_processed</code> <code>int</code> Total rows read from source <code>rows_written</code> <code>int</code> Rows written to sink <code>checks_passed</code> <code>int</code> Number of passed quality checks <code>checks_failed</code> <code>int</code> Number of failed quality checks <code>error</code> <code>str \\| None</code> Error message if failed"},{"location":"api/pipeline/#methods","title":"Methods","text":""},{"location":"api/pipeline/#to_dict","title":"<code>to_dict</code>","text":"<p>Convert result to dictionary.</p> <pre><code>result.to_dict() -&gt; dict\n</code></pre> <p>Example:</p> <pre><code>result = pipeline.run()\ndata = result.to_dict()\n# {\n#     \"pipeline_name\": \"sales_etl\",\n#     \"status\": \"SUCCESS\",\n#     \"duration_ms\": 245.3,\n#     \"rows_processed\": 1000,\n#     \"rows_written\": 950,\n#     \"checks_passed\": 2,\n#     \"checks_failed\": 0\n# }\n</code></pre>"},{"location":"api/pipeline/#to_json","title":"<code>to_json</code>","text":"<p>Convert result to JSON string.</p> <pre><code>result.to_json() -&gt; str\n</code></pre>"},{"location":"api/pipeline/#to_dataframe","title":"<code>to_dataframe</code>","text":"<p>Get the result data as a DataFrame.</p> <pre><code>result.to_dataframe() -&gt; Any  # Returns backend-specific DataFrame\n</code></pre> <p>Example:</p> <pre><code>result = pipeline.run()\ndf = result.to_dataframe()\nprint(df.head())\n</code></pre>"},{"location":"api/pipeline/#complete-example","title":"Complete Example","text":"<pre><code>from etlx import Pipeline\nfrom etlx.exceptions import QualityCheckError\n\n# Load pipeline\npipeline = Pipeline.from_yaml(\"pipelines/daily_sales.yml\")\n\n# Validate first\nerrors = pipeline.validate()\nif errors:\n    raise ValueError(f\"Invalid config: {errors}\")\n\n# Show execution plan\nprint(pipeline.explain())\n\n# Run with variables\ntry:\n    result = pipeline.run(\n        variables={\"DATE\": \"2025-01-15\"},\n        fail_on_checks=True\n    )\n\n    print(f\"\u2713 Pipeline completed in {result.duration_ms:.1f}ms\")\n    print(f\"  Rows: {result.rows_processed} \u2192 {result.rows_written}\")\n    print(f\"  Checks: {result.checks_passed} passed\")\n\nexcept QualityCheckError as e:\n    print(f\"\u2717 Quality checks failed\")\n    for check in e.failed_checks:\n        print(f\"  - {check}\")\n</code></pre>"},{"location":"api/pipeline/#related","title":"Related","text":"<ul> <li>ETLXEngine - Low-level engine API</li> <li>Configuration Models - Type-safe configuration</li> <li>CLI run Command - Command-line execution</li> </ul>"},{"location":"api/quality/","title":"Quality Check Classes","text":"<p>ETLX provides data quality check classes for programmatic validation. These classes can be used directly or via configuration models.</p>"},{"location":"api/quality/#import","title":"Import","text":"<pre><code>from etlx.quality import (\n    NotNullCheck,\n    UniqueCheck,\n    RowCountCheck,\n    AcceptedValuesCheck,\n    ExpressionCheck,\n    CheckRunner,\n    CheckResult,\n    CheckResults\n)\n</code></pre>"},{"location":"api/quality/#check-classes","title":"Check Classes","text":""},{"location":"api/quality/#notnullcheck","title":"NotNullCheck","text":"<p>Validates that specified columns contain no NULL values.</p> <pre><code>class NotNullCheck:\n    def __init__(self, columns: list[str])\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>columns</code> <code>list[str]</code> Columns to check for NULLs <p>Example:</p> <pre><code>from etlx.quality import NotNullCheck\n\ncheck = NotNullCheck(columns=[\"id\", \"email\", \"created_at\"])\n</code></pre>"},{"location":"api/quality/#uniquecheck","title":"UniqueCheck","text":"<p>Validates that values in specified columns are unique.</p> <pre><code>class UniqueCheck:\n    def __init__(self, columns: list[str])\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>columns</code> <code>list[str]</code> Columns that should be unique (individually or combined) <p>Example:</p> <pre><code>from etlx.quality import UniqueCheck\n\n# Single column uniqueness\ncheck = UniqueCheck(columns=[\"id\"])\n\n# Composite uniqueness\ncheck = UniqueCheck(columns=[\"order_id\", \"product_id\"])\n</code></pre>"},{"location":"api/quality/#rowcountcheck","title":"RowCountCheck","text":"<p>Validates the number of rows in the result.</p> <pre><code>class RowCountCheck:\n    def __init__(\n        self,\n        min: int | None = None,\n        max: int | None = None,\n        exact: int | None = None\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>min</code> <code>int \\| None</code> Minimum row count <code>max</code> <code>int \\| None</code> Maximum row count <code>exact</code> <code>int \\| None</code> Exact row count (overrides min/max) <p>Example:</p> <pre><code>from etlx.quality import RowCountCheck\n\n# At least 1 row\ncheck = RowCountCheck(min=1)\n\n# Between 100 and 10000 rows\ncheck = RowCountCheck(min=100, max=10000)\n\n# Exactly 1000 rows\ncheck = RowCountCheck(exact=1000)\n</code></pre>"},{"location":"api/quality/#acceptedvaluescheck","title":"AcceptedValuesCheck","text":"<p>Validates that a column contains only allowed values.</p> <pre><code>class AcceptedValuesCheck:\n    def __init__(\n        self,\n        column: str,\n        values: list[Any]\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>column</code> <code>str</code> Column to validate <code>values</code> <code>list[Any]</code> List of accepted values <p>Example:</p> <pre><code>from etlx.quality import AcceptedValuesCheck\n\n# Validate status values\ncheck = AcceptedValuesCheck(\n    column=\"status\",\n    values=[\"pending\", \"active\", \"completed\", \"cancelled\"]\n)\n\n# Validate boolean-like values\ncheck = AcceptedValuesCheck(\n    column=\"is_active\",\n    values=[0, 1, True, False]\n)\n</code></pre>"},{"location":"api/quality/#expressioncheck","title":"ExpressionCheck","text":"<p>Validates rows using a custom SQL expression.</p> <pre><code>class ExpressionCheck:\n    def __init__(\n        self,\n        expr: str,\n        threshold: float = 1.0\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>expr</code> <code>str</code> Boolean SQL expression <code>threshold</code> <code>float</code> Fraction of rows that must pass (0.0 to 1.0) <p>Example:</p> <pre><code>from etlx.quality import ExpressionCheck\n\n# All rows must pass\ncheck = ExpressionCheck(expr=\"amount &gt; 0\")\n\n# At least 95% must pass\ncheck = ExpressionCheck(\n    expr=\"email LIKE '%@%.%'\",\n    threshold=0.95\n)\n\n# Complex expression\ncheck = ExpressionCheck(\n    expr=\"start_date &lt;= end_date AND status IN ('active', 'pending')\"\n)\n</code></pre>"},{"location":"api/quality/#checkrunner","title":"CheckRunner","text":"<p>Execute checks against a table.</p> <pre><code>class CheckRunner:\n    def __init__(self, engine: ETLXEngine)\n\n    def run(\n        self,\n        table: Table,\n        checks: list[Check]\n    ) -&gt; CheckResults\n</code></pre> <p>Example:</p> <pre><code>from etlx import ETLXEngine\nfrom etlx.quality import (\n    CheckRunner,\n    NotNullCheck,\n    UniqueCheck,\n    RowCountCheck\n)\n\n# Setup\nengine = ETLXEngine(backend=\"duckdb\")\nrunner = CheckRunner(engine)\n\n# Read data\ntable = engine.read_source({\n    \"type\": \"file\",\n    \"path\": \"data/users.parquet\",\n    \"format\": \"parquet\"\n})\n\n# Define checks\nchecks = [\n    NotNullCheck(columns=[\"id\", \"email\"]),\n    UniqueCheck(columns=[\"id\"]),\n    UniqueCheck(columns=[\"email\"]),\n    RowCountCheck(min=1)\n]\n\n# Run checks\nresults = runner.run(table, checks)\n\nprint(f\"Passed: {results.passed}/{results.total}\")\nfor detail in results.details:\n    status = \"\u2713\" if detail.passed else \"\u2717\"\n    print(f\"  {status} {detail.name}: {detail.message}\")\n</code></pre>"},{"location":"api/quality/#result-classes","title":"Result Classes","text":""},{"location":"api/quality/#checkresult","title":"CheckResult","text":"<p>Individual check result.</p> <pre><code>class CheckResult:\n    name: str           # Check name/description\n    passed: bool        # Whether check passed\n    message: str        # Human-readable result\n    details: dict       # Additional details\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>name</code> <code>str</code> Check name (e.g., \"not_null: id, email\") <code>passed</code> <code>bool</code> Whether the check passed <code>message</code> <code>str</code> Result message <code>details</code> <code>dict</code> Additional metadata"},{"location":"api/quality/#checkresults","title":"CheckResults","text":"<p>Collection of check results.</p> <pre><code>class CheckResults:\n    passed: int              # Number of passed checks\n    failed: int              # Number of failed checks\n    total: int               # Total checks\n    details: list[CheckResult]  # Individual results\n</code></pre> <p>Methods:</p> <pre><code># Check if all passed\nif results.all_passed:\n    print(\"All checks passed!\")\n\n# Iterate failed checks\nfor result in results.failed_checks:\n    print(f\"Failed: {result.name} - {result.message}\")\n\n# Convert to dict/JSON\ndata = results.to_dict()\njson_str = results.to_json()\n</code></pre>"},{"location":"api/quality/#custom-check-classes","title":"Custom Check Classes","text":"<p>Create custom checks by extending the base class:</p> <pre><code>from etlx.quality import BaseCheck, CheckResult\n\nclass CustomRangeCheck(BaseCheck):\n    \"\"\"Check that numeric values are within a range.\"\"\"\n\n    def __init__(self, column: str, min_val: float, max_val: float):\n        self.column = column\n        self.min_val = min_val\n        self.max_val = max_val\n\n    @property\n    def name(self) -&gt; str:\n        return f\"range_check: {self.column} [{self.min_val}, {self.max_val}]\"\n\n    def run(self, table, engine) -&gt; CheckResult:\n        # Count rows outside range\n        expr = f\"{self.column} &lt; {self.min_val} OR {self.column} &gt; {self.max_val}\"\n        invalid_count = table.filter(expr).count().execute()\n        total_count = table.count().execute()\n\n        passed = invalid_count == 0\n        message = (\n            f\"All values in range\" if passed\n            else f\"{invalid_count}/{total_count} values out of range\"\n        )\n\n        return CheckResult(\n            name=self.name,\n            passed=passed,\n            message=message,\n            details={\n                \"invalid_count\": invalid_count,\n                \"total_count\": total_count\n            }\n        )\n\n# Usage\ncheck = CustomRangeCheck(column=\"age\", min_val=0, max_val=150)\n</code></pre>"},{"location":"api/quality/#complete-example","title":"Complete Example","text":"<pre><code>from etlx import ETLXEngine\nfrom etlx.quality import (\n    CheckRunner,\n    NotNullCheck,\n    UniqueCheck,\n    RowCountCheck,\n    AcceptedValuesCheck,\n    ExpressionCheck\n)\n\n# Initialize\nengine = ETLXEngine(backend=\"duckdb\")\nrunner = CheckRunner(engine)\n\n# Load data\ntable = engine.read_source({\n    \"type\": \"file\",\n    \"path\": \"data/orders.parquet\",\n    \"format\": \"parquet\"\n})\n\n# Apply transforms\ntable = engine.apply_transforms(table, [\n    {\"op\": \"filter\", \"predicate\": \"status != 'cancelled'\"}\n])\n\n# Define comprehensive checks\nchecks = [\n    # Required fields\n    NotNullCheck(columns=[\"order_id\", \"customer_id\", \"amount\"]),\n\n    # Primary key\n    UniqueCheck(columns=[\"order_id\"]),\n\n    # Data presence\n    RowCountCheck(min=1, max=1000000),\n\n    # Valid status values\n    AcceptedValuesCheck(\n        column=\"status\",\n        values=[\"pending\", \"processing\", \"shipped\", \"delivered\"]\n    ),\n\n    # Business rules\n    ExpressionCheck(expr=\"amount &gt; 0\"),\n    ExpressionCheck(expr=\"quantity &gt;= 1\"),\n    ExpressionCheck(expr=\"order_date &lt;= ship_date OR ship_date IS NULL\"),\n\n    # Data quality threshold (95% must have valid email)\n    ExpressionCheck(\n        expr=\"email LIKE '%@%.%'\",\n        threshold=0.95\n    )\n]\n\n# Run all checks\nresults = runner.run(table, checks)\n\n# Report results\nprint(f\"\\nQuality Check Results: {results.passed}/{results.total} passed\\n\")\n\nfor result in results.details:\n    icon = \"\u2713\" if result.passed else \"\u2717\"\n    print(f\"  {icon} {result.name}\")\n    if not result.passed:\n        print(f\"    \u2192 {result.message}\")\n\n# Fail pipeline if checks failed\nif not results.all_passed:\n    raise Exception(f\"Quality checks failed: {results.failed} failures\")\n</code></pre>"},{"location":"api/quality/#related","title":"Related","text":"<ul> <li>Quality Checks Guide - YAML configuration</li> <li>Pipeline API - Running checks in pipelines</li> <li>Best Practices - Testing strategies</li> </ul>"},{"location":"best-practices/","title":"Best Practices","text":"<p>This section covers best practices for building reliable, maintainable, and performant ETLX pipelines.</p>"},{"location":"best-practices/#overview","title":"Overview","text":"Guide Description Pipeline Design Structuring and organizing pipelines Error Handling Dealing with failures gracefully Performance Optimizing pipeline execution Testing Testing strategies for data pipelines Production Running pipelines in production"},{"location":"best-practices/#quick-tips","title":"Quick Tips","text":""},{"location":"best-practices/#pipeline-design","title":"Pipeline Design","text":"<ul> <li>Keep pipelines focused: One pipeline = one responsibility</li> <li>Use descriptive names: <code>daily_sales_by_region</code> not <code>pipeline1</code></li> <li>Add descriptions: Document what the pipeline does and why</li> <li>Filter early: Reduce data volume before expensive operations</li> </ul>"},{"location":"best-practices/#error-handling","title":"Error Handling","text":"<ul> <li>Use quality checks: Validate data before writing</li> <li>Set appropriate thresholds: Not all checks need 100% pass rate</li> <li>Log context: Include dates, row counts, durations</li> <li>Alert on failures: Don't let failures go unnoticed</li> </ul>"},{"location":"best-practices/#performance","title":"Performance","text":"<ul> <li>Choose the right backend: DuckDB for local, Spark for distributed</li> <li>Use Parquet format: Columnar, compressed, fast</li> <li>Select only needed columns: Reduce memory usage</li> <li>Batch operations: Minimize I/O overhead</li> </ul>"},{"location":"best-practices/#testing","title":"Testing","text":"<ul> <li>Test with sample data: Create representative test datasets</li> <li>Validate schemas: Ensure output matches expectations</li> <li>Test edge cases: Empty files, NULLs, duplicates</li> <li>Use CI/CD: Automate validation on every change</li> </ul>"},{"location":"best-practices/#production","title":"Production","text":"<ul> <li>Use environment variables: Never hardcode credentials</li> <li>Monitor pipelines: Track duration, row counts, errors</li> <li>Set up alerting: Get notified of failures</li> <li>Document runbooks: Know how to recover from failures</li> </ul>"},{"location":"best-practices/#common-patterns","title":"Common Patterns","text":""},{"location":"best-practices/#idempotent-pipelines","title":"Idempotent Pipelines","text":"<p>Pipelines that can be safely re-run:</p> <pre><code>sink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: replace  # Replaces existing data for the partition\n\n# Or use merge for upsert behavior\nsink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: merge\n  merge_keys: [date, region]  # Unique identifier\n</code></pre>"},{"location":"best-practices/#incremental-loading","title":"Incremental Loading","text":"<p>Process only new data:</p> <pre><code>name: incremental_load\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT * FROM events\n    WHERE created_at &gt;= '${LAST_RUN}'\n      AND created_at &lt; '${CURRENT_RUN}'\n</code></pre>"},{"location":"best-practices/#data-validation-gates","title":"Data Validation Gates","text":"<p>Ensure data quality before loading:</p> <pre><code>transforms:\n  - op: derive_column\n    name: total\n    expr: quantity * price\n\nchecks:\n  # Critical checks - must pass\n  - check: not_null\n    columns: [id, customer_id, total]\n\n  - check: unique\n    columns: [id]\n\n  # Warning checks - log but don't fail\n  - check: expression\n    expr: email LIKE '%@%.%'\n    threshold: 0.95  # 95% must pass\n\nsink:\n  # Only write if checks pass\n  type: file\n  path: output/validated_data.parquet\n</code></pre>"},{"location":"best-practices/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<p>Use variables for environment differences:</p> <pre><code>name: env_aware_pipeline\nsource:\n  type: file\n  path: ${DATA_PATH}/input/*.parquet\n  format: parquet\n\nsink:\n  type: database\n  connection: ${DATABASE_CONNECTION}\n  table: ${SCHEMA}.output_table\n</code></pre> <pre><code># Development\nDATA_PATH=./data DATABASE_CONNECTION=postgres_dev SCHEMA=dev \\\n  etlx run pipeline.yml\n\n# Production\nDATA_PATH=s3://bucket DATABASE_CONNECTION=postgres_prod SCHEMA=prod \\\n  etlx run pipeline.yml\n</code></pre>"},{"location":"best-practices/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":""},{"location":"best-practices/#dont","title":"Don't","text":"<pre><code># Avoid: No description\nname: p1\n\n# Avoid: SELECT * when you don't need all columns\ntransforms:\n  - op: select\n    columns: [\"*\"]\n\n# Avoid: Filtering after expensive operations\ntransforms:\n  - op: aggregate\n    group_by: [region]\n    aggregations:\n      total: sum(amount)\n  - op: filter  # Should be BEFORE aggregate\n    predicate: date &gt;= '2025-01-01'\n</code></pre>"},{"location":"best-practices/#do","title":"Do","text":"<pre><code># Good: Descriptive name and documentation\nname: daily_regional_sales_summary\ndescription: |\n  Aggregates daily sales by region for the analytics dashboard.\n  Runs daily at 6 AM UTC.\n\n# Good: Select only what you need\ntransforms:\n  - op: select\n    columns: [id, date, region, amount]\n\n# Good: Filter early\ntransforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [region]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"best-practices/#related","title":"Related","text":"<ul> <li>Pipeline Design - Detailed design guidance</li> <li>Examples - Complete working examples</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"best-practices/error-handling/","title":"Error Handling Best Practices","text":"<p>Strategies for handling errors gracefully in ETLX pipelines.</p>"},{"location":"best-practices/error-handling/#types-of-errors","title":"Types of Errors","text":""},{"location":"best-practices/error-handling/#configuration-errors","title":"Configuration Errors","text":"<p>Invalid YAML or missing required fields:</p> <pre><code>$ etlx validate pipeline.yml\nConfiguration is invalid\n\nErrors:\n  - sink: Field required\n  - transforms -&gt; 0 -&gt; op: Input should be 'select', 'filter', ...\n</code></pre> <p>Prevention: Always validate before running:</p> <pre><code>etlx validate pipeline.yml &amp;&amp; etlx run pipeline.yml\n</code></pre>"},{"location":"best-practices/error-handling/#runtime-errors","title":"Runtime Errors","text":"<p>Errors during execution:</p> <ul> <li>File not found</li> <li>Database connection failed</li> <li>Out of memory</li> <li>Permission denied</li> </ul>"},{"location":"best-practices/error-handling/#data-quality-errors","title":"Data Quality Errors","text":"<p>Quality checks that fail:</p> <pre><code>Quality Checks: FAILED (2/3 passed)\n  \u2713 not_null: id, name\n  \u2717 unique: email (152 duplicates found)\n  \u2713 row_count: min=1\n</code></pre>"},{"location":"best-practices/error-handling/#quality-check-strategies","title":"Quality Check Strategies","text":""},{"location":"best-practices/error-handling/#critical-vs-non-critical-checks","title":"Critical vs Non-Critical Checks","text":"<pre><code>checks:\n  # Critical: Must pass 100%\n  - check: not_null\n    columns: [id, customer_id]\n\n  - check: unique\n    columns: [id]\n\n  # Non-critical: Warning only (use threshold)\n  - check: expression\n    expr: email LIKE '%@%.%'\n    threshold: 0.95  # 95% must pass\n\n  - check: expression\n    expr: amount &gt; 0\n    threshold: 0.99  # 99% must pass\n</code></pre>"},{"location":"best-practices/error-handling/#continue-on-check-failure","title":"Continue on Check Failure","text":"<p>For non-critical pipelines:</p> <pre><code>etlx run pipeline.yml --no-fail-on-checks\n</code></pre>"},{"location":"best-practices/error-handling/#programmatic-handling","title":"Programmatic Handling","text":"<pre><code>from etlx import Pipeline\nfrom etlx.exceptions import QualityCheckError\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\n\ntry:\n    result = pipeline.run(fail_on_checks=True)\nexcept QualityCheckError as e:\n    print(f\"Quality checks failed:\")\n    for check in e.failed_checks:\n        print(f\"  - {check.name}: {check.message}\")\n\n    # Decide what to do\n    if \"critical\" in e.failed_checks[0].name:\n        raise  # Re-raise for critical failures\n    else:\n        # Log and continue for warnings\n        logger.warning(f\"Non-critical check failed: {e}\")\n</code></pre>"},{"location":"best-practices/error-handling/#retry-strategies","title":"Retry Strategies","text":""},{"location":"best-practices/error-handling/#simple-retry-script","title":"Simple Retry Script","text":"<pre><code>#!/bin/bash\nMAX_RETRIES=3\nRETRY_DELAY=60\n\nfor i in $(seq 1 $MAX_RETRIES); do\n    if etlx run pipeline.yml --var DATE=$1; then\n        echo \"Success on attempt $i\"\n        exit 0\n    fi\n\n    if [ $i -lt $MAX_RETRIES ]; then\n        echo \"Attempt $i failed, retrying in ${RETRY_DELAY}s...\"\n        sleep $RETRY_DELAY\n    fi\ndone\n\necho \"Failed after $MAX_RETRIES attempts\"\nexit 1\n</code></pre>"},{"location":"best-practices/error-handling/#exponential-backoff","title":"Exponential Backoff","text":"<pre><code>#!/bin/bash\nMAX_RETRIES=5\nBASE_DELAY=30\n\nfor i in $(seq 1 $MAX_RETRIES); do\n    if etlx run pipeline.yml; then\n        exit 0\n    fi\n\n    if [ $i -lt $MAX_RETRIES ]; then\n        DELAY=$((BASE_DELAY * 2 ** (i - 1)))\n        echo \"Retry $i in ${DELAY}s...\"\n        sleep $DELAY\n    fi\ndone\n\nexit 1\n</code></pre>"},{"location":"best-practices/error-handling/#python-retry","title":"Python Retry","text":"<pre><code>import time\nfrom etlx import Pipeline\nfrom etlx.exceptions import ExecutionError\n\ndef run_with_retry(config_path, max_retries=3, base_delay=30):\n    pipeline = Pipeline.from_yaml(config_path)\n\n    for attempt in range(1, max_retries + 1):\n        try:\n            return pipeline.run()\n        except ExecutionError as e:\n            if attempt == max_retries:\n                raise\n\n            delay = base_delay * (2 ** (attempt - 1))\n            print(f\"Attempt {attempt} failed: {e}\")\n            print(f\"Retrying in {delay}s...\")\n            time.sleep(delay)\n</code></pre>"},{"location":"best-practices/error-handling/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"best-practices/error-handling/#verbose-output","title":"Verbose Output","text":"<pre><code>etlx run pipeline.yml --verbose\n</code></pre> <p>Shows detailed step-by-step execution.</p>"},{"location":"best-practices/error-handling/#json-output-for-monitoring","title":"JSON Output for Monitoring","text":"<pre><code>etlx run pipeline.yml --json &gt; result.json\n</code></pre> <pre><code>{\n  \"pipeline_name\": \"daily_sales\",\n  \"status\": \"SUCCESS\",\n  \"duration_ms\": 1234.5,\n  \"rows_processed\": 10000,\n  \"rows_written\": 9500,\n  \"checks_passed\": 3,\n  \"checks_failed\": 0\n}\n</code></pre>"},{"location":"best-practices/error-handling/#send-to-monitoring-system","title":"Send to Monitoring System","text":"<pre><code>#!/bin/bash\nRESULT=$(etlx run pipeline.yml --json)\nSTATUS=$(echo $RESULT | jq -r '.status')\nDURATION=$(echo $RESULT | jq -r '.duration_ms')\nROWS=$(echo $RESULT | jq -r '.rows_written')\n\n# Send to DataDog\ncurl -X POST \"https://api.datadoghq.com/api/v1/series\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"DD-API-KEY: $DD_API_KEY\" \\\n  -d \"{\n    \\\"series\\\": [{\n      \\\"metric\\\": \\\"etlx.pipeline.duration\\\",\n      \\\"points\\\": [[$(date +%s), $DURATION]],\n      \\\"tags\\\": [\\\"pipeline:daily_sales\\\", \\\"status:$STATUS\\\"]\n    }]\n  }\"\n</code></pre>"},{"location":"best-practices/error-handling/#error-recovery-patterns","title":"Error Recovery Patterns","text":""},{"location":"best-practices/error-handling/#idempotent-pipelines","title":"Idempotent Pipelines","text":"<p>Design pipelines that can be safely re-run:</p> <pre><code># Replace mode: Safe to re-run\nsink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: replace\n\n# Or use merge with keys\nsink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: merge\n  merge_keys: [date, region]  # Unique identifier\n</code></pre>"},{"location":"best-practices/error-handling/#checkpoint-pattern","title":"Checkpoint Pattern","text":"<p>For long-running pipelines, break into checkpoints:</p> <pre><code># Step 1: Extract (can re-run)\nname: extract_raw\nsink:\n  type: file\n  path: staging/raw_${DATE}.parquet\n\n# Step 2: Transform (starts from checkpoint)\nname: transform_data\nsource:\n  type: file\n  path: staging/raw_${DATE}.parquet  # Checkpoint\nsink:\n  type: file\n  path: staging/transformed_${DATE}.parquet\n\n# Step 3: Load (final step)\nname: load_warehouse\nsource:\n  type: file\n  path: staging/transformed_${DATE}.parquet\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.metrics\n</code></pre>"},{"location":"best-practices/error-handling/#dead-letter-queue","title":"Dead Letter Queue","text":"<p>Capture failed records:</p> <pre><code>from etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\n\ntry:\n    result = pipeline.run(fail_on_checks=False)\n\n    if result.checks_failed &gt; 0:\n        # Get failed records\n        failed_df = result.get_failed_records()\n\n        # Write to dead letter queue\n        failed_df.to_parquet(f\"dlq/failed_{date}.parquet\")\n\nexcept Exception as e:\n    # Log entire batch to DLQ\n    logger.error(f\"Pipeline failed: {e}\")\n    shutil.copy(input_file, f\"dlq/failed_batch_{date}.parquet\")\n</code></pre>"},{"location":"best-practices/error-handling/#alerting","title":"Alerting","text":""},{"location":"best-practices/error-handling/#email-on-failure","title":"Email on Failure","text":"<pre><code>#!/bin/bash\nif ! etlx run pipeline.yml; then\n    echo \"Pipeline failed at $(date)\" | \\\n    mail -s \"ALERT: ETL Pipeline Failed\" team@company.com\n    exit 1\nfi\n</code></pre>"},{"location":"best-practices/error-handling/#slack-notification","title":"Slack Notification","text":"<pre><code>import requests\nfrom etlx import Pipeline\n\nSLACK_WEBHOOK = \"https://hooks.slack.com/services/...\"\n\ndef notify_slack(message, color=\"danger\"):\n    requests.post(SLACK_WEBHOOK, json={\n        \"attachments\": [{\n            \"color\": color,\n            \"text\": message\n        }]\n    })\n\ntry:\n    pipeline = Pipeline.from_yaml(\"pipeline.yml\")\n    result = pipeline.run()\n\n    notify_slack(\n        f\"\u2713 Pipeline completed: {result.rows_written} rows\",\n        color=\"good\"\n    )\nexcept Exception as e:\n    notify_slack(f\"\u2717 Pipeline failed: {e}\")\n    raise\n</code></pre>"},{"location":"best-practices/error-handling/#pagerduty-integration","title":"PagerDuty Integration","text":"<pre><code>#!/bin/bash\nif ! etlx run pipeline.yml; then\n    curl -X POST https://events.pagerduty.com/v2/enqueue \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"routing_key\": \"'$PD_ROUTING_KEY'\",\n        \"event_action\": \"trigger\",\n        \"payload\": {\n          \"summary\": \"ETL Pipeline Failed\",\n          \"severity\": \"critical\",\n          \"source\": \"etlx\"\n        }\n      }'\n    exit 1\nfi\n</code></pre>"},{"location":"best-practices/error-handling/#debugging-tips","title":"Debugging Tips","text":""},{"location":"best-practices/error-handling/#dry-run","title":"Dry Run","text":"<p>Test without writing output:</p> <pre><code>etlx run pipeline.yml --dry-run\n</code></pre>"},{"location":"best-practices/error-handling/#verbose-logging","title":"Verbose Logging","text":"<pre><code>etlx run pipeline.yml --verbose\n</code></pre>"},{"location":"best-practices/error-handling/#validate-configuration","title":"Validate Configuration","text":"<pre><code>etlx validate pipeline.yml --verbose\n</code></pre>"},{"location":"best-practices/error-handling/#check-backend-availability","title":"Check Backend Availability","text":"<pre><code>etlx info --backends --check\n</code></pre>"},{"location":"best-practices/error-handling/#test-with-sample-data","title":"Test with Sample Data","text":"<p>Create a small test dataset:</p> <pre><code>head -100 data/large_file.csv &gt; data/test_sample.csv\netlx run pipeline.yml --var INPUT=data/test_sample.csv\n</code></pre>"},{"location":"best-practices/error-handling/#related","title":"Related","text":"<ul> <li>Quality Checks - Check configuration</li> <li>Production - Production deployment</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"best-practices/performance/","title":"Performance Best Practices","text":"<p>Optimize ETLX pipeline execution for speed and efficiency.</p>"},{"location":"best-practices/performance/#backend-selection","title":"Backend Selection","text":"<p>Choose the right backend for your workload:</p> Scenario Recommended Backend Local files &lt; 1GB DuckDB Local files 1-10GB DuckDB or Polars Local files &gt; 10GB Polars (streaming) Distributed processing Spark Data in warehouse Snowflake/BigQuery Quick prototyping DuckDB"},{"location":"best-practices/performance/#quick-comparison","title":"Quick Comparison","text":"<pre><code># Test different backends\ntime etlx run pipeline.yml --engine duckdb\ntime etlx run pipeline.yml --engine polars\n</code></pre>"},{"location":"best-practices/performance/#file-format-optimization","title":"File Format Optimization","text":""},{"location":"best-practices/performance/#use-parquet","title":"Use Parquet","text":"<p>Parquet is significantly faster than CSV:</p> <pre><code># Slow: CSV\nsource:\n  type: file\n  path: data/input.csv\n  format: csv\n\n# Fast: Parquet\nsource:\n  type: file\n  path: data/input.parquet\n  format: parquet\n</code></pre> <p>Why Parquet is faster:</p> <ul> <li>Columnar storage (read only needed columns)</li> <li>Built-in compression</li> <li>Type preservation (no parsing)</li> <li>Predicate pushdown</li> </ul>"},{"location":"best-practices/performance/#convert-csv-to-parquet","title":"Convert CSV to Parquet","text":"<p>One-time conversion:</p> <pre><code>name: convert_to_parquet\nsource:\n  type: file\n  path: data/large_file.csv\n  format: csv\nsink:\n  type: file\n  path: data/large_file.parquet\n  format: parquet\n</code></pre>"},{"location":"best-practices/performance/#transform-optimization","title":"Transform Optimization","text":""},{"location":"best-practices/performance/#1-filter-early","title":"1. Filter Early","text":"<p>Reduce data volume before expensive operations:</p> <pre><code>transforms:\n  # Good: Filter first\n  - op: filter\n    predicate: date &gt;= '2025-01-01' AND status = 'active'\n\n  - op: join  # Joins fewer rows\n    right: ...\n\n  - op: aggregate  # Aggregates fewer rows\n    group_by: ...\n</code></pre> <p>Impact: Can reduce processing time by 10-100x.</p>"},{"location":"best-practices/performance/#2-select-early","title":"2. Select Early","text":"<p>Only keep columns you need:</p> <pre><code>transforms:\n  # Good: Select needed columns early\n  - op: select\n    columns: [id, date, amount, category]\n\n  # Now operations work with less data\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"best-practices/performance/#3-avoid-unnecessary-operations","title":"3. Avoid Unnecessary Operations","text":"<pre><code># Bad: Unnecessary sort before aggregate\ntransforms:\n  - op: sort\n    by: [{column: date, order: asc}]\n  - op: aggregate  # Aggregate doesn't need sorted input\n    group_by: [category]\n\n# Good: Remove unnecessary sort\ntransforms:\n  - op: aggregate\n    group_by: [category]\n</code></pre>"},{"location":"best-practices/performance/#4-combine-filters","title":"4. Combine Filters","text":"<pre><code># Less efficient: Multiple filter operations\ntransforms:\n  - op: filter\n    predicate: status = 'active'\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n  - op: filter\n    predicate: amount &gt; 0\n\n# More efficient: Single filter\ntransforms:\n  - op: filter\n    predicate: |\n      status = 'active'\n      AND date &gt;= '2025-01-01'\n      AND amount &gt; 0\n</code></pre>"},{"location":"best-practices/performance/#join-optimization","title":"Join Optimization","text":""},{"location":"best-practices/performance/#1-filter-before-joining","title":"1. Filter Before Joining","text":"<pre><code>transforms:\n  # Filter main table first\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  # Then join (fewer rows to match)\n  - op: join\n    right:\n      type: file\n      path: data/dimension.parquet\n      format: parquet\n    on: [id]\n    how: left\n</code></pre>"},{"location":"best-practices/performance/#2-join-smaller-tables","title":"2. Join Smaller Tables","text":"<p>Put the larger table on the left:</p> <pre><code># Source is large (1M rows)\nsource:\n  type: file\n  path: data/transactions.parquet  # 1M rows\n\ntransforms:\n  # Join with smaller dimension table (10K rows)\n  - op: join\n    right:\n      type: file\n      path: data/products.parquet  # 10K rows\n      format: parquet\n    on: [product_id]\n    how: left\n</code></pre>"},{"location":"best-practices/performance/#3-use-appropriate-join-type","title":"3. Use Appropriate Join Type","text":"<pre><code># inner: Only matching rows (smallest result)\n- op: join\n  how: inner\n\n# left: All left rows (may include NULLs)\n- op: join\n  how: left\n</code></pre>"},{"location":"best-practices/performance/#memory-management","title":"Memory Management","text":""},{"location":"best-practices/performance/#1-process-in-chunks","title":"1. Process in Chunks","text":"<p>For very large files, use streaming-capable backends:</p> <pre><code>engine: polars  # Supports streaming\n\nsource:\n  type: file\n  path: data/huge_file.parquet\n  format: parquet\n</code></pre>"},{"location":"best-practices/performance/#2-reduce-column-count","title":"2. Reduce Column Count","text":"<pre><code>transforms:\n  - op: select\n    columns: [id, amount, date]  # Only what's needed\n</code></pre>"},{"location":"best-practices/performance/#3-use-appropriate-data-types","title":"3. Use Appropriate Data Types","text":"<pre><code>transforms:\n  - op: cast\n    columns:\n      id: int32      # Instead of int64\n      amount: float32  # Instead of float64\n</code></pre>"},{"location":"best-practices/performance/#parallel-execution","title":"Parallel Execution","text":""},{"location":"best-practices/performance/#multiple-independent-pipelines","title":"Multiple Independent Pipelines","text":"<p>Run independent pipelines in parallel:</p> <pre><code># Sequential (slow)\netlx run pipeline1.yml\netlx run pipeline2.yml\netlx run pipeline3.yml\n\n# Parallel (fast)\netlx run pipeline1.yml &amp;\netlx run pipeline2.yml &amp;\netlx run pipeline3.yml &amp;\nwait\n</code></pre>"},{"location":"best-practices/performance/#spark-parallelism","title":"Spark Parallelism","text":"<p>For Spark backend:</p> <pre><code>export SPARK_EXECUTOR_INSTANCES=10\nexport SPARK_EXECUTOR_CORES=4\nexport SPARK_EXECUTOR_MEMORY=8g\n\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"best-practices/performance/#io-optimization","title":"I/O Optimization","text":""},{"location":"best-practices/performance/#1-use-local-storage","title":"1. Use Local Storage","text":"<p>Local SSD is faster than network storage:</p> <pre><code># Fast: Local SSD\nsource:\n  type: file\n  path: /local/ssd/data.parquet\n\n# Slower: Network mount\nsource:\n  type: file\n  path: /mnt/network/data.parquet\n</code></pre>"},{"location":"best-practices/performance/#2-minimize-network-calls","title":"2. Minimize Network Calls","text":"<p>For cloud storage, batch reads:</p> <pre><code># Efficient: Read all matching files at once\nsource:\n  type: file\n  path: s3://bucket/data/*.parquet  # Single glob\n  format: parquet\n</code></pre>"},{"location":"best-practices/performance/#3-compress-output","title":"3. Compress Output","text":"<pre><code>sink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n  options:\n    compression: snappy  # Fast compression\n</code></pre>"},{"location":"best-practices/performance/#benchmarking","title":"Benchmarking","text":""},{"location":"best-practices/performance/#measure-execution-time","title":"Measure Execution Time","text":"<pre><code>time etlx run pipeline.yml\n</code></pre>"},{"location":"best-practices/performance/#json-metrics","title":"JSON Metrics","text":"<pre><code>etlx run pipeline.yml --json | jq '.duration_ms'\n</code></pre>"},{"location":"best-practices/performance/#compare-backends","title":"Compare Backends","text":"<pre><code>import time\nfrom etlx import Pipeline\n\nbackends = [\"duckdb\", \"polars\", \"pandas\"]\n\nfor backend in backends:\n    pipeline = Pipeline.from_yaml(\"pipeline.yml\")\n    start = time.time()\n    pipeline.run(engine=backend)\n    duration = time.time() - start\n    print(f\"{backend}: {duration:.2f}s\")\n</code></pre>"},{"location":"best-practices/performance/#profiling","title":"Profiling","text":""},{"location":"best-practices/performance/#verbose-output","title":"Verbose Output","text":"<pre><code>etlx run pipeline.yml --verbose\n</code></pre> <p>Shows timing for each step.</p>"},{"location":"best-practices/performance/#python-profiling","title":"Python Profiling","text":"<pre><code>import cProfile\nfrom etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\ncProfile.run(\"pipeline.run()\", sort=\"cumtime\")\n</code></pre>"},{"location":"best-practices/performance/#common-bottlenecks","title":"Common Bottlenecks","text":"Symptom Likely Cause Solution Slow start Large CSV parsing Use Parquet Memory error Too much data Filter early, use Polars Slow joins Large tables Filter before join Slow writes Many small files Batch writes Network timeout Cloud storage Use local cache"},{"location":"best-practices/performance/#performance-checklist","title":"Performance Checklist","text":"<ul> <li> Using Parquet instead of CSV?</li> <li> Filtering early in pipeline?</li> <li> Selecting only needed columns?</li> <li> Using appropriate backend for data size?</li> <li> Joins ordered correctly (large left, small right)?</li> <li> No unnecessary transforms?</li> <li> Output compressed?</li> </ul>"},{"location":"best-practices/performance/#related","title":"Related","text":"<ul> <li>Backend Selection - Choose the right backend</li> <li>DuckDB - Optimize for DuckDB</li> <li>Polars - Optimize for Polars</li> </ul>"},{"location":"best-practices/pipeline-design/","title":"Pipeline Design Best Practices","text":"<p>Guidelines for designing clean, maintainable, and efficient ETLX pipelines.</p>"},{"location":"best-practices/pipeline-design/#naming-conventions","title":"Naming Conventions","text":""},{"location":"best-practices/pipeline-design/#pipeline-names","title":"Pipeline Names","text":"<p>Use descriptive, action-oriented names:</p> <pre><code># Good\nname: extract_daily_orders\nname: transform_sales_metrics\nname: load_customer_warehouse\n\n# Avoid\nname: pipeline1\nname: test\nname: data\n</code></pre>"},{"location":"best-practices/pipeline-design/#file-organization","title":"File Organization","text":"<pre><code>pipelines/\n\u251c\u2500\u2500 extract/\n\u2502   \u251c\u2500\u2500 orders.yml\n\u2502   \u251c\u2500\u2500 products.yml\n\u2502   \u2514\u2500\u2500 customers.yml\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 daily_metrics.yml\n\u2502   \u2514\u2500\u2500 weekly_rollup.yml\n\u251c\u2500\u2500 load/\n\u2502   \u2514\u2500\u2500 warehouse.yml\n\u2514\u2500\u2500 quality/\n    \u2514\u2500\u2500 data_validation.yml\n</code></pre>"},{"location":"best-practices/pipeline-design/#variable-names","title":"Variable Names","text":"<p>Use SCREAMING_SNAKE_CASE for variables:</p> <pre><code>source:\n  type: file\n  path: data/sales_${DATE}.csv\n  format: csv\n\n# Run with: --var DATE=2025-01-15\n</code></pre>"},{"location":"best-practices/pipeline-design/#single-responsibility","title":"Single Responsibility","text":"<p>Each pipeline should do one thing well.</p>"},{"location":"best-practices/pipeline-design/#dont-monolithic-pipeline","title":"Don't: Monolithic Pipeline","text":"<pre><code># Avoid: Too many responsibilities\nname: do_everything\ntransforms:\n  # Extract from multiple sources\n  - op: join ...\n  - op: join ...\n  # Transform\n  - op: filter ...\n  - op: aggregate ...\n  - op: aggregate ...\n  # Multiple outputs (not supported)\n</code></pre>"},{"location":"best-practices/pipeline-design/#do-focused-pipelines","title":"Do: Focused Pipelines","text":"<pre><code># Pipeline 1: Extract and stage\nname: extract_orders\nsource:\n  type: database\n  connection: postgres\n  table: orders\nsink:\n  type: file\n  path: staging/orders.parquet\n</code></pre> <pre><code># Pipeline 2: Transform\nname: transform_metrics\nsource:\n  type: file\n  path: staging/orders.parquet\ntransforms:\n  - op: aggregate ...\nsink:\n  type: file\n  path: processed/metrics.parquet\n</code></pre> <pre><code># Pipeline 3: Load\nname: load_warehouse\nsource:\n  type: file\n  path: processed/metrics.parquet\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.metrics\n</code></pre>"},{"location":"best-practices/pipeline-design/#documentation","title":"Documentation","text":""},{"location":"best-practices/pipeline-design/#pipeline-level-documentation","title":"Pipeline-Level Documentation","text":"<pre><code>name: daily_revenue_report\ndescription: |\n  Generates daily revenue metrics by region and category.\n\n  Data Sources:\n  - orders: Transactional order data from PostgreSQL\n  - products: Product catalog from S3\n\n  Output:\n  - Aggregated revenue metrics for dashboard consumption\n\n  Schedule: Daily at 6 AM UTC\n  Owner: data-team@company.com\n</code></pre>"},{"location":"best-practices/pipeline-design/#transform-comments","title":"Transform Comments","text":"<p>YAML supports comments - use them:</p> <pre><code>transforms:\n  # Remove test orders (order_id starting with 'TEST')\n  - op: filter\n    predicate: NOT order_id LIKE 'TEST%'\n\n  # Calculate gross margin\n  # Formula: (revenue - cost) / revenue\n  - op: derive_column\n    name: gross_margin\n    expr: (amount - cost) / amount\n\n  # Aggregate to daily level for dashboard\n  - op: aggregate\n    group_by: [date, region]\n    aggregations:\n      revenue: sum(amount)\n</code></pre>"},{"location":"best-practices/pipeline-design/#transform-ordering","title":"Transform Ordering","text":""},{"location":"best-practices/pipeline-design/#filter-early","title":"Filter Early","text":"<p>Reduce data volume before expensive operations:</p> <pre><code>transforms:\n  # Good: Filter first\n  - op: filter\n    predicate: status = 'completed' AND date &gt;= '2025-01-01'\n\n  - op: join\n    right: ...  # Joins fewer rows\n\n  - op: aggregate  # Aggregates fewer rows\n    group_by: ...\n</code></pre>"},{"location":"best-practices/pipeline-design/#select-early","title":"Select Early","text":"<p>Only keep columns you need:</p> <pre><code>transforms:\n  - op: select\n    columns: [id, date, amount, category]\n\n  # Subsequent operations work with fewer columns\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"best-practices/pipeline-design/#logical-order","title":"Logical Order","text":"<ol> <li>Filter - Remove unwanted rows</li> <li>Select - Keep only needed columns</li> <li>Derive - Create calculated columns</li> <li>Join - Combine with other data</li> <li>Aggregate - Summarize</li> <li>Sort - Order results</li> <li>Limit - Truncate if needed</li> </ol>"},{"location":"best-practices/pipeline-design/#quality-gates","title":"Quality Gates","text":""},{"location":"best-practices/pipeline-design/#critical-vs-warning-checks","title":"Critical vs Warning Checks","text":"<pre><code>checks:\n  # Critical: Pipeline fails if these don't pass\n  - check: not_null\n    columns: [id, amount]\n\n  - check: unique\n    columns: [id]\n\n  # Warning: Log but don't fail (95% threshold)\n  - check: expression\n    expr: amount &gt; 0\n    threshold: 0.95\n</code></pre>"},{"location":"best-practices/pipeline-design/#meaningful-checks","title":"Meaningful Checks","text":"<pre><code>checks:\n  # Check data freshness\n  - check: expression\n    expr: date &gt;= current_date - interval '2 days'\n\n  # Check referential integrity\n  - check: expression\n    expr: customer_id IS NOT NULL\n\n  # Check business rules\n  - check: expression\n    expr: quantity &gt; 0 AND quantity &lt; 1000\n\n  # Check for expected volume\n  - check: row_count\n    min: 100\n    max: 1000000\n</code></pre>"},{"location":"best-practices/pipeline-design/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"best-practices/pipeline-design/#environment-variables","title":"Environment Variables","text":"<p>Externalize environment-specific values:</p> <pre><code>name: ${PIPELINE_NAME:-default_pipeline}\nengine: ${ENGINE:-duckdb}\n\nsource:\n  type: file\n  path: ${INPUT_PATH}\n  format: parquet\n\nsink:\n  type: database\n  connection: ${DB_CONNECTION}\n  table: ${SCHEMA}.${TABLE}\n</code></pre>"},{"location":"best-practices/pipeline-design/#defaults","title":"Defaults","text":"<p>Use defaults for optional values:</p> <pre><code>source:\n  type: file\n  path: ${INPUT_PATH:-data/default.parquet}\n  format: ${FORMAT:-parquet}\n</code></pre>"},{"location":"best-practices/pipeline-design/#modular-configuration","title":"Modular Configuration","text":"<p>Split large pipelines into includes (future feature):</p> <pre><code># base.yml\nname: base_pipeline\nengine: duckdb\n\n# pipeline.yml\nextends: base.yml\nsource: ...\ntransforms: ...\n</code></pre>"},{"location":"best-practices/pipeline-design/#anti-patterns","title":"Anti-Patterns","text":""},{"location":"best-practices/pipeline-design/#avoid-wide-select","title":"Avoid: Wide SELECT *","text":"<pre><code># Bad: Selects all columns\ntransforms:\n  - op: select\n    columns: [\"*\"]\n\n# Good: Explicit columns\ntransforms:\n  - op: select\n    columns: [id, name, amount, date]\n</code></pre>"},{"location":"best-practices/pipeline-design/#avoid-late-filtering","title":"Avoid: Late Filtering","text":"<pre><code># Bad: Filter after expensive operations\ntransforms:\n  - op: aggregate\n    group_by: [region, date]\n    aggregations:\n      total: sum(amount)\n  - op: filter\n    predicate: date &gt;= '2025-01-01'  # Should be first!\n\n# Good: Filter early\ntransforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [region, date]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"best-practices/pipeline-design/#avoid-hardcoded-values","title":"Avoid: Hardcoded Values","text":"<pre><code># Bad: Hardcoded\nsource:\n  type: file\n  path: /home/user/data/sales_2025-01-15.csv\n\n# Good: Parameterized\nsource:\n  type: file\n  path: ${DATA_DIR}/sales_${DATE}.csv\n</code></pre>"},{"location":"best-practices/pipeline-design/#avoid-no-quality-checks","title":"Avoid: No Quality Checks","text":"<pre><code># Bad: No validation\nsink:\n  type: database\n  connection: postgres\n  table: production.critical_table\n\n# Good: Validate before writing\nchecks:\n  - check: not_null\n    columns: [id]\n  - check: row_count\n    min: 1\nsink:\n  type: database\n  connection: postgres\n  table: production.critical_table\n</code></pre>"},{"location":"best-practices/pipeline-design/#related","title":"Related","text":"<ul> <li>Error Handling - Handling failures</li> <li>Performance - Optimization tips</li> <li>Examples - Complete examples</li> </ul>"},{"location":"best-practices/production/","title":"Production Best Practices","text":"<p>Guidelines for running ETLX pipelines reliably in production environments.</p>"},{"location":"best-practices/production/#environment-configuration","title":"Environment Configuration","text":""},{"location":"best-practices/production/#use-environment-variables","title":"Use Environment Variables","text":"<p>Never hardcode credentials or environment-specific values:</p> <pre><code># Good: Environment variables\nsource:\n  type: database\n  connection: ${DATABASE_CONNECTION}\n\nsink:\n  type: file\n  path: ${OUTPUT_BUCKET}/data/${DATE}/\n</code></pre> <pre><code># Set in environment\nexport DATABASE_CONNECTION=postgres_prod\nexport OUTPUT_BUCKET=s3://prod-data-lake\nexport DATE=$(date +%Y-%m-%d)\n\netlx run pipeline.yml\n</code></pre>"},{"location":"best-practices/production/#use-env-files","title":"Use .env Files","text":"<p>For local development and deployment:</p> <pre><code># .env.production\nDATABASE_URL=postgresql://user:pass@prod-db:5432/analytics\nS3_BUCKET=prod-data-lake\nSNOWFLAKE_ACCOUNT=xy12345.us-east-1\nSNOWFLAKE_USER=etl_service\nSNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}  # From secrets manager\n</code></pre>"},{"location":"best-practices/production/#secrets-management","title":"Secrets Management","text":"<p>Never commit secrets to git.</p> <p>Use secret managers:</p> <pre><code># AWS Secrets Manager\nexport DB_PASSWORD=$(aws secretsmanager get-secret-value \\\n  --secret-id prod/etlx/db-password \\\n  --query SecretString --output text)\n\n# HashiCorp Vault\nexport DB_PASSWORD=$(vault kv get -field=password secret/etlx/database)\n\n# Google Secret Manager\nexport DB_PASSWORD=$(gcloud secrets versions access latest --secret=db-password)\n</code></pre>"},{"location":"best-practices/production/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"best-practices/production/#structured-logging","title":"Structured Logging","text":"<p>Use JSON output for parsing:</p> <pre><code>etlx run pipeline.yml --json &gt; /var/log/etlx/$(date +%Y%m%d_%H%M%S).json\n</code></pre>"},{"location":"best-practices/production/#metrics-collection","title":"Metrics Collection","text":"<pre><code>#!/bin/bash\n# run_pipeline.sh\n\nSTART_TIME=$(date +%s)\nRESULT=$(etlx run pipeline.yml --json)\nEND_TIME=$(date +%s)\n\n# Extract metrics\nSTATUS=$(echo $RESULT | jq -r '.status')\nDURATION=$(echo $RESULT | jq -r '.duration_ms')\nROWS=$(echo $RESULT | jq -r '.rows_written')\nCHECKS_PASSED=$(echo $RESULT | jq -r '.checks_passed')\nCHECKS_FAILED=$(echo $RESULT | jq -r '.checks_failed')\n\n# Send to monitoring system\nsend_metrics \"etlx.pipeline.duration\" $DURATION\nsend_metrics \"etlx.pipeline.rows\" $ROWS\nsend_metrics \"etlx.pipeline.status\" $([ \"$STATUS\" = \"SUCCESS\" ] &amp;&amp; echo 1 || echo 0)\n</code></pre>"},{"location":"best-practices/production/#datadog-integration","title":"DataDog Integration","text":"<pre><code>from datadog import statsd\nfrom etlx import Pipeline\n\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\n\nwith statsd.timed(\"etlx.pipeline.duration\", tags=[\"pipeline:daily_sales\"]):\n    result = pipeline.run()\n\nstatsd.gauge(\"etlx.pipeline.rows_written\", result.rows_written,\n             tags=[\"pipeline:daily_sales\"])\nstatsd.gauge(\"etlx.pipeline.checks_passed\", result.checks_passed,\n             tags=[\"pipeline:daily_sales\"])\n</code></pre>"},{"location":"best-practices/production/#health-checks","title":"Health Checks","text":"<pre><code># health_check.py\nfrom etlx import ETLXEngine\n\ndef check_backends():\n    \"\"\"Verify backend availability.\"\"\"\n    backends = [\"duckdb\", \"postgres\"]\n    results = {}\n\n    for backend in backends:\n        try:\n            engine = ETLXEngine(backend=backend)\n            results[backend] = \"healthy\"\n        except Exception as e:\n            results[backend] = f\"unhealthy: {e}\"\n\n    return results\n\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(check_backends()))\n</code></pre>"},{"location":"best-practices/production/#scheduling","title":"Scheduling","text":""},{"location":"best-practices/production/#cron","title":"Cron","text":"<pre><code># /etc/cron.d/etlx\n# Daily at 6 AM UTC\n0 6 * * * etlx /opt/etlx/run_pipeline.sh daily_sales &gt;&gt; /var/log/etlx/cron.log 2&gt;&amp;1\n\n# Hourly\n0 * * * * etlx /opt/etlx/run_pipeline.sh hourly_metrics &gt;&gt; /var/log/etlx/cron.log 2&gt;&amp;1\n</code></pre>"},{"location":"best-practices/production/#systemd-timer","title":"Systemd Timer","text":"<pre><code># /etc/systemd/system/etlx-daily.service\n[Unit]\nDescription=ETLX Daily Pipeline\nAfter=network.target\n\n[Service]\nType=oneshot\nUser=etlx\nExecStart=/opt/etlx/run_pipeline.sh daily_sales\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code># /etc/systemd/system/etlx-daily.timer\n[Unit]\nDescription=Run ETLX Daily Pipeline\n\n[Timer]\nOnCalendar=*-*-* 06:00:00\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre> <pre><code>sudo systemctl enable etlx-daily.timer\nsudo systemctl start etlx-daily.timer\n</code></pre>"},{"location":"best-practices/production/#orchestrators","title":"Orchestrators","text":"<p>For complex workflows, use orchestrators:</p> <ul> <li>Airflow: Airflow Integration</li> <li>Prefect: Task-based orchestration</li> <li>Dagster: Software-defined assets</li> </ul>"},{"location":"best-practices/production/#error-handling","title":"Error Handling","text":""},{"location":"best-practices/production/#retry-logic","title":"Retry Logic","text":"<pre><code>#!/bin/bash\n# run_with_retry.sh\n\nMAX_RETRIES=3\nRETRY_DELAY=300  # 5 minutes\n\nfor i in $(seq 1 $MAX_RETRIES); do\n    if etlx run pipeline.yml --var DATE=$1; then\n        echo \"$(date): Pipeline succeeded on attempt $i\"\n        exit 0\n    fi\n\n    if [ $i -lt $MAX_RETRIES ]; then\n        echo \"$(date): Attempt $i failed, retrying in ${RETRY_DELAY}s...\"\n        sleep $RETRY_DELAY\n    fi\ndone\n\necho \"$(date): Pipeline failed after $MAX_RETRIES attempts\"\nexit 1\n</code></pre>"},{"location":"best-practices/production/#alerting","title":"Alerting","text":"<pre><code>#!/bin/bash\n# run_pipeline.sh\n\nPIPELINE_NAME=$1\nDATE=${2:-$(date +%Y-%m-%d)}\n\nif ! etlx run \"pipelines/${PIPELINE_NAME}.yml\" --var DATE=$DATE; then\n    # Send alert\n    curl -X POST \"$SLACK_WEBHOOK\" \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\n            \\\"text\\\": \\\"\ud83d\udea8 Pipeline Failed: ${PIPELINE_NAME}\\\",\n            \\\"attachments\\\": [{\n                \\\"color\\\": \\\"danger\\\",\n                \\\"fields\\\": [\n                    {\\\"title\\\": \\\"Pipeline\\\", \\\"value\\\": \\\"${PIPELINE_NAME}\\\", \\\"short\\\": true},\n                    {\\\"title\\\": \\\"Date\\\", \\\"value\\\": \\\"${DATE}\\\", \\\"short\\\": true}\n                ]\n            }]\n        }\"\n    exit 1\nfi\n</code></pre>"},{"location":"best-practices/production/#pagerduty-for-critical-pipelines","title":"PagerDuty for Critical Pipelines","text":"<pre><code>if ! etlx run critical_pipeline.yml; then\n    curl -X POST https://events.pagerduty.com/v2/enqueue \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\n            \\\"routing_key\\\": \\\"$PAGERDUTY_KEY\\\",\n            \\\"event_action\\\": \\\"trigger\\\",\n            \\\"payload\\\": {\n                \\\"summary\\\": \\\"Critical ETL Pipeline Failed\\\",\n                \\\"severity\\\": \\\"critical\\\",\n                \\\"source\\\": \\\"etlx-prod\\\"\n            }\n        }\"\nfi\n</code></pre>"},{"location":"best-practices/production/#deployment","title":"Deployment","text":""},{"location":"best-practices/production/#docker","title":"Docker","text":"<pre><code># Dockerfile\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install ETLX with required backends\nRUN pip install etlx[duckdb,postgres,snowflake]\n\n# Copy pipelines\nCOPY pipelines/ /app/pipelines/\n\n# Run as non-root user\nRUN useradd -m etlx\nUSER etlx\n\nENTRYPOINT [\"etlx\"]\nCMD [\"--help\"]\n</code></pre> <pre><code># Build and run\ndocker build -t etlx-pipelines .\ndocker run --env-file .env etlx-pipelines run pipelines/daily.yml\n</code></pre>"},{"location":"best-practices/production/#kubernetes","title":"Kubernetes","text":"<pre><code># k8s/etlx-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etlx-daily-sales\nspec:\n  template:\n    spec:\n      containers:\n        - name: etlx\n          image: etlx-pipelines:latest\n          command: [\"etlx\", \"run\", \"pipelines/daily_sales.yml\"]\n          env:\n            - name: DATE\n              value: \"2025-01-15\"\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: etlx-secrets\n                  key: database-url\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2\"\n      restartPolicy: OnFailure\n  backoffLimit: 3\n</code></pre>"},{"location":"best-practices/production/#kubernetes-cronjob","title":"Kubernetes CronJob","text":"<pre><code># k8s/etlx-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etlx-daily-sales\nspec:\n  schedule: \"0 6 * * *\"  # 6 AM daily\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: etlx\n              image: etlx-pipelines:latest\n              command: [\"etlx\", \"run\", \"pipelines/daily_sales.yml\"]\n              envFrom:\n                - secretRef:\n                    name: etlx-secrets\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 3\n</code></pre>"},{"location":"best-practices/production/#resource-management","title":"Resource Management","text":""},{"location":"best-practices/production/#memory-limits","title":"Memory Limits","text":"<p>For large datasets:</p> <pre><code># Limit Python memory\nexport PYTHONMALLOC=malloc\n\n# Use memory-efficient backend\netlx run pipeline.yml --engine polars\n</code></pre>"},{"location":"best-practices/production/#disk-space","title":"Disk Space","text":"<p>Clean up temporary files:</p> <pre><code>#!/bin/bash\n# cleanup.sh\n\n# Remove staging files older than 7 days\nfind /data/staging -type f -mtime +7 -delete\n\n# Remove logs older than 30 days\nfind /var/log/etlx -type f -mtime +30 -delete\n</code></pre>"},{"location":"best-practices/production/#database-connections","title":"Database Connections","text":"<p>Use connection pooling:</p> <pre><code># Use PgBouncer for PostgreSQL\nexport POSTGRES_HOST=pgbouncer.internal\nexport POSTGRES_PORT=6432\n</code></pre>"},{"location":"best-practices/production/#idempotency","title":"Idempotency","text":"<p>Design pipelines to be safely re-runnable:</p> <pre><code># Use replace mode for full refresh\nsink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: replace\n\n# Or use merge for incremental\nsink:\n  type: database\n  connection: postgres\n  table: analytics.daily_metrics\n  mode: merge\n  merge_keys: [date, region]\n</code></pre>"},{"location":"best-practices/production/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"best-practices/production/#backup-before-major-changes","title":"Backup Before Major Changes","text":"<pre><code>#!/bin/bash\n# backup_and_run.sh\n\nTABLE=\"analytics.daily_metrics\"\nBACKUP_TABLE=\"${TABLE}_backup_$(date +%Y%m%d)\"\n\n# Create backup\npsql -c \"CREATE TABLE $BACKUP_TABLE AS SELECT * FROM $TABLE;\"\n\n# Run pipeline\nif ! etlx run pipeline.yml; then\n    echo \"Pipeline failed, restoring from backup...\"\n    psql -c \"TRUNCATE $TABLE; INSERT INTO $TABLE SELECT * FROM $BACKUP_TABLE;\"\n    exit 1\nfi\n\n# Cleanup old backups (keep last 7 days)\npsql -c \"DROP TABLE IF EXISTS ${TABLE}_backup_$(date -d '7 days ago' +%Y%m%d);\"\n</code></pre>"},{"location":"best-practices/production/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production:</p> <ul> <li> All pipelines validated (<code>etlx validate</code>)</li> <li> Environment variables documented</li> <li> Secrets stored in secret manager</li> <li> Monitoring and alerting configured</li> <li> Retry logic implemented</li> <li> Backup strategy defined</li> <li> Resource limits set</li> <li> Runbook documented</li> <li> On-call rotation established</li> </ul>"},{"location":"best-practices/production/#related","title":"Related","text":"<ul> <li>Error Handling - Handle failures gracefully</li> <li>Testing - Test before deploying</li> <li>Airflow Integration - Orchestration</li> </ul>"},{"location":"best-practices/testing/","title":"Testing Best Practices","text":"<p>Strategies for testing ETLX pipelines to ensure data quality and reliability.</p>"},{"location":"best-practices/testing/#testing-pyramid","title":"Testing Pyramid","text":"<pre><code>         /\\\n        /  \\\n       / E2E \\        Integration tests (full pipeline)\n      /--------\\\n     /   Unit   \\     Component tests (transforms, checks)\n    /--------------\\\n   /   Validation   \\  Schema &amp; config validation\n  /------------------\\\n</code></pre>"},{"location":"best-practices/testing/#configuration-validation","title":"Configuration Validation","text":""},{"location":"best-practices/testing/#validate-before-running","title":"Validate Before Running","text":"<pre><code># Always validate first\netlx validate pipeline.yml\n\n# Validate all pipelines\nfor f in pipelines/*.yml; do\n  etlx validate \"$f\" || exit 1\ndone\n</code></pre>"},{"location":"best-practices/testing/#cicd-validation","title":"CI/CD Validation","text":"<pre><code># .github/workflows/validate.yml\nname: Validate Pipelines\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install ETLX\n        run: pip install etlx[duckdb]\n\n      - name: Validate all pipelines\n        run: |\n          for f in pipelines/*.yml; do\n            echo \"Validating $f...\"\n            etlx validate \"$f\"\n          done\n</code></pre>"},{"location":"best-practices/testing/#sample-data-testing","title":"Sample Data Testing","text":""},{"location":"best-practices/testing/#create-test-fixtures","title":"Create Test Fixtures","text":"<p>Create representative test data:</p> <pre><code>tests/\n\u251c\u2500\u2500 fixtures/\n\u2502   \u251c\u2500\u2500 orders_valid.csv\n\u2502   \u251c\u2500\u2500 orders_with_nulls.csv\n\u2502   \u251c\u2500\u2500 orders_empty.csv\n\u2502   \u2514\u2500\u2500 orders_duplicates.csv\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 test_pipeline.yml\n</code></pre>"},{"location":"best-practices/testing/#test-fixture-valid-data","title":"Test Fixture: Valid Data","text":"<p><code>tests/fixtures/orders_valid.csv</code>:</p> <pre><code>id,customer_id,amount,status,date\n1,C001,99.99,completed,2025-01-15\n2,C002,149.99,completed,2025-01-15\n3,C003,49.99,completed,2025-01-16\n</code></pre>"},{"location":"best-practices/testing/#test-fixture-edge-cases","title":"Test Fixture: Edge Cases","text":"<p><code>tests/fixtures/orders_with_nulls.csv</code>:</p> <pre><code>id,customer_id,amount,status,date\n1,C001,99.99,completed,2025-01-15\n2,,149.99,completed,2025-01-15\n3,C003,,pending,2025-01-16\n4,C004,49.99,,2025-01-16\n</code></pre>"},{"location":"best-practices/testing/#test-pipeline","title":"Test Pipeline","text":"<p><code>tests/pipelines/test_pipeline.yml</code>:</p> <pre><code>name: test_pipeline\nengine: duckdb\n\nsource:\n  type: file\n  path: ${TEST_DATA_PATH}\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: status = 'completed'\n  - op: derive_column\n    name: amount_with_tax\n    expr: amount * 1.1\n\nchecks:\n  - check: not_null\n    columns: [id, customer_id, amount]\n  - check: unique\n    columns: [id]\n\nsink:\n  type: file\n  path: ${OUTPUT_PATH}\n  format: parquet\n</code></pre>"},{"location":"best-practices/testing/#unit-tests-with-pytest","title":"Unit Tests with pytest","text":""},{"location":"best-practices/testing/#test-setup","title":"Test Setup","text":"<pre><code># tests/conftest.py\nimport pytest\nimport tempfile\nimport os\nfrom pathlib import Path\n\n@pytest.fixture\ndef test_data_dir():\n    return Path(__file__).parent / \"fixtures\"\n\n@pytest.fixture\ndef temp_output_dir():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield Path(tmpdir)\n\n@pytest.fixture\ndef sample_orders(test_data_dir):\n    return test_data_dir / \"orders_valid.csv\"\n</code></pre>"},{"location":"best-practices/testing/#test-pipeline-execution","title":"Test Pipeline Execution","text":"<pre><code># tests/test_pipelines.py\nimport pytest\nfrom etlx import Pipeline\n\ndef test_basic_pipeline_runs(sample_orders, temp_output_dir):\n    \"\"\"Test that pipeline executes successfully.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    result = pipeline.run(variables={\n        \"INPUT_PATH\": str(sample_orders),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n    })\n\n    assert result.status == \"SUCCESS\"\n    assert result.rows_written &gt; 0\n\ndef test_pipeline_filters_correctly(sample_orders, temp_output_dir):\n    \"\"\"Test that filter transform works correctly.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    result = pipeline.run(variables={\n        \"INPUT_PATH\": str(sample_orders),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n    })\n\n    # Read output and verify filter worked\n    import pandas as pd\n    output = pd.read_parquet(temp_output_dir / \"output.parquet\")\n\n    assert all(output[\"status\"] == \"completed\")\n\ndef test_quality_checks_pass(sample_orders, temp_output_dir):\n    \"\"\"Test that quality checks pass for valid data.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    result = pipeline.run(variables={\n        \"INPUT_PATH\": str(sample_orders),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n    })\n\n    assert result.checks_failed == 0\n    assert result.checks_passed &gt; 0\n</code></pre>"},{"location":"best-practices/testing/#test-edge-cases","title":"Test Edge Cases","text":"<pre><code># tests/test_edge_cases.py\nimport pytest\nfrom etlx import Pipeline\nfrom etlx.exceptions import QualityCheckError\n\ndef test_empty_input_file(test_data_dir, temp_output_dir):\n    \"\"\"Test handling of empty input file.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    result = pipeline.run(\n        variables={\n            \"INPUT_PATH\": str(test_data_dir / \"orders_empty.csv\"),\n            \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n        },\n        fail_on_checks=False\n    )\n\n    # row_count check should fail for empty file\n    assert result.checks_failed &gt; 0\n\ndef test_null_handling(test_data_dir, temp_output_dir):\n    \"\"\"Test that NULL values are handled correctly.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    with pytest.raises(QualityCheckError):\n        pipeline.run(\n            variables={\n                \"INPUT_PATH\": str(test_data_dir / \"orders_with_nulls.csv\"),\n                \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n            },\n            fail_on_checks=True\n        )\n\ndef test_duplicate_handling(test_data_dir, temp_output_dir):\n    \"\"\"Test deduplication logic.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/dedup_orders.yml\")\n\n    result = pipeline.run(variables={\n        \"INPUT_PATH\": str(test_data_dir / \"orders_duplicates.csv\"),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n    })\n\n    import pandas as pd\n    output = pd.read_parquet(temp_output_dir / \"output.parquet\")\n\n    # Verify no duplicates in output\n    assert output[\"id\"].is_unique\n</code></pre>"},{"location":"best-practices/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"best-practices/testing/#full-pipeline-test","title":"Full Pipeline Test","text":"<pre><code># tests/test_integration.py\nimport pytest\nfrom etlx import Pipeline\n\n@pytest.mark.integration\ndef test_full_etl_pipeline(test_data_dir, temp_output_dir):\n    \"\"\"Test complete ETL workflow.\"\"\"\n    # Step 1: Extract\n    extract = Pipeline.from_yaml(\"pipelines/extract.yml\")\n    extract_result = extract.run(variables={\n        \"INPUT_PATH\": str(test_data_dir / \"source_data\"),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"staging\")\n    })\n    assert extract_result.status == \"SUCCESS\"\n\n    # Step 2: Transform\n    transform = Pipeline.from_yaml(\"pipelines/transform.yml\")\n    transform_result = transform.run(variables={\n        \"INPUT_PATH\": str(temp_output_dir / \"staging\"),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"processed\")\n    })\n    assert transform_result.status == \"SUCCESS\"\n\n    # Step 3: Validate output\n    import pandas as pd\n    output = pd.read_parquet(temp_output_dir / \"processed\" / \"data.parquet\")\n\n    # Assertions on final output\n    assert len(output) &gt; 0\n    assert \"calculated_field\" in output.columns\n    assert output[\"calculated_field\"].notna().all()\n</code></pre>"},{"location":"best-practices/testing/#schema-testing","title":"Schema Testing","text":""},{"location":"best-practices/testing/#verify-output-schema","title":"Verify Output Schema","text":"<pre><code>def test_output_schema(sample_orders, temp_output_dir):\n    \"\"\"Verify output has expected columns and types.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/orders.yml\")\n\n    result = pipeline.run(variables={\n        \"INPUT_PATH\": str(sample_orders),\n        \"OUTPUT_PATH\": str(temp_output_dir / \"output.parquet\")\n    })\n\n    import pandas as pd\n    output = pd.read_parquet(temp_output_dir / \"output.parquet\")\n\n    # Check expected columns exist\n    expected_columns = [\"id\", \"customer_id\", \"amount\", \"amount_with_tax\"]\n    assert all(col in output.columns for col in expected_columns)\n\n    # Check data types\n    assert output[\"id\"].dtype == \"int64\"\n    assert output[\"amount\"].dtype == \"float64\"\n</code></pre>"},{"location":"best-practices/testing/#test-automation","title":"Test Automation","text":""},{"location":"best-practices/testing/#pytest-configuration","title":"pytest Configuration","text":"<pre><code># pytest.ini\n[pytest]\ntestpaths = tests\npython_files = test_*.py\npython_functions = test_*\nmarkers =\n    integration: marks tests as integration tests\n    slow: marks tests as slow\n</code></pre>"},{"location":"best-practices/testing/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run only unit tests (exclude integration)\npytest -m \"not integration\"\n\n# Run with coverage\npytest --cov=etlx --cov-report=html\n\n# Run verbose\npytest -v\n</code></pre>"},{"location":"best-practices/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":""},{"location":"best-practices/testing/#setup","title":"Setup","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: validate-pipelines\n        name: Validate ETLX Pipelines\n        entry: bash -c 'for f in pipelines/*.yml; do etlx validate \"$f\" || exit 1; done'\n        language: system\n        files: \\.yml$\n        pass_filenames: false\n</code></pre>"},{"location":"best-practices/testing/#install","title":"Install","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"best-practices/testing/#test-data-generation","title":"Test Data Generation","text":""},{"location":"best-practices/testing/#generate-realistic-test-data","title":"Generate Realistic Test Data","text":"<pre><code># scripts/generate_test_data.py\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef generate_orders(n=1000):\n    np.random.seed(42)\n\n    return pd.DataFrame({\n        \"id\": range(1, n + 1),\n        \"customer_id\": [f\"C{i:04d}\" for i in np.random.randint(1, 100, n)],\n        \"amount\": np.random.uniform(10, 500, n).round(2),\n        \"status\": np.random.choice(\n            [\"completed\", \"pending\", \"cancelled\"],\n            n,\n            p=[0.8, 0.15, 0.05]\n        ),\n        \"date\": [\n            (datetime(2025, 1, 1) + timedelta(days=int(d))).strftime(\"%Y-%m-%d\")\n            for d in np.random.randint(0, 30, n)\n        ]\n    })\n\nif __name__ == \"__main__\":\n    orders = generate_orders(1000)\n    orders.to_csv(\"tests/fixtures/orders_generated.csv\", index=False)\n</code></pre>"},{"location":"best-practices/testing/#related","title":"Related","text":"<ul> <li>Error Handling - Handle test failures</li> <li>CI/CD Integration - Automated testing</li> <li>Quality Checks - Data validation</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>ETLX provides a command-line interface for running and managing pipelines.</p>"},{"location":"cli/#commands","title":"Commands","text":"Command Description <code>run</code> Execute a pipeline <code>validate</code> Validate configuration <code>init</code> Create new project or pipeline <code>info</code> Display version and backend info <code>schema</code> Output JSON schema"},{"location":"cli/#global-options","title":"Global Options","text":"<pre><code>etlx --version    # Show version\netlx --help       # Show help\n</code></pre>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<pre><code># Create project with sample data\netlx init my_project\ncd my_project\n\n# Run the sample pipeline\netlx run pipelines/sample.yml\n\n# Validate without running\netlx validate pipelines/sample.yml\n</code></pre>"},{"location":"cli/#common-usage","title":"Common Usage","text":""},{"location":"cli/#run-pipeline","title":"Run Pipeline","text":"<pre><code>etlx run pipeline.yml\netlx run pipeline.yml --var DATE=2025-01-15\netlx run pipeline.yml --engine polars\netlx run pipeline.yml --dry-run\n</code></pre>"},{"location":"cli/#validate-configuration","title":"Validate Configuration","text":"<pre><code>etlx validate pipeline.yml\netlx validate pipeline.yml --verbose\n</code></pre>"},{"location":"cli/#check-backends","title":"Check Backends","text":"<pre><code>etlx info --backends --check\n</code></pre>"},{"location":"cli/#generate-schema","title":"Generate Schema","text":"<pre><code>etlx schema -o .etlx-schema.json\n</code></pre>"},{"location":"cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error (validation, execution, etc.) 2 Command not found"},{"location":"cli/#shell-completion","title":"Shell Completion","text":"<p>Enable tab completion:</p> <pre><code># Bash\netlx --install-completion bash\n\n# Zsh\netlx --install-completion zsh\n\n# Fish\netlx --install-completion fish\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>The CLI respects environment variables for configuration:</p> <pre><code>export DATABASE_URL=postgresql://localhost/db\netlx run pipeline.yml\n</code></pre> <p>See Environment Variables for details.</p>"},{"location":"cli/info/","title":"etlx info","text":"<p>Display ETLX version and backend information.</p>"},{"location":"cli/info/#usage","title":"Usage","text":"<pre><code>etlx info [options]\n</code></pre>"},{"location":"cli/info/#options","title":"Options","text":"Option Short Description <code>--backends</code> <code>-b</code> Show available backends <code>--check</code> <code>-c</code> Check backend availability"},{"location":"cli/info/#examples","title":"Examples","text":""},{"location":"cli/info/#version-info","title":"Version Info","text":"<pre><code>etlx info\n</code></pre> <p>Output:</p> <pre><code>ETLX v0.1.0\nPython 3.12.0\n</code></pre>"},{"location":"cli/info/#list-backends","title":"List Backends","text":"<pre><code>etlx info --backends\n</code></pre>"},{"location":"cli/info/#check-backend-availability","title":"Check Backend Availability","text":"<pre><code>etlx info --backends --check\n</code></pre> <p>Output:</p> <pre><code>Available Backends\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend    \u2503 Name            \u2503 Description                  \u2503 Status         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 duckdb     \u2502 DuckDB          \u2502 Fast in-process database     \u2502 OK             \u2502\n\u2502 polars     \u2502 Polars          \u2502 Rust-powered DataFrames      \u2502 OK             \u2502\n\u2502 spark      \u2502 Apache Spark    \u2502 Distributed compute          \u2502 Not installed  \u2502\n\u2502 snowflake  \u2502 Snowflake       \u2502 Cloud data warehouse         \u2502 Not installed  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/info/#use-cases","title":"Use Cases","text":""},{"location":"cli/info/#verify-installation","title":"Verify Installation","text":"<pre><code>etlx info --backends --check\n</code></pre>"},{"location":"cli/info/#scripting","title":"Scripting","text":"<pre><code>if etlx info --backends | grep -q \"spark.*OK\"; then\n  echo \"Spark is available\"\nfi\n</code></pre>"},{"location":"cli/info/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success"},{"location":"cli/info/#related","title":"Related","text":"<ul> <li>Installation - Install backends</li> <li>Backends - Backend details</li> </ul>"},{"location":"cli/init/","title":"etlx init","text":"<p>Initialize a new ETLX project or pipeline file.</p>"},{"location":"cli/init/#usage","title":"Usage","text":"<pre><code>etlx init &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/init/#arguments","title":"Arguments","text":"Argument Required Description <code>name</code> Yes Project or pipeline name"},{"location":"cli/init/#options","title":"Options","text":"Option Short Description <code>--pipeline</code> <code>-p</code> Create pipeline file only (not full project) <code>--output</code> <code>-o</code> Output directory (default: current) <code>--force</code> <code>-f</code> Overwrite existing files"},{"location":"cli/init/#examples","title":"Examples","text":""},{"location":"cli/init/#create-project","title":"Create Project","text":"<pre><code>etlx init my_project\ncd my_project\n</code></pre> <p>Creates:</p> <pre><code>my_project/\n\u251c\u2500\u2500 pipelines/\n\u2502   \u2514\u2500\u2500 sample.yml      # Working sample pipeline\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 sales.csv       # Sample data\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 .gitignore\n</code></pre>"},{"location":"cli/init/#create-pipeline-only","title":"Create Pipeline Only","text":"<pre><code>etlx init my_pipeline -p\n</code></pre> <p>Creates: <code>my_pipeline.yml</code></p>"},{"location":"cli/init/#specify-output-directory","title":"Specify Output Directory","text":"<pre><code>etlx init my_project -o ./projects/\n</code></pre>"},{"location":"cli/init/#force-overwrite","title":"Force Overwrite","text":"<pre><code>etlx init my_project --force\n</code></pre>"},{"location":"cli/init/#project-contents","title":"Project Contents","text":""},{"location":"cli/init/#sample-pipeline","title":"Sample Pipeline","text":"<p>The generated <code>sample.yml</code> is runnable immediately:</p> <pre><code>cd my_project\netlx run pipelines/sample.yml\n</code></pre>"},{"location":"cli/init/#sample-data","title":"Sample Data","text":"<p><code>data/sales.csv</code> contains sample records for testing.</p>"},{"location":"cli/init/#environment-template","title":"Environment Template","text":"<p><code>.env</code> contains placeholder environment variables.</p>"},{"location":"cli/init/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error (directory exists, etc.)"},{"location":"cli/init/#related","title":"Related","text":"<ul> <li>Quick Start - Using generated project</li> <li>Project Structure - Organization best practices</li> </ul>"},{"location":"cli/run/","title":"etlx run","text":"<p>Execute a pipeline from a YAML configuration file.</p>"},{"location":"cli/run/#usage","title":"Usage","text":"<pre><code>etlx run &lt;config_file&gt; [options]\n</code></pre>"},{"location":"cli/run/#arguments","title":"Arguments","text":"Argument Required Description <code>config_file</code> Yes Path to pipeline YAML file"},{"location":"cli/run/#options","title":"Options","text":"Option Short Description <code>--engine</code> <code>-e</code> Override compute engine <code>--var</code> <code>-v</code> Set variable (KEY=VALUE) <code>--dry-run</code> Execute without writing output <code>--fail-on-checks</code> Fail on quality check failure (default) <code>--no-fail-on-checks</code> Continue despite check failures <code>--verbose</code> <code>-V</code> Enable verbose logging <code>--json</code> <code>-j</code> Output result as JSON"},{"location":"cli/run/#examples","title":"Examples","text":""},{"location":"cli/run/#basic-run","title":"Basic Run","text":"<pre><code>etlx run pipeline.yml\n</code></pre>"},{"location":"cli/run/#with-variables","title":"With Variables","text":"<pre><code>etlx run pipeline.yml --var DATE=2025-01-15\netlx run pipeline.yml --var DATE=2025-01-15 --var REGION=north\n</code></pre>"},{"location":"cli/run/#override-engine","title":"Override Engine","text":"<pre><code>etlx run pipeline.yml --engine polars\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"cli/run/#dry-run","title":"Dry Run","text":"<p>Execute transforms without writing to sink:</p> <pre><code>etlx run pipeline.yml --dry-run\n</code></pre>"},{"location":"cli/run/#continue-on-check-failure","title":"Continue on Check Failure","text":"<pre><code>etlx run pipeline.yml --no-fail-on-checks\n</code></pre>"},{"location":"cli/run/#json-output","title":"JSON Output","text":"<p>For scripting and automation:</p> <pre><code>etlx run pipeline.yml --json\n</code></pre> <p>Output:</p> <pre><code>{\n  \"pipeline_name\": \"sales_etl\",\n  \"status\": \"SUCCESS\",\n  \"duration_ms\": 245.3,\n  \"rows_processed\": 1000,\n  \"rows_written\": 950,\n  \"checks_passed\": 3,\n  \"checks_failed\": 0\n}\n</code></pre>"},{"location":"cli/run/#verbose-logging","title":"Verbose Logging","text":"<pre><code>etlx run pipeline.yml --verbose\n</code></pre>"},{"location":"cli/run/#output","title":"Output","text":"<p>Successful run:</p> <pre><code>Running pipeline: sales_etl\n  Process daily sales data\n  Engine: duckdb\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pipeline: sales_etl \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 SUCCESS                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duration: 245.3ms \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nSteps\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Step              \u2503 Type          \u2503 Status \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 read_source       \u2502 file          \u2502 OK     \u2502   45.2ms \u2502\n\u2502 transform_0       \u2502 filter        \u2502 OK     \u2502    0.3ms \u2502\n\u2502 quality_checks    \u2502 checks        \u2502 OK     \u2502   12.4ms \u2502\n\u2502 write_sink        \u2502 file          \u2502 OK     \u2502    8.1ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nQuality Checks: PASSED (2/2 passed)\n\nRows processed: 1000\nRows written: 950\n</code></pre>"},{"location":"cli/run/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Pipeline failed"},{"location":"cli/run/#related","title":"Related","text":"<ul> <li>validate - Validate without running</li> <li>Pipeline YAML - Configuration reference</li> <li>Variable Substitution - Using <code>--var</code></li> </ul>"},{"location":"cli/schema/","title":"etlx schema","text":"<p>Output JSON schema for pipeline configuration.</p>"},{"location":"cli/schema/#usage","title":"Usage","text":"<pre><code>etlx schema [options]\n</code></pre>"},{"location":"cli/schema/#options","title":"Options","text":"Option Short Description <code>--output</code> <code>-o</code> Output file path (default: stdout) <code>--indent</code> <code>-i</code> JSON indentation level (default: 2)"},{"location":"cli/schema/#examples","title":"Examples","text":""},{"location":"cli/schema/#output-to-stdout","title":"Output to Stdout","text":"<pre><code>etlx schema\n</code></pre>"},{"location":"cli/schema/#save-to-file","title":"Save to File","text":"<pre><code>etlx schema -o .etlx-schema.json\n</code></pre>"},{"location":"cli/schema/#custom-indentation","title":"Custom Indentation","text":"<pre><code>etlx schema -o schema.json --indent 4\n</code></pre>"},{"location":"cli/schema/#use-cases","title":"Use Cases","text":""},{"location":"cli/schema/#vs-code-integration","title":"VS Code Integration","text":"<pre><code>etlx schema -o .etlx-schema.json\n</code></pre> <p>Then in <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"yaml.schemas\": {\n    \".etlx-schema.json\": [\"pipelines/*.yml\"]\n  }\n}\n</code></pre>"},{"location":"cli/schema/#generate-for-distribution","title":"Generate for Distribution","text":"<pre><code>etlx schema -o docs/schema.json --indent 2\n</code></pre>"},{"location":"cli/schema/#output-format","title":"Output Format","text":"<p>The schema follows JSON Schema draft-07:</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"ETLX Pipeline Configuration\",\n  \"type\": \"object\",\n  \"required\": [\"name\", \"source\", \"sink\"],\n  \"properties\": {\n    \"name\": { \"type\": \"string\" },\n    \"engine\": { \"enum\": [\"duckdb\", \"polars\", ...] },\n    ...\n  }\n}\n</code></pre>"},{"location":"cli/schema/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error"},{"location":"cli/schema/#related","title":"Related","text":"<ul> <li>JSON Schema for IDEs - IDE setup guide</li> </ul>"},{"location":"cli/validate/","title":"etlx validate","text":"<p>Validate a pipeline configuration without executing it.</p>"},{"location":"cli/validate/#usage","title":"Usage","text":"<pre><code>etlx validate &lt;config_file&gt; [options]\n</code></pre>"},{"location":"cli/validate/#arguments","title":"Arguments","text":"Argument Required Description <code>config_file</code> Yes Path to pipeline YAML file"},{"location":"cli/validate/#options","title":"Options","text":"Option Short Description <code>--verbose</code> <code>-v</code> Show detailed configuration"},{"location":"cli/validate/#examples","title":"Examples","text":""},{"location":"cli/validate/#basic-validation","title":"Basic Validation","text":"<pre><code>etlx validate pipeline.yml\n</code></pre>"},{"location":"cli/validate/#verbose-output","title":"Verbose Output","text":"<pre><code>etlx validate pipeline.yml --verbose\n</code></pre>"},{"location":"cli/validate/#output","title":"Output","text":""},{"location":"cli/validate/#valid-configuration","title":"Valid Configuration","text":"<pre><code>Configuration is valid\n\nPipeline: sales_etl\n  Engine: duckdb\n  Source: file (data/sales.parquet)\n  Transforms: 3\n  Checks: 2\n  Sink: file (output/results.parquet)\n</code></pre>"},{"location":"cli/validate/#invalid-configuration","title":"Invalid Configuration","text":"<pre><code>Configuration is invalid\n\nErrors:\n  - transforms -&gt; 0 -&gt; op: Input should be 'select', 'filter', ...\n    [input_value='invalid_op']\n  - sink: Field required\n</code></pre>"},{"location":"cli/validate/#use-cases","title":"Use Cases","text":""},{"location":"cli/validate/#cicd-validation","title":"CI/CD Validation","text":"<pre><code># .github/workflows/validate.yml\n- name: Validate pipelines\n  run: |\n    for f in pipelines/*.yml; do\n      etlx validate \"$f\"\n    done\n</code></pre>"},{"location":"cli/validate/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code>#!/bin/bash\n# .git/hooks/pre-commit\n\nfor file in $(git diff --cached --name-only | grep 'pipelines/.*\\.yml$'); do\n  etlx validate \"$file\" || exit 1\ndone\n</code></pre>"},{"location":"cli/validate/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Configuration valid 1 Configuration invalid"},{"location":"cli/validate/#related","title":"Related","text":"<ul> <li>run - Execute validated pipeline</li> <li>Pipeline YAML - Configuration reference</li> </ul>"},{"location":"development/","title":"Development","text":"<p>Resources for contributing to ETLX development.</p>"},{"location":"development/#getting-started","title":"Getting Started","text":"<ul> <li>Contributing Guide - How to contribute</li> <li>Architecture - System design</li> <li>Changelog - Version history</li> </ul>"},{"location":"development/#quick-links","title":"Quick Links","text":""},{"location":"development/#contributing","title":"Contributing","text":"<p>Want to contribute? Start here:</p> <ol> <li>Read the Contributing Guide</li> <li>Set up your development environment</li> <li>Find an issue to work on</li> <li>Submit a pull request</li> </ol>"},{"location":"development/#architecture","title":"Architecture","text":"<p>Understand how ETLX works:</p> <ul> <li>Core abstractions</li> <li>Backend system</li> <li>Transform pipeline</li> <li>Quality framework</li> </ul> <p>Architecture Overview \u2192</p>"},{"location":"development/#version-history","title":"Version History","text":"<p>See what's changed in each release:</p> <p>Changelog \u2192</p>"},{"location":"development/architecture/","title":"Architecture","text":"<p>This document describes the internal architecture of ETLX.</p>"},{"location":"development/architecture/#overview","title":"Overview","text":"<p>ETLX follows a layered architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  CLI                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              Pipeline API                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Configuration Layer              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            Engine Layer                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          Ibis Abstraction                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    DuckDB \u2502 Polars \u2502 Spark \u2502 ...        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/architecture/#core-components","title":"Core Components","text":""},{"location":"development/architecture/#configuration-system","title":"Configuration System","text":"<p>Pydantic models for type-safe configuration:</p> <pre><code># etlx/config/models.py\nclass PipelineConfig(BaseModel):\n    name: str\n    description: str | None = None\n    engine: str = \"duckdb\"\n    source: SourceConfig\n    transforms: list[TransformConfig] = []\n    checks: list[CheckConfig] = []\n    sink: SinkConfig\n</code></pre> <p>Discriminated unions for transform types:</p> <pre><code>TransformConfig = Annotated[\n    SelectTransform | FilterTransform | DeriveColumnTransform | ...,\n    Field(discriminator=\"op\")\n]\n</code></pre>"},{"location":"development/architecture/#engine-layer","title":"Engine Layer","text":"<p>The engine orchestrates pipeline execution:</p> <pre><code># etlx/engine/engine.py\nclass ETLXEngine:\n    def __init__(self, backend: str = \"duckdb\"):\n        self.backend = backend\n        self.connection = self._create_connection()\n\n    def execute(self, config: PipelineConfig) -&gt; ExecutionResult:\n        table = self.read_source(config.source)\n        table = self.apply_transforms(table, config.transforms)\n        check_results = self.run_checks(table, config.checks)\n        self.write_sink(table, config.sink)\n        return ExecutionResult(...)\n</code></pre>"},{"location":"development/architecture/#transform-system","title":"Transform System","text":"<p>Each transform is a class implementing a common interface:</p> <pre><code># etlx/transforms/base.py\nclass BaseTransform(ABC):\n    @abstractmethod\n    def apply(self, table: Table) -&gt; Table:\n        \"\"\"Apply transform to table.\"\"\"\n        pass\n\n# etlx/transforms/filter.py\nclass FilterTransform(BaseTransform):\n    def __init__(self, predicate: str):\n        self.predicate = predicate\n\n    def apply(self, table: Table) -&gt; Table:\n        return table.filter(self.predicate)\n</code></pre>"},{"location":"development/architecture/#quality-framework","title":"Quality Framework","text":"<p>Quality checks follow the same pattern:</p> <pre><code># etlx/quality/base.py\nclass BaseCheck(ABC):\n    @abstractmethod\n    def run(self, table: Table, engine: ETLXEngine) -&gt; CheckResult:\n        \"\"\"Execute quality check.\"\"\"\n        pass\n\n# etlx/quality/not_null.py\nclass NotNullCheck(BaseCheck):\n    def __init__(self, columns: list[str]):\n        self.columns = columns\n\n    def run(self, table: Table, engine: ETLXEngine) -&gt; CheckResult:\n        null_counts = {}\n        for col in self.columns:\n            null_count = table.filter(f\"{col} IS NULL\").count().execute()\n            null_counts[col] = null_count\n\n        passed = all(c == 0 for c in null_counts.values())\n        return CheckResult(passed=passed, details=null_counts)\n</code></pre>"},{"location":"development/architecture/#backend-abstraction","title":"Backend Abstraction","text":"<p>Ibis provides the backend abstraction layer:</p> <pre><code># etlx/backends/factory.py\ndef create_connection(backend: str) -&gt; ibis.BaseBackend:\n    if backend == \"duckdb\":\n        return ibis.duckdb.connect()\n    elif backend == \"polars\":\n        return ibis.polars.connect()\n    elif backend == \"spark\":\n        return ibis.spark.connect()\n    # ... etc\n</code></pre>"},{"location":"development/architecture/#data-flow","title":"Data Flow","text":"<pre><code>YAML Config\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Parse YAML  \u2502 \u2500\u2192 PipelineConfig (Pydantic)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Read Source \u2502 \u2500\u2192 Ibis Table Expression\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Transforms  \u2502 \u2500\u2192 Ibis Table Expression (transformed)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checks    \u2502 \u2500\u2192 CheckResults\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Write Sink  \u2502 \u2500\u2192 Output (file/database)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/architecture/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"development/architecture/#why-ibis","title":"Why Ibis?","text":"<p>Ibis provides:</p> <ul> <li>Unified API across 15+ backends</li> <li>Lazy evaluation (query optimization)</li> <li>Familiar pandas-like syntax</li> <li>SQL expression support</li> </ul>"},{"location":"development/architecture/#why-pydantic","title":"Why Pydantic?","text":"<p>Pydantic provides:</p> <ul> <li>Runtime type validation</li> <li>Clear error messages</li> <li>JSON Schema generation</li> <li>IDE autocomplete</li> </ul>"},{"location":"development/architecture/#why-yaml","title":"Why YAML?","text":"<p>YAML provides:</p> <ul> <li>Human-readable configuration</li> <li>Comment support</li> <li>Version control friendly</li> <li>Easy environment variable substitution</li> </ul>"},{"location":"development/architecture/#extension-points","title":"Extension Points","text":""},{"location":"development/architecture/#adding-a-new-transform","title":"Adding a New Transform","text":"<ol> <li>Create transform class:</li> </ol> <pre><code># etlx/transforms/my_transform.py\nclass MyTransform(BaseTransform):\n    op: Literal[\"my_transform\"] = \"my_transform\"\n    param: str\n\n    def apply(self, table: Table) -&gt; Table:\n        # Implementation\n        return table\n</code></pre> <ol> <li>Register in discriminated union:</li> </ol> <pre><code># etlx/config/transforms.py\nTransformConfig = Annotated[\n    ... | MyTransform,\n    Field(discriminator=\"op\")\n]\n</code></pre>"},{"location":"development/architecture/#adding-a-new-backend","title":"Adding a New Backend","text":"<ol> <li>Implement connection factory:</li> </ol> <pre><code># etlx/backends/my_backend.py\ndef create_my_backend_connection(**options):\n    return ibis.my_backend.connect(**options)\n</code></pre> <ol> <li>Register in backend factory:</li> </ol> <pre><code># etlx/backends/factory.py\nBACKENDS[\"my_backend\"] = create_my_backend_connection\n</code></pre>"},{"location":"development/architecture/#adding-a-new-check","title":"Adding a New Check","text":"<ol> <li>Create check class:</li> </ol> <pre><code># etlx/quality/my_check.py\nclass MyCheck(BaseCheck):\n    check: Literal[\"my_check\"] = \"my_check\"\n\n    def run(self, table: Table, engine: ETLXEngine) -&gt; CheckResult:\n        # Implementation\n        return CheckResult(...)\n</code></pre> <ol> <li>Register in discriminated union.</li> </ol>"},{"location":"development/architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>src/etlx/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 cli/                 # Command-line interface\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 run.py\n\u2502   \u251c\u2500\u2500 validate.py\n\u2502   \u2514\u2500\u2500 init.py\n\u251c\u2500\u2500 config/              # Configuration models\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models.py\n\u2502   \u251c\u2500\u2500 sources.py\n\u2502   \u251c\u2500\u2500 sinks.py\n\u2502   \u2514\u2500\u2500 transforms.py\n\u251c\u2500\u2500 engine/              # Execution engine\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u2514\u2500\u2500 result.py\n\u251c\u2500\u2500 transforms/          # Transform implementations\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 select.py\n\u2502   \u251c\u2500\u2500 filter.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 quality/             # Quality checks\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 not_null.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 backends/            # Backend factory\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 factory.py\n\u2514\u2500\u2500 io/                  # I/O handlers\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 readers.py\n    \u2514\u2500\u2500 writers.py\n</code></pre>"},{"location":"development/architecture/#related","title":"Related","text":"<ul> <li>Contributing Guide - How to contribute</li> <li>API Reference - API documentation</li> </ul>"},{"location":"development/changelog/","title":"Changelog","text":"<p>All notable changes to ETLX will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"development/changelog/#unreleased","title":"Unreleased","text":""},{"location":"development/changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation site</li> </ul>"},{"location":"development/changelog/#010-2025-01-15","title":"0.1.0 - 2025-01-15","text":""},{"location":"development/changelog/#added_1","title":"Added","text":""},{"location":"development/changelog/#core-features","title":"Core Features","text":"<ul> <li>YAML-based pipeline configuration</li> <li>Multiple compute backends via Ibis (DuckDB, Polars, Spark, Snowflake, BigQuery, PostgreSQL, MySQL, ClickHouse, DataFusion, Pandas)</li> <li>12 transform operations:</li> <li><code>select</code> - Column selection</li> <li><code>rename</code> - Column renaming</li> <li><code>filter</code> - Row filtering</li> <li><code>derive_column</code> - Calculated columns</li> <li><code>cast</code> - Type conversion</li> <li><code>fill_null</code> - NULL handling</li> <li><code>dedup</code> - Deduplication</li> <li><code>sort</code> - Row ordering</li> <li><code>join</code> - Data combining</li> <li><code>aggregate</code> - Grouping and summarization</li> <li><code>union</code> - Dataset concatenation</li> <li><code>limit</code> - Row limiting</li> </ul>"},{"location":"development/changelog/#quality-checks","title":"Quality Checks","text":"<ul> <li><code>not_null</code> - NULL value detection</li> <li><code>unique</code> - Uniqueness validation</li> <li><code>row_count</code> - Row count bounds</li> <li><code>accepted_values</code> - Value enumeration</li> <li><code>expression</code> - Custom SQL expressions with thresholds</li> </ul>"},{"location":"development/changelog/#io-support","title":"I/O Support","text":"<ul> <li>File sources: CSV, Parquet, JSON, Excel</li> <li>Database sources: PostgreSQL, MySQL, Snowflake, BigQuery</li> <li>Cloud storage: S3, GCS, Azure Blob</li> <li>All sources supported as sinks</li> </ul>"},{"location":"development/changelog/#cli","title":"CLI","text":"<ul> <li><code>etlx run</code> - Execute pipelines</li> <li><code>etlx validate</code> - Configuration validation</li> <li><code>etlx init</code> - Project scaffolding</li> <li><code>etlx info</code> - Version and backend information</li> <li><code>etlx schema</code> - JSON schema generation</li> </ul>"},{"location":"development/changelog/#python-api","title":"Python API","text":"<ul> <li><code>Pipeline</code> class for programmatic execution</li> <li><code>ETLXEngine</code> for low-level control</li> <li>Pydantic configuration models</li> <li>Quality check classes</li> </ul>"},{"location":"development/changelog/#integrations","title":"Integrations","text":"<ul> <li>Apache Airflow operator and decorator</li> <li>Environment variable substitution</li> <li>JSON output for monitoring</li> </ul>"},{"location":"development/changelog/#developer-experience","title":"Developer Experience","text":"<ul> <li>JSON Schema for IDE autocomplete</li> <li>Verbose logging mode</li> <li>Dry run support</li> <li>Sample project generation</li> </ul>"},{"location":"development/changelog/#future-roadmap","title":"Future Roadmap","text":""},{"location":"development/changelog/#planned-for-020","title":"Planned for 0.2.0","text":"<ul> <li> Prefect integration</li> <li> Dagster integration</li> <li> Additional backends (Trino, Presto)</li> <li> Pipeline composition (includes/extends)</li> <li> Parallel transform execution</li> </ul>"},{"location":"development/changelog/#planned-for-030","title":"Planned for 0.3.0","text":"<ul> <li> Web UI for pipeline management</li> <li> Pipeline versioning</li> <li> Lineage tracking</li> <li> Metric collection</li> </ul>"},{"location":"development/contributing/","title":"Contributing to ETLX","text":"<p>Thank you for your interest in contributing to ETLX! This guide will help you get started.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Git</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-org/etlx.git\ncd etlx\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev,docs]\"\n\n# Run tests\npytest\n\n# Run linting\nruff check src/\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/my-feature\n</code></pre>"},{"location":"development/contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following the style guide</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=etlx --cov-report=html\n\n# Run specific tests\npytest tests/test_transforms.py\n</code></pre>"},{"location":"development/contributing/#4-run-linting","title":"4. Run Linting","text":"<pre><code># Check code style\nruff check src/\n\n# Format code\nruff format src/\n</code></pre>"},{"location":"development/contributing/#5-submit-pull-request","title":"5. Submit Pull Request","text":"<ul> <li>Push your branch</li> <li>Create a pull request</li> <li>Fill out the PR template</li> <li>Wait for review</li> </ul>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python","title":"Python","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/contributing/#example","title":"Example","text":"<pre><code>def apply_filter(\n    table: Table,\n    predicate: str,\n) -&gt; Table:\n    \"\"\"Apply a filter predicate to a table.\n\n    Args:\n        table: Input Ibis table.\n        predicate: SQL-compatible filter expression.\n\n    Returns:\n        Filtered table.\n\n    Raises:\n        ValueError: If predicate is invalid.\n\n    Example:\n        &gt;&gt;&gt; filtered = apply_filter(table, \"amount &gt; 100\")\n    \"\"\"\n    return table.filter(predicate)\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u251c\u2500\u2500 test_transforms.py   # Transform tests\n\u251c\u2500\u2500 test_checks.py       # Quality check tests\n\u251c\u2500\u2500 test_engine.py       # Engine tests\n\u2514\u2500\u2500 fixtures/            # Test data\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>import pytest\nfrom etlx.transforms import FilterTransform\n\ndef test_filter_basic():\n    \"\"\"Test basic filter functionality.\"\"\"\n    transform = FilterTransform(predicate=\"amount &gt; 0\")\n    result = transform.apply(sample_table)\n    assert len(result) == expected_count\n\ndef test_filter_invalid_predicate():\n    \"\"\"Test filter with invalid predicate raises error.\"\"\"\n    with pytest.raises(ValueError):\n        FilterTransform(predicate=\"invalid syntax &gt;&gt;&gt;\")\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#building-docs","title":"Building Docs","text":"<pre><code># Serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n\n# Check for errors\nmkdocs build --strict\n</code></pre>"},{"location":"development/contributing/#writing-docs","title":"Writing Docs","text":"<ul> <li>Use clear, concise language</li> <li>Include examples</li> <li>Cross-link related pages</li> <li>Test code examples</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#title-format","title":"Title Format","text":"<pre><code>feat: Add new transform for X\nfix: Handle NULL values in filter\ndocs: Update quickstart guide\ntest: Add tests for aggregate transform\nrefactor: Simplify engine initialization\n</code></pre>"},{"location":"development/contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests pass</li> <li> Linting passes</li> <li> Documentation updated</li> <li> Changelog entry added</li> <li> Linked to issue (if applicable)</li> </ul>"},{"location":"development/contributing/#issue-guidelines","title":"Issue Guidelines","text":""},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":"<p>Include:</p> <ul> <li>ETLX version</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages</li> </ul>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":"<p>Include:</p> <ul> <li>Use case description</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Check existing issues</li> <li>Ask in discussions</li> <li>Read the documentation</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section contains complete, runnable examples demonstrating common ETLX patterns and use cases. Each example includes:</p> <ul> <li>Complete YAML configuration</li> <li>Sample data</li> <li>Expected output</li> <li>Step-by-step explanation</li> </ul>"},{"location":"examples/#getting-started-examples","title":"Getting Started Examples","text":""},{"location":"examples/#basic-pipeline","title":"Basic Pipeline","text":"<p>A minimal pipeline that reads a CSV, applies filters, and writes to Parquet. Perfect for understanding ETLX fundamentals.</p> <pre><code>name: basic_example\nsource:\n  type: file\n  path: data/input.csv\n  format: csv\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\nsink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/#data-processing-examples","title":"Data Processing Examples","text":""},{"location":"examples/#multi-source-join","title":"Multi-Source Join","text":"<p>Combine data from multiple sources (orders + customers + products) into a single enriched dataset.</p> <pre><code>transforms:\n  - op: join\n    right:\n      type: file\n      path: data/customers.csv\n      format: csv\n    on: [customer_id]\n    how: left\n</code></pre>"},{"location":"examples/#aggregation-pipeline","title":"Aggregation Pipeline","text":"<p>Compute metrics, summaries, and roll-ups from transactional data.</p> <pre><code>transforms:\n  - op: aggregate\n    group_by: [region, category]\n    aggregations:\n      total_revenue: sum(amount)\n      order_count: count(*)\n      avg_order_value: avg(amount)\n</code></pre>"},{"location":"examples/#cloud-production-examples","title":"Cloud &amp; Production Examples","text":""},{"location":"examples/#cloud-etl","title":"Cloud ETL","text":"<p>End-to-end pipeline reading from S3, transforming with Spark, and loading to a data warehouse.</p> <pre><code>engine: spark\nsource:\n  type: file\n  path: s3://bucket/raw/*.parquet\n  format: parquet\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.fact_sales\n</code></pre>"},{"location":"examples/#airflow-dag","title":"Airflow DAG","text":"<p>Complete Airflow DAG with ETLX tasks, error handling, and monitoring.</p> <pre><code>@etlx_task(config=\"pipelines/daily.yml\")\ndef process_daily(**context):\n    return {\"DATE\": context[\"ds\"]}\n</code></pre>"},{"location":"examples/#example-categories","title":"Example Categories","text":"Category Examples Description Basic Basic Pipeline Core concepts Joins Multi-Source Join Combining data Analytics Aggregation Metrics and summaries Cloud Cloud ETL Production cloud pipelines Orchestration Airflow DAG Workflow integration"},{"location":"examples/#running-examples","title":"Running Examples","text":""},{"location":"examples/#setup","title":"Setup","text":"<pre><code># Clone or create project\netlx init examples\ncd examples\n\n# Install dependencies\npip install etlx[duckdb]\n</code></pre>"},{"location":"examples/#run-any-example","title":"Run Any Example","text":"<pre><code># Validate first\netlx validate pipelines/example.yml\n\n# Run\netlx run pipelines/example.yml\n</code></pre>"},{"location":"examples/#with-variables","title":"With Variables","text":"<pre><code>etlx run pipelines/example.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"examples/#sample-data","title":"Sample Data","text":"<p>All examples use sample data available in the <code>docs/assets/data/</code> directory:</p> File Description Rows <code>sales.csv</code> Transaction records 12 <code>customers.csv</code> Customer master data 5 <code>products.csv</code> Product catalog 10"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful pattern? Contribute an example:</p> <ol> <li>Create a new markdown file in <code>docs/examples/</code></li> <li>Include complete, runnable YAML</li> <li>Add sample input/output data</li> <li>Document each step</li> </ol> <p>See Contributing Guide for details.</p>"},{"location":"examples/#related","title":"Related","text":"<ul> <li>Getting Started - First steps with ETLX</li> <li>User Guide - Complete reference</li> <li>Best Practices - Design patterns</li> </ul>"},{"location":"examples/aggregation/","title":"Aggregation Pipeline Example","text":"<p>This example demonstrates how to compute metrics, summaries, and roll-ups from transactional data using ETLX aggregations.</p>"},{"location":"examples/aggregation/#overview","title":"Overview","text":"<p>Goal: Create a sales analytics report with:</p> <ul> <li>Revenue by region and category</li> <li>Order counts and averages</li> <li>Top products by revenue</li> </ul>"},{"location":"examples/aggregation/#sample-data","title":"Sample Data","text":"<p>Create <code>data/transactions.csv</code>:</p> <pre><code>transaction_id,date,region,category,product,quantity,unit_price,customer_id\nT001,2025-01-15,North,Electronics,Widget A,2,29.99,C001\nT002,2025-01-15,South,Electronics,Gadget B,1,49.99,C002\nT003,2025-01-15,North,Services,Service C,1,99.99,C003\nT004,2025-01-16,East,Electronics,Widget A,3,29.99,C004\nT005,2025-01-16,West,Hardware,Tool D,2,39.99,C005\nT006,2025-01-16,North,Electronics,Gadget B,1,49.99,C001\nT007,2025-01-17,South,Services,Service C,2,99.99,C002\nT008,2025-01-17,East,Electronics,Widget A,1,29.99,C006\nT009,2025-01-17,North,Hardware,Tool D,1,39.99,C003\nT010,2025-01-18,West,Electronics,Widget A,4,29.99,C007\nT011,2025-01-18,South,Electronics,Gadget B,2,49.99,C008\nT012,2025-01-18,North,Services,Service C,1,99.99,C001\n</code></pre>"},{"location":"examples/aggregation/#pipeline-1-regional-summary","title":"Pipeline 1: Regional Summary","text":"<p>Create <code>pipelines/regional_summary.yml</code>:</p> <pre><code>name: regional_sales_summary\ndescription: Aggregate sales metrics by region\nengine: duckdb\n\nsource:\n  type: file\n  path: data/transactions.csv\n  format: csv\n\ntransforms:\n  # Calculate line totals\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  # Aggregate by region\n  - op: aggregate\n    group_by: [region]\n    aggregations:\n      total_revenue: sum(line_total)\n      total_orders: count(*)\n      total_quantity: sum(quantity)\n      avg_order_value: avg(line_total)\n      unique_customers: count(distinct customer_id)\n      unique_products: count(distinct product)\n\n  # Sort by revenue descending\n  - op: sort\n    by:\n      - column: total_revenue\n        order: desc\n\nchecks:\n  - check: not_null\n    columns: [region, total_revenue]\n  - check: row_count\n    min: 1\n\nsink:\n  type: file\n  path: output/regional_summary.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/aggregation/#expected-output","title":"Expected Output","text":"region total_revenue total_orders total_quantity avg_order_value unique_customers unique_products North 319.94 5 6 63.99 3 4 South 349.95 3 5 116.65 3 3 East 119.96 2 4 59.98 2 1 West 199.94 2 6 99.97 2 2"},{"location":"examples/aggregation/#pipeline-2-category-by-region","title":"Pipeline 2: Category by Region","text":"<p>Create <code>pipelines/category_region.yml</code>:</p> <pre><code>name: category_region_analysis\ndescription: Cross-tabulation of categories by region\nengine: duckdb\n\nsource:\n  type: file\n  path: data/transactions.csv\n  format: csv\n\ntransforms:\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  # Multi-level grouping\n  - op: aggregate\n    group_by: [region, category]\n    aggregations:\n      revenue: sum(line_total)\n      orders: count(*)\n      avg_quantity: avg(quantity)\n\n  # Sort for readability\n  - op: sort\n    by:\n      - column: region\n        order: asc\n      - column: revenue\n        order: desc\n\nsink:\n  type: file\n  path: output/category_region.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/aggregation/#pipeline-3-time-series-aggregation","title":"Pipeline 3: Time Series Aggregation","text":"<p>Create <code>pipelines/daily_metrics.yml</code>:</p> <pre><code>name: daily_sales_metrics\ndescription: Daily aggregated metrics\nengine: duckdb\n\nsource:\n  type: file\n  path: data/transactions.csv\n  format: csv\n\ntransforms:\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  # Aggregate by date\n  - op: aggregate\n    group_by: [date]\n    aggregations:\n      daily_revenue: sum(line_total)\n      daily_orders: count(*)\n      daily_items: sum(quantity)\n\n  # Calculate running total (requires window function support)\n  - op: derive_column\n    name: cumulative_revenue\n    expr: sum(daily_revenue) over (order by date)\n\n  - op: sort\n    by:\n      - column: date\n        order: asc\n\nsink:\n  type: file\n  path: output/daily_metrics.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/aggregation/#pipeline-4-top-products","title":"Pipeline 4: Top Products","text":"<p>Create <code>pipelines/top_products.yml</code>:</p> <pre><code>name: top_products_analysis\ndescription: Identify best-selling products\nengine: duckdb\n\nsource:\n  type: file\n  path: data/transactions.csv\n  format: csv\n\ntransforms:\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  - op: aggregate\n    group_by: [product, category]\n    aggregations:\n      total_revenue: sum(line_total)\n      units_sold: sum(quantity)\n      order_count: count(*)\n      avg_unit_price: avg(unit_price)\n\n  # Rank by revenue\n  - op: sort\n    by:\n      - column: total_revenue\n        order: desc\n\n  # Keep top 10\n  - op: limit\n    n: 10\n\nsink:\n  type: file\n  path: output/top_products.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/aggregation/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>count(*)</code> Count all rows <code>orders: count(*)</code> <code>count(column)</code> Count non-null values <code>valid_emails: count(email)</code> <code>count(distinct column)</code> Count unique values <code>customers: count(distinct customer_id)</code> <code>sum(column)</code> Sum values <code>revenue: sum(amount)</code> <code>avg(column)</code> Average <code>avg_order: avg(amount)</code> <code>min(column)</code> Minimum <code>first_order: min(date)</code> <code>max(column)</code> Maximum <code>last_order: max(date)</code> <code>stddev(column)</code> Standard deviation <code>volatility: stddev(price)</code> <code>variance(column)</code> Variance <code>var: variance(returns)</code>"},{"location":"examples/aggregation/#conditional-aggregation","title":"Conditional Aggregation","text":"<p>Use expressions for conditional counting:</p> <pre><code>- op: aggregate\n  group_by: [region]\n  aggregations:\n    total_orders: count(*)\n    completed_orders: sum(case when status = 'completed' then 1 else 0 end)\n    pending_orders: sum(case when status = 'pending' then 1 else 0 end)\n    completion_rate: avg(case when status = 'completed' then 1.0 else 0.0 end)\n</code></pre>"},{"location":"examples/aggregation/#percentile-calculations","title":"Percentile Calculations","text":"<pre><code>- op: aggregate\n  group_by: [category]\n  aggregations:\n    median_price: percentile_cont(0.5) within group (order by unit_price)\n    p95_price: percentile_cont(0.95) within group (order by unit_price)\n</code></pre>"},{"location":"examples/aggregation/#running-all-pipelines","title":"Running All Pipelines","text":"<pre><code># Run all aggregation pipelines\nfor f in pipelines/*.yml; do\n  echo \"Running $f...\"\n  etlx run \"$f\"\ndone\n</code></pre>"},{"location":"examples/aggregation/#combining-with-joins","title":"Combining with Joins","text":"<p>Aggregate after enriching with joins:</p> <pre><code>transforms:\n  # Join first\n  - op: join\n    right:\n      type: file\n      path: data/products.csv\n      format: csv\n    on: [product_id]\n    how: left\n\n  # Then aggregate with joined data\n  - op: aggregate\n    group_by: [category, subcategory]  # From products table\n    aggregations:\n      revenue: sum(quantity * unit_price)\n      margin: avg((unit_price - cost) / unit_price)\n</code></pre>"},{"location":"examples/aggregation/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/aggregation/#1-filter-before-aggregating","title":"1. Filter Before Aggregating","text":"<pre><code>transforms:\n  # Filter first\n  - op: filter\n    predicate: date &gt;= '2025-01-01' AND status = 'completed'\n\n  # Then aggregate (fewer rows)\n  - op: aggregate\n    group_by: [region]\n    aggregations:\n      revenue: sum(amount)\n</code></pre>"},{"location":"examples/aggregation/#2-aggregate-in-stages","title":"2. Aggregate in Stages","text":"<p>For complex aggregations, break into steps:</p> <pre><code>transforms:\n  # First aggregation: daily by region\n  - op: aggregate\n    group_by: [date, region]\n    aggregations:\n      daily_revenue: sum(amount)\n\n  # Second aggregation: monthly summary\n  - op: derive_column\n    name: month\n    expr: date_trunc('month', date)\n\n  - op: aggregate\n    group_by: [month, region]\n    aggregations:\n      monthly_revenue: sum(daily_revenue)\n</code></pre>"},{"location":"examples/aggregation/#next-steps","title":"Next Steps","text":"<ul> <li>Cloud ETL Example - Production cloud pipelines</li> <li>Aggregate Transform Reference</li> <li>Performance Best Practices</li> </ul>"},{"location":"examples/airflow-dag/","title":"Airflow DAG Example","text":"<p>This example demonstrates a complete Airflow DAG that orchestrates multiple ETLX pipelines with proper error handling, alerting, and monitoring.</p>"},{"location":"examples/airflow-dag/#overview","title":"Overview","text":"<p>Scenario: Daily data pipeline that:</p> <ol> <li>Extracts data from multiple sources</li> <li>Transforms and validates data</li> <li>Loads to data warehouse</li> <li>Sends notifications on completion/failure</li> </ol>"},{"location":"examples/airflow-dag/#dag-structure","title":"DAG Structure","text":"<pre><code>extract_orders \u2500\u252c\u2500\u25ba transform_data \u2500\u25ba load_warehouse \u2500\u25ba notify_success\nextract_products\u2500\u2524                                    \u2502\nextract_customers\u2518                                    \u2514\u2500\u25ba notify_failure (on error)\n</code></pre>"},{"location":"examples/airflow-dag/#complete-dag-code","title":"Complete DAG Code","text":"<p>Create <code>dags/daily_etl_dag.py</code>:</p> <pre><code>\"\"\"\nDaily ETL Pipeline DAG\n\nThis DAG orchestrates the daily ETL process using ETLX pipelines.\nRuns daily at 6 AM UTC.\n\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom airflow.decorators import dag, task\nfrom airflow.operators.email import EmailOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom etlx.integrations.airflow import etlx_task\n\n\n# Default arguments for all tasks\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"depends_on_past\": False,\n    \"email\": [\"data-alerts@company.com\"],\n    \"email_on_failure\": True,\n    \"email_on_retry\": False,\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"retry_exponential_backoff\": True,\n    \"max_retry_delay\": timedelta(hours=1),\n}\n\n\n@dag(\n    dag_id=\"daily_etl_pipeline\",\n    description=\"Daily ETL pipeline using ETLX\",\n    schedule=\"0 6 * * *\",  # 6 AM UTC daily\n    start_date=datetime(2025, 1, 1),\n    catchup=False,\n    default_args=default_args,\n    tags=[\"etlx\", \"production\", \"daily\"],\n    doc_md=__doc__,\n)\ndef daily_etl_pipeline():\n    \"\"\"\n    Daily ETL pipeline that extracts, transforms, and loads data.\n\n    ## Pipeline Steps\n    1. Extract: Pull data from source systems\n    2. Transform: Clean, validate, and aggregate\n    3. Load: Write to data warehouse\n    4. Notify: Send completion status\n\n    ## Variables Required\n    - `etlx_database_url`: Connection string for warehouse\n    - `etlx_s3_bucket`: S3 bucket for raw data\n\n    ## Connections Required\n    - `snowflake_default`: Snowflake warehouse connection\n    \"\"\"\n\n    # =========================================================================\n    # EXTRACT PHASE\n    # =========================================================================\n\n    @etlx_task(config=\"pipelines/extract/orders.yml\")\n    def extract_orders(**context):\n        \"\"\"Extract orders from source system.\"\"\"\n        return {\n            \"DATE\": context[\"ds\"],\n            \"EXECUTION_DATE\": context[\"execution_date\"].isoformat(),\n        }\n\n    @etlx_task(config=\"pipelines/extract/products.yml\")\n    def extract_products(**context):\n        \"\"\"Extract product catalog.\"\"\"\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/extract/customers.yml\")\n    def extract_customers(**context):\n        \"\"\"Extract customer data.\"\"\"\n        return {\"DATE\": context[\"ds\"]}\n\n    # =========================================================================\n    # TRANSFORM PHASE\n    # =========================================================================\n\n    @etlx_task(config=\"pipelines/transform/daily_metrics.yml\")\n    def transform_data(**context):\n        \"\"\"Transform and aggregate extracted data.\"\"\"\n        return {\n            \"DATE\": context[\"ds\"],\n            \"PREV_DATE\": context[\"prev_ds\"],\n        }\n\n    # =========================================================================\n    # LOAD PHASE\n    # =========================================================================\n\n    @etlx_task(\n        config=\"pipelines/load/warehouse.yml\",\n        fail_on_checks=True,\n    )\n    def load_warehouse(**context):\n        \"\"\"Load transformed data to warehouse.\"\"\"\n        from airflow.models import Variable\n\n        return {\n            \"DATE\": context[\"ds\"],\n            \"WAREHOUSE\": Variable.get(\"etlx_warehouse\", default_var=\"ETL_WH\"),\n        }\n\n    # =========================================================================\n    # NOTIFICATION PHASE\n    # =========================================================================\n\n    @task\n    def prepare_success_report(**context):\n        \"\"\"Prepare success notification with metrics.\"\"\"\n        ti = context[\"task_instance\"]\n\n        # Get results from upstream tasks\n        extract_orders_result = ti.xcom_pull(task_ids=\"extract_orders\")\n        extract_products_result = ti.xcom_pull(task_ids=\"extract_products\")\n        extract_customers_result = ti.xcom_pull(task_ids=\"extract_customers\")\n        transform_result = ti.xcom_pull(task_ids=\"transform_data\")\n        load_result = ti.xcom_pull(task_ids=\"load_warehouse\")\n\n        # Calculate totals\n        total_rows = sum([\n            extract_orders_result.get(\"rows_written\", 0),\n            extract_products_result.get(\"rows_written\", 0),\n            extract_customers_result.get(\"rows_written\", 0),\n        ])\n\n        total_duration = sum([\n            extract_orders_result.get(\"duration_ms\", 0),\n            extract_products_result.get(\"duration_ms\", 0),\n            extract_customers_result.get(\"duration_ms\", 0),\n            transform_result.get(\"duration_ms\", 0),\n            load_result.get(\"duration_ms\", 0),\n        ])\n\n        return {\n            \"date\": context[\"ds\"],\n            \"total_rows_extracted\": total_rows,\n            \"rows_loaded\": load_result.get(\"rows_written\", 0),\n            \"total_duration_ms\": total_duration,\n            \"checks_passed\": load_result.get(\"checks_passed\", 0),\n        }\n\n    notify_success = EmailOperator(\n        task_id=\"notify_success\",\n        to=[\"data-team@company.com\"],\n        subject=\"\u2713 Daily ETL Success: {{ ds }}\",\n        html_content=\"\"\"\n        &lt;h2&gt;Daily ETL Pipeline Completed Successfully&lt;/h2&gt;\n\n        &lt;p&gt;&lt;strong&gt;Date:&lt;/strong&gt; {{ ds }}&lt;/p&gt;\n        &lt;p&gt;&lt;strong&gt;Run ID:&lt;/strong&gt; {{ run_id }}&lt;/p&gt;\n\n        &lt;h3&gt;Metrics&lt;/h3&gt;\n        &lt;ul&gt;\n            &lt;li&gt;Rows Extracted: {{ ti.xcom_pull(task_ids='prepare_success_report')['total_rows_extracted'] }}&lt;/li&gt;\n            &lt;li&gt;Rows Loaded: {{ ti.xcom_pull(task_ids='prepare_success_report')['rows_loaded'] }}&lt;/li&gt;\n            &lt;li&gt;Duration: {{ ti.xcom_pull(task_ids='prepare_success_report')['total_duration_ms'] }}ms&lt;/li&gt;\n            &lt;li&gt;Quality Checks Passed: {{ ti.xcom_pull(task_ids='prepare_success_report')['checks_passed'] }}&lt;/li&gt;\n        &lt;/ul&gt;\n\n        &lt;p&gt;&lt;a href=\"{{ conf.get('webserver', 'base_url') }}/dags/daily_etl_pipeline/grid\"&gt;View in Airflow&lt;/a&gt;&lt;/p&gt;\n        \"\"\",\n    )\n\n    notify_failure = EmailOperator(\n        task_id=\"notify_failure\",\n        to=[\"data-alerts@company.com\", \"oncall@company.com\"],\n        subject=\"\u2717 Daily ETL FAILED: {{ ds }}\",\n        html_content=\"\"\"\n        &lt;h2&gt;Daily ETL Pipeline Failed&lt;/h2&gt;\n\n        &lt;p&gt;&lt;strong&gt;Date:&lt;/strong&gt; {{ ds }}&lt;/p&gt;\n        &lt;p&gt;&lt;strong&gt;Run ID:&lt;/strong&gt; {{ run_id }}&lt;/p&gt;\n\n        &lt;p&gt;Please investigate immediately.&lt;/p&gt;\n\n        &lt;p&gt;&lt;a href=\"{{ conf.get('webserver', 'base_url') }}/dags/daily_etl_pipeline/grid\"&gt;View in Airflow&lt;/a&gt;&lt;/p&gt;\n        \"\"\",\n        trigger_rule=TriggerRule.ONE_FAILED,\n    )\n\n    # =========================================================================\n    # DAG DEPENDENCIES\n    # =========================================================================\n\n    # Extract phase (parallel)\n    orders = extract_orders()\n    products = extract_products()\n    customers = extract_customers()\n\n    # Transform phase (after all extracts)\n    transformed = transform_data()\n    [orders, products, customers] &gt;&gt; transformed\n\n    # Load phase\n    loaded = load_warehouse()\n    transformed &gt;&gt; loaded\n\n    # Notification phase\n    report = prepare_success_report()\n    loaded &gt;&gt; report &gt;&gt; notify_success\n\n    # Failure notification (triggers if any task fails)\n    [orders, products, customers, transformed, loaded] &gt;&gt; notify_failure\n\n\n# Instantiate DAG\ndaily_etl_pipeline()\n</code></pre>"},{"location":"examples/airflow-dag/#pipeline-configuration-files","title":"Pipeline Configuration Files","text":""},{"location":"examples/airflow-dag/#extract-orders-pipelinesextractordersyml","title":"Extract Orders (<code>pipelines/extract/orders.yml</code>)","text":"<pre><code>name: extract_orders\ndescription: Extract orders from source database\nengine: duckdb\n\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT *\n    FROM orders\n    WHERE order_date = '${DATE}'\n\nsink:\n  type: file\n  path: staging/orders/date=${DATE}/orders.parquet\n  format: parquet\n  mode: replace\n</code></pre>"},{"location":"examples/airflow-dag/#extract-products-pipelinesextractproductsyml","title":"Extract Products (<code>pipelines/extract/products.yml</code>)","text":"<pre><code>name: extract_products\ndescription: Extract product catalog\nengine: duckdb\n\nsource:\n  type: database\n  connection: postgres\n  table: products\n\nsink:\n  type: file\n  path: staging/products/products.parquet\n  format: parquet\n  mode: replace\n</code></pre>"},{"location":"examples/airflow-dag/#transform-pipelinestransformdaily_metricsyml","title":"Transform (<code>pipelines/transform/daily_metrics.yml</code>)","text":"<pre><code>name: transform_daily_metrics\ndescription: Transform and aggregate daily data\nengine: duckdb\n\nsource:\n  type: file\n  path: staging/orders/date=${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  - op: join\n    right:\n      type: file\n      path: staging/products/products.parquet\n      format: parquet\n    on: [product_id]\n    how: left\n\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  - op: aggregate\n    group_by: [category, order_date]\n    aggregations:\n      total_revenue: sum(line_total)\n      order_count: count(distinct order_id)\n\nchecks:\n  - check: not_null\n    columns: [category, total_revenue]\n  - check: row_count\n    min: 1\n\nsink:\n  type: file\n  path: staging/metrics/date=${DATE}/metrics.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/airflow-dag/#load-warehouse-pipelinesloadwarehouseyml","title":"Load Warehouse (<code>pipelines/load/warehouse.yml</code>)","text":"<pre><code>name: load_warehouse\ndescription: Load metrics to data warehouse\nengine: snowflake\n\nsource:\n  type: file\n  path: staging/metrics/date=${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  - op: derive_column\n    name: loaded_at\n    expr: current_timestamp()\n\nchecks:\n  - check: not_null\n    columns: [category, total_revenue, loaded_at]\n  - check: expression\n    expr: total_revenue &gt;= 0\n\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.daily_metrics\n  mode: merge\n  merge_keys: [category, order_date]\n</code></pre>"},{"location":"examples/airflow-dag/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/airflow-dag/#dynamic-task-generation","title":"Dynamic Task Generation","text":"<pre><code>@dag(...)\ndef dynamic_pipeline():\n    @task\n    def get_regions():\n        # Could come from database or API\n        return [\"us-east\", \"us-west\", \"eu-west\", \"ap-south\"]\n\n    @etlx_task(config=\"pipelines/regional.yml\")\n    def process_region(region, **context):\n        return {\n            \"DATE\": context[\"ds\"],\n            \"REGION\": region,\n        }\n\n    regions = get_regions()\n    process_region.expand(region=regions)\n</code></pre>"},{"location":"examples/airflow-dag/#conditional-execution","title":"Conditional Execution","text":"<pre><code>from airflow.operators.python import BranchPythonOperator\n\n@dag(...)\ndef conditional_pipeline():\n    def choose_pipeline(**context):\n        # Monday = full refresh, other days = incremental\n        if context[\"execution_date\"].weekday() == 0:\n            return \"full_refresh\"\n        return \"incremental\"\n\n    branch = BranchPythonOperator(\n        task_id=\"choose_pipeline\",\n        python_callable=choose_pipeline,\n    )\n\n    @etlx_task(config=\"pipelines/full_refresh.yml\")\n    def full_refresh(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/incremental.yml\")\n    def incremental(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    branch &gt;&gt; [full_refresh(), incremental()]\n</code></pre>"},{"location":"examples/airflow-dag/#sla-monitoring","title":"SLA Monitoring","text":"<pre><code>@dag(\n    sla_miss_callback=slack_sla_alert,\n    default_args={\n        \"sla\": timedelta(hours=2),\n    },\n)\ndef sla_monitored_pipeline():\n    @etlx_task(\n        config=\"pipelines/critical.yml\",\n        sla=timedelta(hours=1),  # Stricter SLA for this task\n    )\n    def critical_task(**context):\n        return {\"DATE\": context[\"ds\"]}\n</code></pre>"},{"location":"examples/airflow-dag/#deployment","title":"Deployment","text":""},{"location":"examples/airflow-dag/#project-structure","title":"Project Structure","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 daily_etl_dag.py\n\u2502   \u2514\u2500\u2500 weekly_etl_dag.py\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 extract/\n\u2502   \u2502   \u251c\u2500\u2500 orders.yml\n\u2502   \u2502   \u251c\u2500\u2500 products.yml\n\u2502   \u2502   \u2514\u2500\u2500 customers.yml\n\u2502   \u251c\u2500\u2500 transform/\n\u2502   \u2502   \u2514\u2500\u2500 daily_metrics.yml\n\u2502   \u2514\u2500\u2500 load/\n\u2502       \u2514\u2500\u2500 warehouse.yml\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"examples/airflow-dag/#requirements","title":"Requirements","text":"<pre><code># requirements.txt\napache-airflow&gt;=2.8.0\netlx[airflow,duckdb,snowflake]\n</code></pre>"},{"location":"examples/airflow-dag/#airflow-variables","title":"Airflow Variables","text":"<p>Set in Airflow UI or via CLI:</p> <pre><code>airflow variables set etlx_warehouse \"ETL_WH\"\nairflow variables set etlx_s3_bucket \"data-lake-prod\"\n</code></pre>"},{"location":"examples/airflow-dag/#airflow-connections","title":"Airflow Connections","text":"<pre><code># PostgreSQL source\nairflow connections add postgres_source \\\n    --conn-type postgres \\\n    --conn-host localhost \\\n    --conn-login user \\\n    --conn-password pass \\\n    --conn-schema mydb\n\n# Snowflake destination\nairflow connections add snowflake_default \\\n    --conn-type snowflake \\\n    --conn-host xy12345.us-east-1 \\\n    --conn-login etl_user \\\n    --conn-password pass \\\n    --conn-schema analytics\n</code></pre>"},{"location":"examples/airflow-dag/#next-steps","title":"Next Steps","text":"<ul> <li>Airflow Integration Guide - Complete reference</li> <li>Cloud ETL Example - Cloud-native pipelines</li> <li>Production Best Practices</li> </ul>"},{"location":"examples/basic-pipeline/","title":"Basic Pipeline Example","text":"<p>This example demonstrates ETLX fundamentals: reading a CSV file, applying transforms, running quality checks, and writing output.</p>"},{"location":"examples/basic-pipeline/#overview","title":"Overview","text":"<p>Goal: Process sales data to get completed orders with calculated totals.</p> <p>Steps:</p> <ol> <li>Read CSV file</li> <li>Filter to completed orders</li> <li>Calculate order totals</li> <li>Validate data quality</li> <li>Write to Parquet</li> </ol>"},{"location":"examples/basic-pipeline/#sample-data","title":"Sample Data","text":"<p>Create <code>data/sales.csv</code>:</p> <pre><code>id,customer_name,product,quantity,unit_price,status,order_date\n1,Alice Smith,Widget A,2,29.99,completed,2025-01-15\n2,Bob Johnson,Gadget B,1,49.99,pending,2025-01-15\n3,Carol White,Widget A,3,29.99,completed,2025-01-16\n4,David Brown,Service C,1,99.99,completed,2025-01-16\n5,Eve Davis,Gadget B,2,49.99,cancelled,2025-01-17\n6,Frank Miller,Widget A,5,29.99,completed,2025-01-17\n7,Grace Lee,Service C,1,99.99,pending,2025-01-18\n8,Henry Wilson,Gadget B,1,49.99,completed,2025-01-18\n</code></pre>"},{"location":"examples/basic-pipeline/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Create <code>pipelines/basic.yml</code>:</p> <pre><code># Basic Pipeline Example\n# Processes sales data to calculate order totals\n\nname: basic_sales_pipeline\ndescription: Filter completed orders and calculate totals\nengine: duckdb\n\n# Read from CSV file\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\n# Apply transformations\ntransforms:\n  # Step 1: Keep only completed orders\n  - op: filter\n    predicate: status = 'completed'\n\n  # Step 2: Calculate total amount for each order\n  - op: derive_column\n    name: total_amount\n    expr: quantity * unit_price\n\n  # Step 3: Select and reorder columns\n  - op: select\n    columns:\n      - id\n      - customer_name\n      - product\n      - quantity\n      - unit_price\n      - total_amount\n      - order_date\n\n  # Step 4: Sort by date and total\n  - op: sort\n    by:\n      - column: order_date\n        order: asc\n      - column: total_amount\n        order: desc\n\n# Validate data quality\nchecks:\n  # Ensure required fields are present\n  - check: not_null\n    columns: [id, customer_name, total_amount]\n\n  # Verify we have data\n  - check: row_count\n    min: 1\n\n  # Business rule: totals must be positive\n  - check: expression\n    expr: total_amount &gt; 0\n\n# Write output\nsink:\n  type: file\n  path: output/completed_orders.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/basic-pipeline/#running-the-pipeline","title":"Running the Pipeline","text":""},{"location":"examples/basic-pipeline/#validate-first","title":"Validate First","text":"<pre><code>etlx validate pipelines/basic.yml\n</code></pre> <p>Expected output:</p> <pre><code>Configuration is valid\n\nPipeline: basic_sales_pipeline\n  Engine: duckdb\n  Source: file (data/sales.csv)\n  Transforms: 4\n  Checks: 3\n  Sink: file (output/completed_orders.parquet)\n</code></pre>"},{"location":"examples/basic-pipeline/#execute","title":"Execute","text":"<pre><code>etlx run pipelines/basic.yml\n</code></pre> <p>Expected output:</p> <pre><code>Running pipeline: basic_sales_pipeline\n  Filter completed orders and calculate totals\n  Engine: duckdb\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pipeline: basic_sales_pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 SUCCESS                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duration: 45.2ms \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nSteps\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Step             \u2503 Type       \u2503 Status \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 read_source      \u2502 file       \u2502 OK     \u2502   12.1ms \u2502\n\u2502 transform_0      \u2502 filter     \u2502 OK     \u2502    0.8ms \u2502\n\u2502 transform_1      \u2502 derive     \u2502 OK     \u2502    0.3ms \u2502\n\u2502 transform_2      \u2502 select     \u2502 OK     \u2502    0.2ms \u2502\n\u2502 transform_3      \u2502 sort       \u2502 OK     \u2502    0.4ms \u2502\n\u2502 quality_checks   \u2502 checks     \u2502 OK     \u2502    5.2ms \u2502\n\u2502 write_sink       \u2502 file       \u2502 OK     \u2502    8.1ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nQuality Checks: PASSED (3/3 passed)\n\nRows processed: 8\nRows written: 5\n</code></pre>"},{"location":"examples/basic-pipeline/#expected-output","title":"Expected Output","text":"<p>The output file <code>output/completed_orders.parquet</code> contains:</p> id customer_name product quantity unit_price total_amount order_date 1 Alice Smith Widget A 2 29.99 59.98 2025-01-15 4 David Brown Service C 1 99.99 99.99 2025-01-16 3 Carol White Widget A 3 29.99 89.97 2025-01-16 6 Frank Miller Widget A 5 29.99 149.95 2025-01-17 8 Henry Wilson Gadget B 1 49.99 49.99 2025-01-18"},{"location":"examples/basic-pipeline/#verify-output","title":"Verify Output","text":"<p>Read the output with DuckDB:</p> <pre><code>duckdb -c \"SELECT * FROM 'output/completed_orders.parquet'\"\n</code></pre> <p>Or with Python:</p> <pre><code>import pandas as pd\ndf = pd.read_parquet(\"output/completed_orders.parquet\")\nprint(df)\n</code></pre>"},{"location":"examples/basic-pipeline/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"examples/basic-pipeline/#1-source-configuration","title":"1. Source Configuration","text":"<pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n</code></pre> <ul> <li><code>type: file</code> - Read from filesystem</li> <li><code>path</code> - File location (supports glob patterns)</li> <li><code>format: csv</code> - File format for parsing</li> </ul>"},{"location":"examples/basic-pipeline/#2-filter-transform","title":"2. Filter Transform","text":"<pre><code>- op: filter\n  predicate: status = 'completed'\n</code></pre> <ul> <li>Keeps only rows where <code>status</code> equals <code>'completed'</code></li> <li>Removes pending and cancelled orders</li> </ul>"},{"location":"examples/basic-pipeline/#3-derive-column","title":"3. Derive Column","text":"<pre><code>- op: derive_column\n  name: total_amount\n  expr: quantity * unit_price\n</code></pre> <ul> <li>Creates a new column <code>total_amount</code></li> <li>Calculates value using SQL expression</li> </ul>"},{"location":"examples/basic-pipeline/#4-select-columns","title":"4. Select Columns","text":"<pre><code>- op: select\n  columns: [id, customer_name, product, quantity, unit_price, total_amount, order_date]\n</code></pre> <ul> <li>Explicitly choose which columns to keep</li> <li>Defines output column order</li> </ul>"},{"location":"examples/basic-pipeline/#5-sort","title":"5. Sort","text":"<pre><code>- op: sort\n  by:\n    - column: order_date\n      order: asc\n    - column: total_amount\n      order: desc\n</code></pre> <ul> <li>Primary sort by <code>order_date</code> ascending</li> <li>Secondary sort by <code>total_amount</code> descending</li> </ul>"},{"location":"examples/basic-pipeline/#6-quality-checks","title":"6. Quality Checks","text":"<pre><code>checks:\n  - check: not_null\n    columns: [id, customer_name, total_amount]\n  - check: row_count\n    min: 1\n  - check: expression\n    expr: total_amount &gt; 0\n</code></pre> <ul> <li>Ensures no NULL values in key columns</li> <li>Verifies at least 1 row output</li> <li>Validates business rule (positive totals)</li> </ul>"},{"location":"examples/basic-pipeline/#variations","title":"Variations","text":""},{"location":"examples/basic-pipeline/#output-to-csv","title":"Output to CSV","text":"<pre><code>sink:\n  type: file\n  path: output/completed_orders.csv\n  format: csv\n</code></pre>"},{"location":"examples/basic-pipeline/#with-variables","title":"With Variables","text":"<pre><code>source:\n  type: file\n  path: data/sales_${DATE}.csv\n  format: csv\n</code></pre> <pre><code>etlx run pipelines/basic.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"examples/basic-pipeline/#dry-run","title":"Dry Run","text":"<p>Preview without writing output:</p> <pre><code>etlx run pipelines/basic.yml --dry-run\n</code></pre>"},{"location":"examples/basic-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Source Join - Combine multiple data sources</li> <li>Aggregation - Compute metrics and summaries</li> <li>Transforms Reference - All available transforms</li> </ul>"},{"location":"examples/cloud-etl/","title":"Cloud ETL Example","text":"<p>This example demonstrates a production-ready pipeline that reads from cloud storage (S3), processes with a distributed engine (Spark), and writes to a data warehouse (Snowflake).</p>"},{"location":"examples/cloud-etl/#overview","title":"Overview","text":"<p>Scenario: Daily ETL pipeline that:</p> <ol> <li>Reads raw event data from S3</li> <li>Joins with dimension tables from S3</li> <li>Aggregates metrics</li> <li>Loads to Snowflake data warehouse</li> </ol>"},{"location":"examples/cloud-etl/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/cloud-etl/#aws-configuration","title":"AWS Configuration","text":"<pre><code>export AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"examples/cloud-etl/#snowflake-configuration","title":"Snowflake Configuration","text":"<pre><code>export SNOWFLAKE_ACCOUNT=xy12345.us-east-1\nexport SNOWFLAKE_USER=etl_user\nexport SNOWFLAKE_PASSWORD=your_password\nexport SNOWFLAKE_DATABASE=analytics\nexport SNOWFLAKE_SCHEMA=public\nexport SNOWFLAKE_WAREHOUSE=etl_warehouse\n</code></pre>"},{"location":"examples/cloud-etl/#data-architecture","title":"Data Architecture","text":"<pre><code>S3 Raw Layer                    Snowflake\n\u251c\u2500\u2500 events/                     \u251c\u2500\u2500 raw.events\n\u2502   \u2514\u2500\u2500 date=2025-01-15/       \u251c\u2500\u2500 dim.products\n\u2502       \u2514\u2500\u2500 *.parquet          \u251c\u2500\u2500 dim.customers\n\u251c\u2500\u2500 products/                   \u2514\u2500\u2500 analytics.daily_metrics\n\u2502   \u2514\u2500\u2500 products.parquet\n\u2514\u2500\u2500 customers/\n    \u2514\u2500\u2500 customers.parquet\n</code></pre>"},{"location":"examples/cloud-etl/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Create <code>pipelines/cloud_etl.yml</code>:</p> <pre><code>name: daily_cloud_etl\ndescription: Process daily events from S3 to Snowflake\nengine: spark\n\n# Environment variables for dates\n# Run with: etlx run pipeline.yml --var DATE=2025-01-15\n\nsource:\n  type: file\n  path: s3://data-lake/raw/events/date=${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  # Filter valid events\n  - op: filter\n    predicate: |\n      event_type IN ('purchase', 'view', 'click')\n      AND user_id IS NOT NULL\n\n  # Join with product dimension\n  - op: join\n    right:\n      type: file\n      path: s3://data-lake/dim/products/products.parquet\n      format: parquet\n    on: [product_id]\n    how: left\n\n  # Join with customer dimension\n  - op: join\n    right:\n      type: file\n      path: s3://data-lake/dim/customers/customers.parquet\n      format: parquet\n    on: [user_id]\n    how: left\n\n  # Rename joined columns\n  - op: rename\n    columns:\n      name: product_name\n      name_1: customer_name\n      category: product_category\n      segment: customer_segment\n\n  # Calculate derived metrics\n  - op: derive_column\n    name: event_date\n    expr: date(event_timestamp)\n\n  - op: derive_column\n    name: event_hour\n    expr: hour(event_timestamp)\n\n  - op: derive_column\n    name: revenue\n    expr: |\n      case\n        when event_type = 'purchase' then quantity * unit_price\n        else 0\n      end\n\n  # Aggregate daily metrics\n  - op: aggregate\n    group_by:\n      - event_date\n      - event_hour\n      - product_category\n      - customer_segment\n      - event_type\n    aggregations:\n      event_count: count(*)\n      unique_users: count(distinct user_id)\n      total_revenue: sum(revenue)\n      total_quantity: sum(quantity)\n\n# Data quality checks\nchecks:\n  - check: not_null\n    columns: [event_date, event_type, event_count]\n\n  - check: row_count\n    min: 1\n\n  - check: expression\n    expr: total_revenue &gt;= 0\n\n  - check: accepted_values\n    column: event_type\n    values: [purchase, view, click]\n\n# Write to Snowflake\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.daily_metrics\n  mode: merge\n  merge_keys: [event_date, event_hour, product_category, customer_segment, event_type]\n</code></pre>"},{"location":"examples/cloud-etl/#running-the-pipeline","title":"Running the Pipeline","text":""},{"location":"examples/cloud-etl/#daily-execution","title":"Daily Execution","text":"<pre><code># Today's date\netlx run pipelines/cloud_etl.yml --var DATE=$(date +%Y-%m-%d)\n\n# Specific date\netlx run pipelines/cloud_etl.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"examples/cloud-etl/#with-json-output-for-monitoring","title":"With JSON Output (for monitoring)","text":"<pre><code>etlx run pipelines/cloud_etl.yml --var DATE=2025-01-15 --json\n</code></pre> <p>Output:</p> <pre><code>{\n  \"pipeline_name\": \"daily_cloud_etl\",\n  \"status\": \"SUCCESS\",\n  \"duration_ms\": 45230.5,\n  \"rows_processed\": 1250000,\n  \"rows_written\": 8640,\n  \"checks_passed\": 4,\n  \"checks_failed\": 0\n}\n</code></pre>"},{"location":"examples/cloud-etl/#incremental-loading-variant","title":"Incremental Loading Variant","text":"<p>For incremental loads, use merge mode with date filtering:</p> <pre><code>name: incremental_cloud_etl\ndescription: Incremental load with watermark\n\nsource:\n  type: database\n  connection: snowflake\n  query: |\n    SELECT MAX(event_timestamp) as watermark\n    FROM analytics.events\n\n# Store watermark for use in main source\n# This example shows the pattern - actual implementation varies\n</code></pre>"},{"location":"examples/cloud-etl/#multi-region-pipeline","title":"Multi-Region Pipeline","text":"<p>Process data from multiple regions in parallel:</p> <pre><code>name: multi_region_etl\ndescription: Process all regions\n\nsource:\n  type: file\n  path: s3://data-lake/raw/events/region=${REGION}/date=${DATE}/*.parquet\n  format: parquet\n\n# ... transforms ...\n\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.regional_metrics_${REGION}\n  mode: replace\n</code></pre> <p>Run for each region:</p> <pre><code>for region in us-east us-west eu-west ap-south; do\n  etlx run pipelines/multi_region.yml \\\n    --var DATE=2025-01-15 \\\n    --var REGION=$region &amp;\ndone\nwait\n</code></pre>"},{"location":"examples/cloud-etl/#cost-optimization","title":"Cost Optimization","text":""},{"location":"examples/cloud-etl/#1-use-appropriate-spark-configuration","title":"1. Use Appropriate Spark Configuration","text":"<pre><code># For small jobs\nexport SPARK_EXECUTOR_INSTANCES=2\nexport SPARK_EXECUTOR_MEMORY=4g\n\n# For large jobs\nexport SPARK_EXECUTOR_INSTANCES=10\nexport SPARK_EXECUTOR_MEMORY=16g\n</code></pre>"},{"location":"examples/cloud-etl/#2-partition-output-data","title":"2. Partition Output Data","text":"<pre><code>sink:\n  type: file\n  path: s3://data-lake/processed/metrics/\n  format: parquet\n  options:\n    partition_by: [event_date, product_category]\n</code></pre>"},{"location":"examples/cloud-etl/#3-use-columnar-formats","title":"3. Use Columnar Formats","text":"<p>Always use Parquet for cloud storage:</p> <pre><code>source:\n  type: file\n  path: s3://bucket/data/*.parquet\n  format: parquet  # Not CSV!\n</code></pre>"},{"location":"examples/cloud-etl/#error-handling","title":"Error Handling","text":""},{"location":"examples/cloud-etl/#retry-logic","title":"Retry Logic","text":"<p>Wrap in a shell script with retries:</p> <pre><code>#!/bin/bash\nMAX_RETRIES=3\nRETRY_DELAY=300\n\nfor i in $(seq 1 $MAX_RETRIES); do\n  if etlx run pipelines/cloud_etl.yml --var DATE=$1; then\n    echo \"Success on attempt $i\"\n    exit 0\n  fi\n  echo \"Attempt $i failed, retrying in ${RETRY_DELAY}s...\"\n  sleep $RETRY_DELAY\ndone\n\necho \"Failed after $MAX_RETRIES attempts\"\nexit 1\n</code></pre>"},{"location":"examples/cloud-etl/#dead-letter-queue","title":"Dead Letter Queue","text":"<p>For failed records:</p> <pre><code># Main pipeline writes successes\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.metrics\n\n# Separate pipeline for failures\n# (would be implemented via quality check failure handling)\n</code></pre>"},{"location":"examples/cloud-etl/#monitoring","title":"Monitoring","text":""},{"location":"examples/cloud-etl/#cloudwatch-integration","title":"CloudWatch Integration","text":"<p>Send metrics to CloudWatch:</p> <pre><code>#!/bin/bash\nRESULT=$(etlx run pipelines/cloud_etl.yml --var DATE=$1 --json)\n\n# Extract metrics\nDURATION=$(echo $RESULT | jq -r '.duration_ms')\nROWS=$(echo $RESULT | jq -r '.rows_written')\nSTATUS=$(echo $RESULT | jq -r '.status')\n\n# Send to CloudWatch\naws cloudwatch put-metric-data \\\n  --namespace \"ETLX/Pipelines\" \\\n  --metric-name \"Duration\" \\\n  --value $DURATION \\\n  --unit Milliseconds \\\n  --dimensions Pipeline=daily_cloud_etl\n\naws cloudwatch put-metric-data \\\n  --namespace \"ETLX/Pipelines\" \\\n  --metric-name \"RowsWritten\" \\\n  --value $ROWS \\\n  --dimensions Pipeline=daily_cloud_etl\n</code></pre>"},{"location":"examples/cloud-etl/#bigquery-variant","title":"BigQuery Variant","text":"<p>For Google Cloud:</p> <pre><code>name: gcs_to_bigquery\nengine: bigquery\n\nsource:\n  type: file\n  path: gs://data-lake/raw/events/date=${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  # Same transforms...\n\nsink:\n  type: database\n  connection: bigquery\n  table: analytics.daily_metrics\n  mode: replace\n  options:\n    partition_field: event_date\n    clustering_fields: [product_category, customer_segment]\n</code></pre>"},{"location":"examples/cloud-etl/#next-steps","title":"Next Steps","text":"<ul> <li>Airflow DAG Example - Orchestrate cloud pipelines</li> <li>Spark Backend - Spark configuration</li> <li>Snowflake Backend - Snowflake setup</li> <li>Production Best Practices</li> </ul>"},{"location":"examples/db-to-lake/","title":"Database to Data Lake Example","text":"<p>This example demonstrates extracting data from a relational database and loading it into a cloud data lake.</p>"},{"location":"examples/db-to-lake/#overview","title":"Overview","text":"<p>Scenario: Extract transactional data from PostgreSQL and load to S3 data lake in Parquet format for analytics.</p> <p>Pattern: Database \u2192 Transform \u2192 Data Lake (ELT)</p>"},{"location":"examples/db-to-lake/#architecture","title":"Architecture","text":"<pre><code>PostgreSQL (OLTP)     S3 Data Lake\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   orders    \u2502       \u2502 raw/        \u2502\n\u2502   products  \u2502  \u2500\u2500\u2500\u25ba \u2502 processed/  \u2502\n\u2502   customers \u2502       \u2502 analytics/  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/db-to-lake/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/db-to-lake/#postgresql-connection","title":"PostgreSQL Connection","text":"<pre><code>export POSTGRES_HOST=db.example.com\nexport POSTGRES_PORT=5432\nexport POSTGRES_USER=etl_reader\nexport POSTGRES_PASSWORD=secret\nexport POSTGRES_DATABASE=production\n</code></pre>"},{"location":"examples/db-to-lake/#aws-credentials","title":"AWS Credentials","text":"<pre><code>export AWS_ACCESS_KEY_ID=AKIA...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"examples/db-to-lake/#pipeline-configurations","title":"Pipeline Configurations","text":""},{"location":"examples/db-to-lake/#extract-orders","title":"Extract Orders","text":"<p><code>pipelines/extract_orders.yml</code>:</p> <pre><code>name: extract_orders_to_lake\ndescription: Extract daily orders to S3 data lake\nengine: duckdb\n\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT\n      order_id,\n      customer_id,\n      order_date,\n      status,\n      total_amount,\n      shipping_address,\n      created_at,\n      updated_at\n    FROM orders\n    WHERE order_date = '${DATE}'\n      AND status != 'deleted'\n\ntransforms:\n  # Add extraction metadata\n  - op: derive_column\n    name: extracted_at\n    expr: current_timestamp\n\n  - op: derive_column\n    name: extract_date\n    expr: date('${DATE}')\n\nchecks:\n  - check: not_null\n    columns: [order_id, customer_id, total_amount]\n\n  - check: row_count\n    min: 1\n\nsink:\n  type: file\n  path: s3://data-lake/raw/orders/date=${DATE}/orders.parquet\n  format: parquet\n  mode: replace\n</code></pre>"},{"location":"examples/db-to-lake/#extract-customers-full-snapshot","title":"Extract Customers (Full Snapshot)","text":"<p><code>pipelines/extract_customers.yml</code>:</p> <pre><code>name: extract_customers_to_lake\ndescription: Full snapshot of customers to S3\nengine: duckdb\n\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT\n      customer_id,\n      email,\n      first_name,\n      last_name,\n      phone,\n      region,\n      segment,\n      lifetime_value,\n      created_at,\n      updated_at\n    FROM customers\n    WHERE status = 'active'\n\ntransforms:\n  - op: derive_column\n    name: extracted_at\n    expr: current_timestamp\n\n  # Hash PII for analytics\n  - op: derive_column\n    name: email_hash\n    expr: md5(lower(email))\n\n  # Remove raw PII for analytics layer\n  - op: select\n    columns:\n      - customer_id\n      - email_hash\n      - first_name\n      - region\n      - segment\n      - lifetime_value\n      - created_at\n      - extracted_at\n\nchecks:\n  - check: not_null\n    columns: [customer_id, region]\n\n  - check: unique\n    columns: [customer_id]\n\nsink:\n  type: file\n  path: s3://data-lake/raw/customers/snapshot.parquet\n  format: parquet\n  mode: replace\n</code></pre>"},{"location":"examples/db-to-lake/#incremental-extract-with-change-data-capture","title":"Incremental Extract with Change Data Capture","text":"<p><code>pipelines/extract_incremental.yml</code>:</p> <pre><code>name: incremental_orders\ndescription: Extract only changed orders since last run\nengine: duckdb\n\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT\n      order_id,\n      customer_id,\n      order_date,\n      status,\n      total_amount,\n      updated_at\n    FROM orders\n    WHERE updated_at &gt;= '${LAST_EXTRACT}'\n      AND updated_at &lt; '${CURRENT_EXTRACT}'\n\ntransforms:\n  - op: derive_column\n    name: cdc_operation\n    expr: |\n      case\n        when created_at &gt;= '${LAST_EXTRACT}' then 'INSERT'\n        else 'UPDATE'\n      end\n\n  - op: derive_column\n    name: extracted_at\n    expr: timestamp('${CURRENT_EXTRACT}')\n\nchecks:\n  - check: not_null\n    columns: [order_id, cdc_operation]\n\nsink:\n  type: file\n  path: s3://data-lake/cdc/orders/${CURRENT_EXTRACT}/changes.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/db-to-lake/#running-the-pipelines","title":"Running the Pipelines","text":""},{"location":"examples/db-to-lake/#daily-full-extract","title":"Daily Full Extract","text":"<pre><code># Extract today's orders\netlx run pipelines/extract_orders.yml --var DATE=$(date +%Y-%m-%d)\n\n# Extract customer snapshot\netlx run pipelines/extract_customers.yml\n</code></pre>"},{"location":"examples/db-to-lake/#incremental-extract","title":"Incremental Extract","text":"<pre><code># Get last extract timestamp from watermark table or file\nLAST_EXTRACT=$(cat /var/etlx/watermarks/orders.txt)\nCURRENT_EXTRACT=$(date -u +%Y-%m-%dT%H:%M:%S)\n\n# Run incremental\netlx run pipelines/extract_incremental.yml \\\n  --var LAST_EXTRACT=$LAST_EXTRACT \\\n  --var CURRENT_EXTRACT=$CURRENT_EXTRACT\n\n# Update watermark\necho $CURRENT_EXTRACT &gt; /var/etlx/watermarks/orders.txt\n</code></pre>"},{"location":"examples/db-to-lake/#orchestration-script","title":"Orchestration Script","text":"<pre><code>#!/bin/bash\n# extract_to_lake.sh\n\nset -e\n\nDATE=${1:-$(date +%Y-%m-%d)}\nLOG_DIR=/var/log/etlx\n\necho \"$(date): Starting extraction for $DATE\"\n\n# Extract orders\nif etlx run pipelines/extract_orders.yml --var DATE=$DATE --json &gt; $LOG_DIR/orders_$DATE.json; then\n    echo \"$(date): Orders extracted successfully\"\nelse\n    echo \"$(date): Orders extraction failed\"\n    exit 1\nfi\n\n# Extract products\nif etlx run pipelines/extract_products.yml --json &gt; $LOG_DIR/products_$DATE.json; then\n    echo \"$(date): Products extracted successfully\"\nelse\n    echo \"$(date): Products extraction failed\"\n    exit 1\nfi\n\n# Extract customers\nif etlx run pipelines/extract_customers.yml --json &gt; $LOG_DIR/customers_$DATE.json; then\n    echo \"$(date): Customers extracted successfully\"\nelse\n    echo \"$(date): Customers extraction failed\"\n    exit 1\nfi\n\necho \"$(date): All extractions completed successfully\"\n</code></pre>"},{"location":"examples/db-to-lake/#data-lake-organization","title":"Data Lake Organization","text":"<pre><code>s3://data-lake/\n\u251c\u2500\u2500 raw/                          # Raw extracts\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2502   \u251c\u2500\u2500 date=2025-01-15/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 orders.parquet\n\u2502   \u2502   \u2514\u2500\u2500 date=2025-01-16/\n\u2502   \u2502       \u2514\u2500\u2500 orders.parquet\n\u2502   \u251c\u2500\u2500 customers/\n\u2502   \u2502   \u2514\u2500\u2500 snapshot.parquet\n\u2502   \u2514\u2500\u2500 products/\n\u2502       \u2514\u2500\u2500 snapshot.parquet\n\u251c\u2500\u2500 cdc/                          # Change data capture\n\u2502   \u2514\u2500\u2500 orders/\n\u2502       \u251c\u2500\u2500 2025-01-15T00:00:00/\n\u2502       \u2514\u2500\u2500 2025-01-15T06:00:00/\n\u2514\u2500\u2500 processed/                    # Transformed data\n    \u2514\u2500\u2500 daily_metrics/\n        \u2514\u2500\u2500 date=2025-01-15/\n</code></pre>"},{"location":"examples/db-to-lake/#partitioning-strategy","title":"Partitioning Strategy","text":"<p>For large tables, partition by date:</p> <pre><code>sink:\n  type: file\n  path: s3://data-lake/raw/events/\n  format: parquet\n  options:\n    partition_by: [date, region]\n</code></pre> <p>Output structure:</p> <pre><code>s3://data-lake/raw/events/\n\u251c\u2500\u2500 date=2025-01-15/\n\u2502   \u251c\u2500\u2500 region=north/\n\u2502   \u2502   \u2514\u2500\u2500 part-0001.parquet\n\u2502   \u2514\u2500\u2500 region=south/\n\u2502       \u2514\u2500\u2500 part-0001.parquet\n\u2514\u2500\u2500 date=2025-01-16/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"examples/db-to-lake/#monitoring","title":"Monitoring","text":""},{"location":"examples/db-to-lake/#json-output-for-metrics","title":"JSON Output for Metrics","text":"<pre><code>RESULT=$(etlx run pipelines/extract_orders.yml --var DATE=$DATE --json)\n\nROWS=$(echo $RESULT | jq -r '.rows_written')\nDURATION=$(echo $RESULT | jq -r '.duration_ms')\n\n# Log to CloudWatch\naws cloudwatch put-metric-data \\\n  --namespace \"ETLX/DataLake\" \\\n  --metric-name \"RowsExtracted\" \\\n  --value $ROWS \\\n  --dimensions Table=orders,Date=$DATE\n</code></pre>"},{"location":"examples/db-to-lake/#next-steps","title":"Next Steps","text":"<ul> <li>Cloud ETL Example - Full cloud pipeline</li> <li>Airflow DAG Example - Orchestration</li> <li>Production Best Practices</li> </ul>"},{"location":"examples/multi-source-join/","title":"Multi-Source Join Example","text":"<p>This example demonstrates how to combine data from multiple sources using joins, creating an enriched dataset from orders, customers, and products.</p>"},{"location":"examples/multi-source-join/#overview","title":"Overview","text":"<p>Goal: Create an enriched order report combining:</p> <ul> <li>Order data (transactions)</li> <li>Customer data (demographics)</li> <li>Product data (catalog information)</li> </ul> <p>Output: A denormalized table with complete order information.</p>"},{"location":"examples/multi-source-join/#sample-data","title":"Sample Data","text":""},{"location":"examples/multi-source-join/#orders-dataorderscsv","title":"Orders (<code>data/orders.csv</code>)","text":"<pre><code>order_id,customer_id,product_id,quantity,order_date,status\n1001,C001,P001,2,2025-01-15,completed\n1002,C002,P003,1,2025-01-15,completed\n1003,C001,P002,3,2025-01-16,completed\n1004,C003,P001,1,2025-01-16,pending\n1005,C002,P004,2,2025-01-17,completed\n1006,C004,P002,1,2025-01-17,completed\n1007,C001,P003,2,2025-01-18,cancelled\n1008,C005,P001,4,2025-01-18,completed\n</code></pre>"},{"location":"examples/multi-source-join/#customers-datacustomerscsv","title":"Customers (<code>data/customers.csv</code>)","text":"<pre><code>customer_id,name,email,region,signup_date\nC001,Alice Smith,alice@example.com,North,2024-06-15\nC002,Bob Johnson,bob@example.com,South,2024-08-22\nC003,Carol White,carol@example.com,East,2024-09-10\nC004,David Brown,david@example.com,West,2024-11-05\nC005,Eve Davis,eve@example.com,North,2025-01-02\n</code></pre>"},{"location":"examples/multi-source-join/#products-dataproductscsv","title":"Products (<code>data/products.csv</code>)","text":"<pre><code>product_id,name,category,unit_price,cost\nP001,Widget A,Electronics,29.99,15.00\nP002,Gadget B,Electronics,49.99,25.00\nP003,Service C,Services,99.99,40.00\nP004,Widget D,Electronics,19.99,10.00\nP005,Tool E,Hardware,39.99,20.00\n</code></pre>"},{"location":"examples/multi-source-join/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Create <code>pipelines/order_report.yml</code>:</p> <pre><code>name: enriched_order_report\ndescription: Join orders with customer and product data\nengine: duckdb\n\n# Start with orders\nsource:\n  type: file\n  path: data/orders.csv\n  format: csv\n\ntransforms:\n  # Filter to completed orders only\n  - op: filter\n    predicate: status = 'completed'\n\n  # Join with customers\n  - op: join\n    right:\n      type: file\n      path: data/customers.csv\n      format: csv\n    on: [customer_id]\n    how: left\n\n  # Join with products\n  - op: join\n    right:\n      type: file\n      path: data/products.csv\n      format: csv\n    on: [product_id]\n    how: left\n\n  # Calculate derived fields\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  - op: derive_column\n    name: profit\n    expr: quantity * (unit_price - cost)\n\n  - op: derive_column\n    name: profit_margin\n    expr: (unit_price - cost) / unit_price\n\n  # Select and rename final columns\n  - op: select\n    columns:\n      - order_id\n      - order_date\n      - customer_id\n      - name        # customer name from join\n      - email\n      - region\n      - product_id\n      - name        # product name - will need rename\n      - category\n      - quantity\n      - unit_price\n      - line_total\n      - profit\n      - profit_margin\n\n  # Rename duplicate 'name' columns\n  - op: rename\n    columns:\n      name: product_name\n      name_1: customer_name\n\n  # Sort by order date\n  - op: sort\n    by:\n      - column: order_date\n        order: desc\n      - column: order_id\n        order: asc\n\n# Quality checks\nchecks:\n  - check: not_null\n    columns: [order_id, customer_id, product_id, line_total]\n\n  - check: expression\n    expr: line_total &gt; 0\n\n  - check: expression\n    expr: profit_margin &gt;= 0 AND profit_margin &lt;= 1\n\n# Write enriched data\nsink:\n  type: file\n  path: output/order_report.parquet\n  format: parquet\n</code></pre>"},{"location":"examples/multi-source-join/#alternative-explicit-column-selection","title":"Alternative: Explicit Column Selection","text":"<p>To avoid column name conflicts, use explicit selection:</p> <pre><code>transforms:\n  # Join with customers\n  - op: join\n    right:\n      type: file\n      path: data/customers.csv\n      format: csv\n    on: [customer_id]\n    how: left\n\n  # Rename immediately after join\n  - op: rename\n    columns:\n      name: customer_name\n\n  # Join with products\n  - op: join\n    right:\n      type: file\n      path: data/products.csv\n      format: csv\n    on: [product_id]\n    how: left\n\n  # Rename product name\n  - op: rename\n    columns:\n      name: product_name\n</code></pre>"},{"location":"examples/multi-source-join/#running-the-pipeline","title":"Running the Pipeline","text":"<pre><code># Validate\netlx validate pipelines/order_report.yml\n\n# Execute\netlx run pipelines/order_report.yml\n</code></pre>"},{"location":"examples/multi-source-join/#expected-output","title":"Expected Output","text":"order_id order_date customer_name region product_name category quantity line_total profit profit_margin 1008 2025-01-18 Eve Davis North Widget A Electronics 4 119.96 59.96 0.50 1006 2025-01-17 David Brown West Gadget B Electronics 1 49.99 24.99 0.50 1005 2025-01-17 Bob Johnson South Widget D Electronics 2 39.98 19.98 0.50 1003 2025-01-16 Alice Smith North Gadget B Electronics 3 149.97 74.97 0.50 1002 2025-01-15 Bob Johnson South Service C Services 1 99.99 59.99 0.60 1001 2025-01-15 Alice Smith North Widget A Electronics 2 59.98 29.98 0.50"},{"location":"examples/multi-source-join/#join-types-explained","title":"Join Types Explained","text":""},{"location":"examples/multi-source-join/#inner-join","title":"Inner Join","text":"<p>Only matching rows from both tables:</p> <pre><code>- op: join\n  right: ...\n  on: [customer_id]\n  how: inner  # Only orders with matching customers\n</code></pre>"},{"location":"examples/multi-source-join/#left-join","title":"Left Join","text":"<p>All rows from left table, matching from right:</p> <pre><code>- op: join\n  right: ...\n  on: [customer_id]\n  how: left  # All orders, customer data where available\n</code></pre>"},{"location":"examples/multi-source-join/#right-join","title":"Right Join","text":"<p>All rows from right table, matching from left:</p> <pre><code>- op: join\n  right: ...\n  on: [customer_id]\n  how: right  # All customers, their orders where available\n</code></pre>"},{"location":"examples/multi-source-join/#outer-join","title":"Outer Join","text":"<p>All rows from both tables:</p> <pre><code>- op: join\n  right: ...\n  on: [customer_id]\n  how: outer  # All orders and all customers\n</code></pre>"},{"location":"examples/multi-source-join/#multiple-join-keys","title":"Multiple Join Keys","text":"<p>For composite keys:</p> <pre><code>- op: join\n  right:\n    type: file\n    path: data/inventory.csv\n    format: csv\n  on: [product_id, warehouse_id]  # Multiple columns\n  how: inner\n</code></pre>"},{"location":"examples/multi-source-join/#handling-missing-data","title":"Handling Missing Data","text":"<p>After a left join, the right side may have NULLs:</p> <pre><code>transforms:\n  - op: join\n    right: ...\n    on: [customer_id]\n    how: left\n\n  # Fill NULL values from failed joins\n  - op: fill_null\n    columns:\n      customer_name: Unknown Customer\n      region: Unknown\n</code></pre>"},{"location":"examples/multi-source-join/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/multi-source-join/#1-filter-before-joining","title":"1. Filter Before Joining","text":"<p>Reduce data volume before expensive joins:</p> <pre><code>transforms:\n  # Filter first\n  - op: filter\n    predicate: status = 'completed' AND order_date &gt;= '2025-01-01'\n\n  # Then join (fewer rows to process)\n  - op: join\n    right: ...\n</code></pre>"},{"location":"examples/multi-source-join/#2-join-smaller-tables","title":"2. Join Smaller Tables","text":"<p>Put larger table on the left, smaller on right:</p> <pre><code># Good: Large orders table on left\nsource:\n  type: file\n  path: data/orders.csv  # 1M rows\n\ntransforms:\n  - op: join\n    right:\n      type: file\n      path: data/products.csv  # 1K rows (smaller)\n</code></pre>"},{"location":"examples/multi-source-join/#3-select-only-needed-columns","title":"3. Select Only Needed Columns","text":"<p>Reduce memory by selecting early:</p> <pre><code>transforms:\n  - op: join\n    right:\n      type: file\n      path: data/customers.csv\n      format: csv\n    on: [customer_id]\n    how: left\n\n  # Select immediately after join\n  - op: select\n    columns: [order_id, customer_id, name, region]\n</code></pre>"},{"location":"examples/multi-source-join/#next-steps","title":"Next Steps","text":"<ul> <li>Aggregation Example - Compute metrics from joined data</li> <li>Join Transform Reference - Full documentation</li> <li>Performance Best Practices</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to ETLX! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<ul> <li> <p> Installation</p> <p>Install ETLX and optional dependencies for your use case.</p> <p> Install ETLX</p> </li> <li> <p> Quick Start</p> <p>Create and run your first pipeline in 5 minutes.</p> <p> Quick Start</p> </li> <li> <p> Your First Pipeline</p> <p>Detailed walkthrough of building a pipeline from scratch.</p> <p> First Pipeline</p> </li> <li> <p> Project Structure</p> <p>Learn the recommended layout for ETLX projects.</p> <p> Project Structure</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.12 or later - ETLX uses modern Python features</li> <li>pip or uv - For package installation</li> <li>Basic Python knowledge - Familiarity with Python basics helps</li> </ul>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this section, you'll know how to:</p> <ol> <li>Install ETLX with the backends you need</li> <li>Create a new ETLX project with sample data</li> <li>Write pipeline configurations in YAML</li> <li>Run pipelines from the command line</li> <li>Organize your pipelines for maintainability</li> </ol>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>If you're new to ETLX, start with the Installation guide, then follow the Quick Start tutorial.</p> <p>If you're familiar with ETL concepts and want to dive deeper, check out the User Guide after completing this section.</p>"},{"location":"getting-started/first-pipeline/","title":"Your First Pipeline","text":"<p>This tutorial walks you through building a complete pipeline from scratch, explaining each component in detail.</p>"},{"location":"getting-started/first-pipeline/#the-scenario","title":"The Scenario","text":"<p>You have sales data in CSV format and need to:</p> <ol> <li>Filter out invalid records</li> <li>Calculate additional metrics</li> <li>Aggregate by category</li> <li>Validate the output quality</li> <li>Save as Parquet for analysis</li> </ol>"},{"location":"getting-started/first-pipeline/#step-1-create-a-new-pipeline-file","title":"Step 1: Create a New Pipeline File","text":"<p>Create a new file <code>pipelines/sales_report.yml</code>:</p> <pre><code>name: sales_report\ndescription: Generate sales summary report by category\nengine: duckdb\n</code></pre>"},{"location":"getting-started/first-pipeline/#pipeline-metadata","title":"Pipeline Metadata","text":"Field Required Description <code>name</code> Yes Unique identifier for the pipeline <code>description</code> No Human-readable description <code>engine</code> No Compute backend (default: <code>duckdb</code>)"},{"location":"getting-started/first-pipeline/#step-2-define-the-source","title":"Step 2: Define the Source","text":"<p>Add a source to read the sales data:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n</code></pre>"},{"location":"getting-started/first-pipeline/#source-options","title":"Source Options","text":"<p>For file sources:</p> Field Required Description <code>type</code> Yes Source type: <code>file</code>, <code>database</code> <code>path</code> Yes File path (local or cloud) <code>format</code> No File format: <code>csv</code>, <code>parquet</code>, <code>json</code> (default: <code>parquet</code>) <p>Cloud Paths</p> <p>Use cloud URIs for remote data:</p> <ul> <li>S3: <code>s3://bucket/path/data.parquet</code></li> <li>GCS: <code>gs://bucket/path/data.parquet</code></li> <li>Azure: <code>abfs://container@account.dfs.core.windows.net/path/data.parquet</code></li> </ul>"},{"location":"getting-started/first-pipeline/#step-3-add-transforms","title":"Step 3: Add Transforms","text":"<p>Transforms are applied in order. Each transform takes the output of the previous step.</p>"},{"location":"getting-started/first-pipeline/#filter-invalid-records","title":"Filter Invalid Records","text":"<p>Remove records with non-positive amounts:</p> <pre><code>transforms:\n  - op: filter\n    predicate: amount &gt; 0\n</code></pre> <p>The <code>predicate</code> uses SQL-like syntax. Supported operators:</p> <ul> <li>Comparison: <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code></li> <li>Logical: <code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li>Null checks: <code>IS NULL</code>, <code>IS NOT NULL</code></li> </ul>"},{"location":"getting-started/first-pipeline/#calculate-metrics","title":"Calculate Metrics","text":"<p>Add computed columns:</p> <pre><code>  - op: derive_column\n    name: total_with_tax\n    expr: amount * 1.1\n\n  - op: derive_column\n    name: profit_margin\n    expr: (amount - cost) / amount\n</code></pre>"},{"location":"getting-started/first-pipeline/#aggregate-by-category","title":"Aggregate by Category","text":"<p>Group and summarize:</p> <pre><code>  - op: aggregate\n    group_by: [category]\n    aggs:\n      total_revenue: sum(amount)\n      total_with_tax: sum(total_with_tax)\n      avg_order_value: avg(amount)\n      order_count: count(*)\n</code></pre> <p>Supported aggregation functions:</p> Function Description <code>sum(col)</code> Sum of values <code>avg(col)</code> Average/mean <code>min(col)</code> Minimum value <code>max(col)</code> Maximum value <code>count(*)</code> Count all rows <code>count(col)</code> Count non-null values"},{"location":"getting-started/first-pipeline/#sort-the-results","title":"Sort the Results","text":"<p>Order by total revenue descending:</p> <pre><code>  - op: sort\n    by: [total_revenue]\n    descending: true\n</code></pre>"},{"location":"getting-started/first-pipeline/#step-4-add-quality-checks","title":"Step 4: Add Quality Checks","text":"<p>Quality checks validate your output data:</p> <pre><code>checks:\n  # Ensure key columns have no nulls\n  - type: not_null\n    columns: [category, total_revenue]\n\n  # Ensure we have at least one category\n  - type: row_count\n    min: 1\n\n  # Ensure revenue is positive\n  - type: expression\n    expr: total_revenue &gt; 0\n</code></pre>"},{"location":"getting-started/first-pipeline/#available-check-types","title":"Available Check Types","text":"Check Description <code>not_null</code> Verify columns have no null values <code>unique</code> Verify uniqueness <code>row_count</code> Validate row count bounds <code>accepted_values</code> Check against whitelist <code>expression</code> Custom SQL predicate"},{"location":"getting-started/first-pipeline/#step-5-define-the-sink","title":"Step 5: Define the Sink","text":"<p>Specify where to write the output:</p> <pre><code>sink:\n  type: file\n  path: data/output/sales_report.parquet\n  format: parquet\n</code></pre>"},{"location":"getting-started/first-pipeline/#sink-options","title":"Sink Options","text":"Field Required Description <code>type</code> Yes Sink type: <code>file</code>, <code>database</code> <code>path</code> Yes Output path <code>format</code> No Output format (default: <code>parquet</code>) <code>partition_by</code> No Columns to partition by <code>mode</code> No <code>overwrite</code> or <code>append</code> (default: <code>overwrite</code>)"},{"location":"getting-started/first-pipeline/#complete-pipeline","title":"Complete Pipeline","text":"<p>Here's the complete pipeline:</p> pipelines/sales_report.yml<pre><code>name: sales_report\ndescription: Generate sales summary report by category\nengine: duckdb\n\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\ntransforms:\n  # Clean data\n  - op: filter\n    predicate: amount &gt; 0\n\n  # Calculate metrics\n  - op: derive_column\n    name: total_with_tax\n    expr: amount * 1.1\n\n  # Aggregate by category\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total_revenue: sum(amount)\n      total_with_tax: sum(total_with_tax)\n      avg_order_value: avg(amount)\n      order_count: count(*)\n\n  # Sort by revenue\n  - op: sort\n    by: [total_revenue]\n    descending: true\n\nchecks:\n  - type: not_null\n    columns: [category, total_revenue]\n  - type: row_count\n    min: 1\n  - type: expression\n    expr: total_revenue &gt; 0\n\nsink:\n  type: file\n  path: data/output/sales_report.parquet\n  format: parquet\n</code></pre>"},{"location":"getting-started/first-pipeline/#run-the-pipeline","title":"Run the Pipeline","text":""},{"location":"getting-started/first-pipeline/#validate-first","title":"Validate First","text":"<p>Check the configuration:</p> <pre><code>etlx validate pipelines/sales_report.yml\n</code></pre>"},{"location":"getting-started/first-pipeline/#dry-run","title":"Dry Run","text":"<p>Execute without writing output:</p> <pre><code>etlx run pipelines/sales_report.yml --dry-run\n</code></pre>"},{"location":"getting-started/first-pipeline/#full-run","title":"Full Run","text":"<p>Execute the complete pipeline:</p> <pre><code>etlx run pipelines/sales_report.yml\n</code></pre>"},{"location":"getting-started/first-pipeline/#verbose-output","title":"Verbose Output","text":"<p>See detailed logs:</p> <pre><code>etlx run pipelines/sales_report.yml --verbose\n</code></pre>"},{"location":"getting-started/first-pipeline/#using-python-instead","title":"Using Python Instead","text":"<p>The same pipeline in Python:</p> <pre><code>from etlx import Pipeline\nfrom etlx.config.models import FileSource, FileSink\nfrom etlx.config.transforms import (\n    FilterTransform,\n    DeriveColumnTransform,\n    AggregateTransform,\n    SortTransform,\n)\nfrom etlx.config.checks import NotNullCheck, RowCountCheck, ExpressionCheck\n\npipeline = (\n    Pipeline(\"sales_report\", description=\"Generate sales summary\", engine=\"duckdb\")\n    .source(FileSource(path=\"data/sales.csv\", format=\"csv\"))\n    .transform(FilterTransform(predicate=\"amount &gt; 0\"))\n    .transform(DeriveColumnTransform(name=\"total_with_tax\", expr=\"amount * 1.1\"))\n    .transform(AggregateTransform(\n        group_by=[\"category\"],\n        aggs={\n            \"total_revenue\": \"sum(amount)\",\n            \"total_with_tax\": \"sum(total_with_tax)\",\n            \"avg_order_value\": \"avg(amount)\",\n            \"order_count\": \"count(*)\",\n        }\n    ))\n    .transform(SortTransform(by=[\"total_revenue\"], descending=True))\n    .check(NotNullCheck(columns=[\"category\", \"total_revenue\"]))\n    .check(RowCountCheck(min=1))\n    .check(ExpressionCheck(expr=\"total_revenue &gt; 0\"))\n    .sink(FileSink(path=\"data/output/sales_report.parquet\"))\n)\n\nresult = pipeline.run()\nprint(f\"Pipeline {'succeeded' if result.succeeded else 'failed'}\")\nprint(f\"Processed {result.rows_processed} rows in {result.duration_ms:.1f}ms\")\n</code></pre>"},{"location":"getting-started/first-pipeline/#next-steps","title":"Next Steps","text":"<p>Now that you understand pipeline basics:</p> <ul> <li>Project Structure - Organize larger projects</li> <li>Transforms - Learn all 12 transforms</li> <li>Quality Checks - Advanced validation</li> <li>Examples - Real-world patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers how to install ETLX and its optional dependencies.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install ETLX with the default backends (DuckDB and Polars):</p> pipuvpipx (CLI only) <pre><code>pip install etlx\n</code></pre> <pre><code>uv pip install etlx\n</code></pre> <pre><code>pipx install etlx\n</code></pre> <p>This gives you:</p> <ul> <li>DuckDB backend (default)</li> <li>Polars backend</li> <li>CLI tools (<code>etlx run</code>, <code>etlx init</code>, etc.)</li> <li>Python API</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that ETLX is installed correctly:</p> <pre><code>etlx --version\n</code></pre> <p>You should see output like:</p> <pre><code>etlx version 0.1.0\n</code></pre> <p>Check available backends:</p> <pre><code>etlx info --backends --check\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>ETLX uses optional dependencies to keep the base installation lightweight. Install only what you need.</p>"},{"location":"getting-started/installation/#cloud-storage","title":"Cloud Storage","text":"<p>For reading/writing to cloud storage:</p> AWS S3Google Cloud StorageAzure ADLS <pre><code>pip install etlx[aws]\n</code></pre> <p>Includes <code>s3fs</code> and <code>boto3</code> for S3 access.</p> <pre><code>pip install etlx[gcp]\n</code></pre> <p>Includes <code>gcsfs</code> and <code>google-cloud-storage</code>.</p> <pre><code>pip install etlx[azure]\n</code></pre> <p>Includes <code>adlfs</code> and <code>azure-storage-blob</code>.</p>"},{"location":"getting-started/installation/#additional-compute-backends","title":"Additional Compute Backends","text":"<p>For distributed or alternative compute engines:</p> Apache SparkDataFusionpandas <pre><code>pip install etlx[spark]\n</code></pre> <p>Requires Java 8+ to be installed.</p> <pre><code>pip install etlx[datafusion]\n</code></pre> <p>Apache Arrow-native query engine.</p> <pre><code>pip install etlx[pandas]\n</code></pre> <p>For pandas-based processing.</p>"},{"location":"getting-started/installation/#cloud-data-warehouses","title":"Cloud Data Warehouses","text":"<p>For connecting to cloud data warehouses:</p> <pre><code># Snowflake\npip install etlx[snowflake]\n\n# Google BigQuery\npip install etlx[bigquery]\n\n# Trino\npip install etlx[trino]\n</code></pre>"},{"location":"getting-started/installation/#databases","title":"Databases","text":"<p>For connecting to relational databases:</p> <pre><code># PostgreSQL\npip install etlx[postgres]\n\n# MySQL\npip install etlx[mysql]\n\n# ClickHouse\npip install etlx[clickhouse]\n</code></pre>"},{"location":"getting-started/installation/#multiple-extras","title":"Multiple Extras","text":"<p>Install multiple extras at once:</p> <pre><code>pip install etlx[aws,spark,snowflake]\n</code></pre>"},{"location":"getting-started/installation/#everything","title":"Everything","text":"<p>Install all optional dependencies:</p> <pre><code>pip install etlx[all]\n</code></pre> <p>Large Installation</p> <p>The <code>[all]</code> extra installs many dependencies including Spark. Only use this if you need everything.</p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to ETLX:</p> <pre><code># Clone the repository\ngit clone https://github.com/etlx/etlx.git\ncd etlx\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # or `.venv\\Scripts\\activate` on Windows\n\n# Install in development mode\npip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors for optional backends:</p> <pre><code>ImportError: No module named 'ibis.backends.snowflake'\n</code></pre> <p>Install the required extra:</p> <pre><code>pip install etlx[snowflake]\n</code></pre>"},{"location":"getting-started/installation/#duckdb-version-conflicts","title":"DuckDB Version Conflicts","text":"<p>If you have version conflicts with DuckDB:</p> <pre><code>pip install etlx --upgrade\n</code></pre>"},{"location":"getting-started/installation/#spark-java-requirements","title":"Spark Java Requirements","text":"<p>Spark requires Java 8 or later. Check your Java version:</p> <pre><code>java -version\n</code></pre> <p>Set <code>JAVA_HOME</code> if needed:</p> <pre><code>export JAVA_HOME=/path/to/java\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that ETLX is installed, continue to the Quick Start to create your first pipeline.</p>"},{"location":"getting-started/project-structure/","title":"Project Structure","text":"<p>Learn how to organize ETLX projects for maintainability and scalability.</p>"},{"location":"getting-started/project-structure/#recommended-layout","title":"Recommended Layout","text":"<pre><code>my_project/\n\u251c\u2500\u2500 pipelines/              # Pipeline configurations\n\u2502   \u251c\u2500\u2500 daily/             # Daily pipelines\n\u2502   \u2502   \u251c\u2500\u2500 sales_etl.yml\n\u2502   \u2502   \u2514\u2500\u2500 inventory_sync.yml\n\u2502   \u251c\u2500\u2500 weekly/            # Weekly pipelines\n\u2502   \u2502   \u2514\u2500\u2500 reports.yml\n\u2502   \u2514\u2500\u2500 adhoc/             # One-off pipelines\n\u2502       \u2514\u2500\u2500 migration.yml\n\u251c\u2500\u2500 data/                   # Data directory\n\u2502   \u251c\u2500\u2500 input/             # Input data (gitignored)\n\u2502   \u251c\u2500\u2500 output/            # Output data (gitignored)\n\u2502   \u2514\u2500\u2500 samples/           # Sample data for testing\n\u251c\u2500\u2500 scripts/               # Python scripts\n\u2502   \u251c\u2500\u2500 custom_transforms.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 dags/                  # Airflow DAGs (if using Airflow)\n\u2502   \u2514\u2500\u2500 etlx_dag.py\n\u251c\u2500\u2500 tests/                 # Pipeline tests\n\u2502   \u251c\u2500\u2500 test_sales_etl.py\n\u2502   \u2514\u2500\u2500 fixtures/\n\u251c\u2500\u2500 .env                   # Environment variables\n\u251c\u2500\u2500 .env.example           # Example env file\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"getting-started/project-structure/#pipeline-organization","title":"Pipeline Organization","text":""},{"location":"getting-started/project-structure/#by-schedule","title":"By Schedule","text":"<p>Group pipelines by their run schedule:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 hourly/\n\u251c\u2500\u2500 daily/\n\u251c\u2500\u2500 weekly/\n\u2514\u2500\u2500 monthly/\n</code></pre>"},{"location":"getting-started/project-structure/#by-domain","title":"By Domain","text":"<p>Group pipelines by business domain:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 sales/\n\u251c\u2500\u2500 marketing/\n\u251c\u2500\u2500 finance/\n\u2514\u2500\u2500 operations/\n</code></pre>"},{"location":"getting-started/project-structure/#by-data-source","title":"By Data Source","text":"<p>Group pipelines by their data source:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 postgres/\n\u251c\u2500\u2500 salesforce/\n\u251c\u2500\u2500 s3/\n\u2514\u2500\u2500 api/\n</code></pre>"},{"location":"getting-started/project-structure/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/project-structure/#the-env-file","title":"The <code>.env</code> File","text":"<p>Store configuration in <code>.env</code>:</p> .env<pre><code># Database connections\nPOSTGRES_URL=postgresql://user:pass@localhost:5432/db\nSNOWFLAKE_ACCOUNT=abc123.us-east-1\n\n# Cloud storage\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\n\n# Pipeline variables\nDEFAULT_DATE=2025-01-01\nOUTPUT_BUCKET=s3://my-data-lake/output\n</code></pre>"},{"location":"getting-started/project-structure/#using-in-pipelines","title":"Using in Pipelines","text":"<p>Reference variables in YAML:</p> <pre><code>source:\n  type: database\n  connection: ${POSTGRES_URL}\n\nsink:\n  type: file\n  path: ${OUTPUT_BUCKET}/sales/${DATE}/data.parquet\n</code></pre>"},{"location":"getting-started/project-structure/#the-envexample-file","title":"The <code>.env.example</code> File","text":"<p>Document required variables (commit this file):</p> .env.example<pre><code># Database connections\nPOSTGRES_URL=postgresql://user:pass@host:5432/db\n\n# Cloud storage (AWS)\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\n\n# Pipeline variables\nDEFAULT_DATE=2025-01-01\n</code></pre>"},{"location":"getting-started/project-structure/#git-ignore-patterns","title":"Git Ignore Patterns","text":"<p>Recommended <code>.gitignore</code>:</p> .gitignore<pre><code># Data files\ndata/input/\ndata/output/\n*.parquet\n*.csv\n!data/samples/*.csv\n\n# Environment\n.env\n.env.local\n.env.*.local\n\n# Python\n__pycache__/\n*.pyc\n*.pyo\n.venv/\nvenv/\n\n# IDE\n.idea/\n.vscode/\n*.swp\n\n# Logs\n*.log\nlogs/\n\n# OS\n.DS_Store\nThumbs.db\n</code></pre>"},{"location":"getting-started/project-structure/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"getting-started/project-structure/#base-override-pattern","title":"Base + Override Pattern","text":"<p>Create a base configuration and override for environments:</p> pipelines/base/sales.yml<pre><code>name: sales_etl\nengine: duckdb\n\nsource:\n  type: file\n  path: ${INPUT_PATH}\n  format: parquet\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n\nsink:\n  type: file\n  path: ${OUTPUT_PATH}\n  format: parquet\n</code></pre> <pre><code># Development\netlx run pipelines/base/sales.yml \\\n  --var INPUT_PATH=data/samples/sales.parquet \\\n  --var OUTPUT_PATH=data/output/sales.parquet\n\n# Production\netlx run pipelines/base/sales.yml \\\n  --var INPUT_PATH=s3://prod-bucket/sales/ \\\n  --var OUTPUT_PATH=s3://prod-bucket/output/sales/\n</code></pre>"},{"location":"getting-started/project-structure/#shared-transforms","title":"Shared Transforms","text":"<p>For complex transform sequences, use Python:</p> scripts/transforms.py<pre><code>from etlx.config.transforms import (\n    FilterTransform,\n    DeriveColumnTransform,\n    AggregateTransform,\n)\n\n# Reusable transform sequences\nCLEAN_SALES = [\n    FilterTransform(predicate=\"amount &gt; 0\"),\n    FilterTransform(predicate=\"status != 'cancelled'\"),\n    DeriveColumnTransform(name=\"net_amount\", expr=\"amount - discount\"),\n]\n\nAGGREGATE_BY_REGION = AggregateTransform(\n    group_by=[\"region\"],\n    aggs={\n        \"total_sales\": \"sum(net_amount)\",\n        \"order_count\": \"count(*)\",\n    }\n)\n</code></pre>"},{"location":"getting-started/project-structure/#testing-pipelines","title":"Testing Pipelines","text":""},{"location":"getting-started/project-structure/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py           # Shared fixtures\n\u251c\u2500\u2500 fixtures/\n\u2502   \u2514\u2500\u2500 sample_sales.csv  # Test data\n\u251c\u2500\u2500 test_sales_etl.py\n\u2514\u2500\u2500 test_transforms.py\n</code></pre>"},{"location":"getting-started/project-structure/#sample-test","title":"Sample Test","text":"tests/test_sales_etl.py<pre><code>import pytest\nfrom pathlib import Path\nfrom etlx import Pipeline\n\nFIXTURES = Path(__file__).parent / \"fixtures\"\n\ndef test_sales_pipeline_runs():\n    \"\"\"Test that the sales pipeline runs successfully.\"\"\"\n    pipeline = Pipeline.from_yaml(\n        \"pipelines/daily/sales_etl.yml\",\n        variables={\n            \"INPUT_PATH\": str(FIXTURES / \"sample_sales.csv\"),\n            \"OUTPUT_PATH\": \"/tmp/test_output.parquet\",\n        }\n    )\n\n    result = pipeline.run()\n\n    assert result.succeeded\n    assert result.rows_processed &gt; 0\n\ndef test_sales_pipeline_checks_pass():\n    \"\"\"Test that quality checks pass.\"\"\"\n    pipeline = Pipeline.from_yaml(\"pipelines/daily/sales_etl.yml\")\n    result = pipeline.run(dry_run=True)\n\n    assert result.check_results[\"all_passed\"]\n</code></pre>"},{"location":"getting-started/project-structure/#airflow-integration","title":"Airflow Integration","text":""},{"location":"getting-started/project-structure/#dag-structure","title":"DAG Structure","text":"dags/etlx_dag.py<pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom etlx.integrations.airflow import etlx_task\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 2,\n}\n\nwith DAG(\n    \"etlx_sales_pipeline\",\n    default_args=default_args,\n    schedule_interval=\"@daily\",\n    start_date=datetime(2025, 1, 1),\n    catchup=False,\n) as dag:\n\n    @etlx_task(config_path=\"pipelines/daily/sales_etl.yml\")\n    def run_sales_etl(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    sales_task = PythonOperator(\n        task_id=\"sales_etl\",\n        python_callable=run_sales_etl,\n    )\n</code></pre>"},{"location":"getting-started/project-structure/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/project-structure/#1-use-descriptive-names","title":"1. Use Descriptive Names","text":"<pre><code># Good\nname: daily_sales_aggregation_by_region\n\n# Bad\nname: pipeline1\n</code></pre>"},{"location":"getting-started/project-structure/#2-document-your-pipelines","title":"2. Document Your Pipelines","text":"<pre><code>name: sales_etl\ndescription: |\n  Daily sales ETL pipeline that:\n  - Reads from PostgreSQL sales table\n  - Filters cancelled orders\n  - Aggregates by region and product category\n  - Writes to S3 data lake\n\n  Owner: data-team@company.com\n  Schedule: Daily at 6 AM UTC\n</code></pre>"},{"location":"getting-started/project-structure/#3-version-your-schemas","title":"3. Version Your Schemas","text":"<p>When schemas change, version your pipelines:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 sales_v1.yml  # Original schema\n\u251c\u2500\u2500 sales_v2.yml  # New schema with additional columns\n\u2514\u2500\u2500 sales.yml     # Symlink to current version\n</code></pre>"},{"location":"getting-started/project-structure/#4-use-consistent-naming","title":"4. Use Consistent Naming","text":"Convention Example Pipeline files <code>snake_case.yml</code> Pipeline names <code>snake_case</code> Column names <code>snake_case</code> Variables <code>UPPER_SNAKE_CASE</code>"},{"location":"getting-started/project-structure/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Deep dive into configuration</li> <li>Airflow Integration - Orchestrate with Airflow</li> <li>Best Practices - Production patterns</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with ETLX in 5 minutes.</p>"},{"location":"getting-started/quickstart/#create-a-new-project","title":"Create a New Project","text":"<p>Use the <code>etlx init</code> command to create a new project with sample data:</p> <pre><code>etlx init my_project\ncd my_project\n</code></pre> <p>This creates:</p> <pre><code>my_project/\n\u251c\u2500\u2500 pipelines/\n\u2502   \u2514\u2500\u2500 sample.yml      # Sample pipeline configuration\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 sales.csv       # Sample data to process\n\u251c\u2500\u2500 README.md           # Project documentation\n\u251c\u2500\u2500 .env                # Environment variables\n\u2514\u2500\u2500 .gitignore\n</code></pre>"},{"location":"getting-started/quickstart/#run-the-sample-pipeline","title":"Run the Sample Pipeline","text":"<p>Run the included sample pipeline:</p> <pre><code>etlx run pipelines/sample.yml\n</code></pre> <p>You'll see output like:</p> <pre><code>Running pipeline: sample_pipeline\n  Sample ETL pipeline - processes sales data\n  Engine: duckdb\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pipeline: sample_pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 SUCCESS                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duration: 245.3ms \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                              Steps\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Step                      \u2503 Type          \u2503 Status \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 read_source               \u2502 file          \u2502 OK     \u2502   45.2ms \u2502\n\u2502 transform_0_filter        \u2502 filter        \u2502 OK     \u2502    0.3ms \u2502\n\u2502 transform_1_derive_column \u2502 derive_column \u2502 OK     \u2502    0.2ms \u2502\n\u2502 transform_2_aggregate     \u2502 aggregate     \u2502 OK     \u2502    0.8ms \u2502\n\u2502 transform_3_sort          \u2502 sort          \u2502 OK     \u2502    0.1ms \u2502\n\u2502 quality_checks            \u2502 checks        \u2502 OK     \u2502   12.4ms \u2502\n\u2502 write_sink                \u2502 file          \u2502 OK     \u2502    8.1ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nQuality Checks: PASSED (2/2 passed)\n\nRows processed: 3\nRows written: 3\n</code></pre>"},{"location":"getting-started/quickstart/#examine-the-output","title":"Examine the Output","text":"<p>The pipeline created a Parquet file in <code>data/output/</code>:</p> <pre><code>ls data/output/\n# sales_summary.parquet\n</code></pre>"},{"location":"getting-started/quickstart/#understand-the-pipeline","title":"Understand the Pipeline","text":"<p>Open <code>pipelines/sample.yml</code> to see the configuration:</p> pipelines/sample.yml<pre><code>name: sample_pipeline\ndescription: Sample ETL pipeline - processes sales data\nengine: duckdb\n\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n\n  - op: derive_column\n    name: total_with_tax\n    expr: amount * 1.1\n\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total_sales: sum(amount)\n      total_with_tax: sum(total_with_tax)\n      order_count: count(*)\n\n  - op: sort\n    by: [total_sales]\n    descending: true\n\nchecks:\n  - type: not_null\n    columns: [category, total_sales]\n  - type: row_count\n    min: 1\n\nsink:\n  type: file\n  path: data/output/sales_summary.parquet\n  format: parquet\n</code></pre>"},{"location":"getting-started/quickstart/#key-sections","title":"Key Sections","text":"Section Description <code>name</code> Pipeline identifier <code>engine</code> Compute backend (duckdb, polars, spark) <code>source</code> Where to read data from <code>transforms</code> List of data transformations <code>checks</code> Data quality validations <code>sink</code> Where to write output"},{"location":"getting-started/quickstart/#modify-the-pipeline","title":"Modify the Pipeline","text":"<p>Try changing the pipeline:</p> <ol> <li> <p>Change the aggregation grouping:</p> <pre><code>- op: aggregate\n  group_by: [category, region]  # Add region\n  aggs:\n    total_sales: sum(amount)\n</code></pre> </li> <li> <p>Add a new filter:</p> <pre><code>- op: filter\n  predicate: category = 'Electronics'\n</code></pre> </li> <li> <p>Run again:</p> <pre><code>etlx run pipelines/sample.yml\n</code></pre> </li> </ol>"},{"location":"getting-started/quickstart/#use-variables","title":"Use Variables","text":"<p>Pass variables at runtime:</p> pipelines/sample.yml<pre><code>source:\n  type: file\n  path: data/${DATE}/sales.csv  # Use variable\n  format: csv\n</code></pre> <pre><code>etlx run pipelines/sample.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"getting-started/quickstart/#validate-without-running","title":"Validate Without Running","text":"<p>Check your configuration is valid without executing:</p> <pre><code>etlx validate pipelines/sample.yml\n</code></pre>"},{"location":"getting-started/quickstart/#dry-run","title":"Dry Run","text":"<p>Execute transforms without writing output:</p> <pre><code>etlx run pipelines/sample.yml --dry-run\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li> <p> Your First Pipeline</p> <p>Build a pipeline from scratch with detailed explanations.</p> <p> First Pipeline</p> </li> <li> <p> Configuration Guide</p> <p>Learn all the configuration options.</p> <p> Configuration</p> </li> <li> <p> Transforms</p> <p>Explore all 12 transform operations.</p> <p> Transforms</p> </li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>ETLX integrates with popular data orchestration and workflow tools. This section covers how to use ETLX within larger data ecosystems.</p>"},{"location":"integrations/#orchestration-platforms","title":"Orchestration Platforms","text":""},{"location":"integrations/#apache-airflow","title":"Apache Airflow","text":"<p>Run ETLX pipelines as Airflow tasks with proper dependency management, retries, and monitoring.</p> <p>Airflow Integration \u2192</p> <p>Features:</p> <ul> <li><code>@etlx_task</code> decorator for simple integration</li> <li>Pass Airflow variables to pipelines</li> <li>Automatic retry handling</li> <li>XCom integration for result passing</li> </ul> <pre><code>from airflow.decorators import dag\nfrom etlx.integrations.airflow import etlx_task\n\n@dag(schedule=\"@daily\")\ndef etl_pipeline():\n    @etlx_task(config=\"pipelines/sales.yml\")\n    def process_sales(**context):\n        return {\"date\": context[\"ds\"]}\n\n    process_sales()\n</code></pre>"},{"location":"integrations/#prefect-coming-soon","title":"Prefect (Coming Soon)","text":"<p>Integration with Prefect for modern workflow orchestration.</p>"},{"location":"integrations/#dagster-coming-soon","title":"Dagster (Coming Soon)","text":"<p>Integration with Dagster for software-defined assets.</p>"},{"location":"integrations/#data-platforms","title":"Data Platforms","text":""},{"location":"integrations/#dbt","title":"dbt","text":"<p>Use ETLX alongside dbt for transformation workflows.</p> <p>Pattern: ETLX for Ingestion, dbt for Transformation</p> <pre><code>Raw Sources \u2192 ETLX \u2192 Staging Tables \u2192 dbt \u2192 Marts\n</code></pre> <pre><code># ETLX: Load raw data\nname: load_raw_orders\nsource:\n  type: file\n  path: s3://bucket/orders/*.parquet\n  format: parquet\nsink:\n  type: database\n  connection: postgres\n  table: staging.raw_orders\n  mode: replace\n</code></pre> <pre><code>-- dbt: Transform\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    total_amount\nFROM {{ source('staging', 'raw_orders') }}\nWHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n</code></pre>"},{"location":"integrations/#great-expectations","title":"Great Expectations","text":"<p>Use Great Expectations for additional data validation.</p> <pre><code>from etlx import Pipeline\nimport great_expectations as gx\n\n# Run ETLX pipeline\npipeline = Pipeline.from_yaml(\"pipeline.yml\")\nresult = pipeline.run()\n\n# Validate with Great Expectations\ndf = result.to_dataframe()\ncontext = gx.get_context()\nvalidator = context.get_validator(df)\nvalidation_result = validator.validate()\n</code></pre>"},{"location":"integrations/#cloud-services","title":"Cloud Services","text":""},{"location":"integrations/#aws","title":"AWS","text":"<p>S3 Integration:</p> <pre><code>source:\n  type: file\n  path: s3://bucket/data/*.parquet\n  format: parquet\n</code></pre> <p>Environment Setup:</p> <pre><code>export AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"integrations/#google-cloud","title":"Google Cloud","text":"<p>GCS Integration:</p> <pre><code>source:\n  type: file\n  path: gs://bucket/data/*.parquet\n  format: parquet\n</code></pre> <p>BigQuery Backend:</p> <pre><code>engine: bigquery\nsource:\n  type: database\n  connection: bigquery\n  table: project.dataset.table\n</code></pre>"},{"location":"integrations/#azure","title":"Azure","text":"<p>Azure Blob Storage:</p> <pre><code>source:\n  type: file\n  path: az://container/data/*.parquet\n  format: parquet\n</code></pre>"},{"location":"integrations/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"integrations/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/etl.yml\nname: Run ETL Pipeline\n\non:\n  schedule:\n    - cron: '0 6 * * *'\n  workflow_dispatch:\n\njobs:\n  etl:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install ETLX\n        run: pip install etlx[duckdb]\n\n      - name: Validate Pipeline\n        run: etlx validate pipelines/daily.yml\n\n      - name: Run Pipeline\n        env:\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n        run: etlx run pipelines/daily.yml --json\n</code></pre>"},{"location":"integrations/#gitlab-ci","title":"GitLab CI","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - validate\n  - run\n\nvalidate:\n  stage: validate\n  image: python:3.12\n  script:\n    - pip install etlx[duckdb]\n    - etlx validate pipelines/*.yml\n\nrun:\n  stage: run\n  image: python:3.12\n  script:\n    - pip install etlx[duckdb]\n    - etlx run pipelines/daily.yml\n  only:\n    - schedules\n</code></pre>"},{"location":"integrations/#python-frameworks","title":"Python Frameworks","text":""},{"location":"integrations/#fastapi","title":"FastAPI","text":"<pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom etlx import Pipeline\n\napp = FastAPI()\n\n@app.post(\"/pipelines/{name}/run\")\nasync def run_pipeline(\n    name: str,\n    background_tasks: BackgroundTasks,\n    variables: dict = None\n):\n    def execute():\n        pipeline = Pipeline.from_yaml(f\"pipelines/{name}.yml\")\n        return pipeline.run(variables=variables)\n\n    background_tasks.add_task(execute)\n    return {\"status\": \"started\", \"pipeline\": name}\n</code></pre>"},{"location":"integrations/#flask","title":"Flask","text":"<pre><code>from flask import Flask, request, jsonify\nfrom etlx import Pipeline\n\napp = Flask(__name__)\n\n@app.route(\"/run/&lt;pipeline_name&gt;\", methods=[\"POST\"])\ndef run_pipeline(pipeline_name):\n    variables = request.json or {}\n\n    pipeline = Pipeline.from_yaml(f\"pipelines/{pipeline_name}.yml\")\n    result = pipeline.run(variables=variables)\n\n    return jsonify(result.to_dict())\n</code></pre>"},{"location":"integrations/#streamlit","title":"Streamlit","text":"<pre><code>import streamlit as st\nfrom etlx import Pipeline\n\nst.title(\"ETLX Dashboard\")\n\nconfig_file = st.selectbox(\"Pipeline\", [\"sales.yml\", \"inventory.yml\"])\nvariables = st.text_input(\"Variables (JSON)\", \"{}\")\n\nif st.button(\"Run Pipeline\"):\n    import json\n    vars_dict = json.loads(variables)\n\n    pipeline = Pipeline.from_yaml(f\"pipelines/{config_file}\")\n    result = pipeline.run(variables=vars_dict)\n\n    st.success(f\"Completed in {result.duration_ms:.1f}ms\")\n    st.metric(\"Rows Processed\", result.rows_processed)\n    st.metric(\"Rows Written\", result.rows_written)\n</code></pre>"},{"location":"integrations/#message-queues","title":"Message Queues","text":""},{"location":"integrations/#celery","title":"Celery","text":"<pre><code>from celery import Celery\nfrom etlx import Pipeline\n\napp = Celery('etlx_tasks', broker='redis://localhost:6379')\n\n@app.task\ndef run_etlx_pipeline(config_path: str, variables: dict = None):\n    pipeline = Pipeline.from_yaml(config_path)\n    result = pipeline.run(variables=variables)\n    return result.to_dict()\n\n# Trigger\nrun_etlx_pipeline.delay(\"pipelines/sales.yml\", {\"date\": \"2025-01-15\"})\n</code></pre>"},{"location":"integrations/#related","title":"Related","text":"<ul> <li>Airflow Integration - Detailed Airflow guide</li> <li>Python API - API reference</li> <li>Production Best Practices</li> </ul>"},{"location":"integrations/airflow/","title":"Airflow Integration","text":"<p>ETLX provides first-class integration with Apache Airflow for orchestrating data pipelines. Run ETLX pipelines as Airflow tasks with proper dependency management, retries, and monitoring.</p>"},{"location":"integrations/airflow/#installation","title":"Installation","text":"<pre><code>pip install etlx[airflow]\n# or\nuv add etlx[airflow]\n</code></pre>"},{"location":"integrations/airflow/#quick-start","title":"Quick Start","text":""},{"location":"integrations/airflow/#using-the-decorator","title":"Using the Decorator","text":"<p>The simplest way to run ETLX in Airflow:</p> <pre><code>from airflow.decorators import dag\nfrom datetime import datetime\nfrom etlx.integrations.airflow import etlx_task\n\n@dag(\n    schedule=\"@daily\",\n    start_date=datetime(2025, 1, 1),\n    catchup=False\n)\ndef sales_pipeline():\n\n    @etlx_task(config=\"pipelines/daily_sales.yml\")\n    def process_sales(**context):\n        # Return variables to pass to pipeline\n        return {\n            \"DATE\": context[\"ds\"],\n            \"REGION\": \"all\"\n        }\n\n    process_sales()\n\nsales_pipeline()\n</code></pre>"},{"location":"integrations/airflow/#using-the-operator","title":"Using the Operator","text":"<p>For more control, use the operator directly:</p> <pre><code>from airflow import DAG\nfrom datetime import datetime\nfrom etlx.integrations.airflow import ETLXOperator\n\nwith DAG(\n    \"sales_etl\",\n    schedule=\"@daily\",\n    start_date=datetime(2025, 1, 1),\n    catchup=False\n) as dag:\n\n    process_sales = ETLXOperator(\n        task_id=\"process_sales\",\n        config_path=\"pipelines/daily_sales.yml\",\n        variables={\n            \"DATE\": \"{{ ds }}\",\n            \"REGION\": \"{{ var.value.region }}\"\n        },\n        engine=\"duckdb\",\n        fail_on_checks=True\n    )\n</code></pre>"},{"location":"integrations/airflow/#etlxoperator-reference","title":"ETLXOperator Reference","text":""},{"location":"integrations/airflow/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>config_path</code> <code>str</code> Required Path to YAML config <code>variables</code> <code>dict</code> <code>None</code> Variables to pass (supports Jinja) <code>engine</code> <code>str</code> <code>None</code> Override engine <code>fail_on_checks</code> <code>bool</code> <code>True</code> Fail task on check failure <code>dry_run</code> <code>bool</code> <code>False</code> Execute without writing"},{"location":"integrations/airflow/#jinja-templating","title":"Jinja Templating","text":"<p>Variables support Airflow's Jinja templating:</p> <pre><code>ETLXOperator(\n    task_id=\"process\",\n    config_path=\"pipelines/sales.yml\",\n    variables={\n        \"DATE\": \"{{ ds }}\",\n        \"EXECUTION_DATE\": \"{{ execution_date }}\",\n        \"PREV_DATE\": \"{{ prev_ds }}\",\n        \"NEXT_DATE\": \"{{ next_ds }}\",\n        \"RUN_ID\": \"{{ run_id }}\",\n        \"DAG_ID\": \"{{ dag.dag_id }}\",\n        \"CUSTOM_VAR\": \"{{ var.value.my_variable }}\",\n        \"CONNECTION\": \"{{ conn.my_connection.host }}\"\n    }\n)\n</code></pre>"},{"location":"integrations/airflow/#etlx_task-decorator","title":"@etlx_task Decorator","text":""},{"location":"integrations/airflow/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>config</code> <code>str</code> Required Path to YAML config <code>engine</code> <code>str</code> <code>None</code> Override engine <code>fail_on_checks</code> <code>bool</code> <code>True</code> Fail on check failure"},{"location":"integrations/airflow/#return-values","title":"Return Values","text":"<p>The decorated function should return a dictionary of variables:</p> <pre><code>@etlx_task(config=\"pipelines/sales.yml\")\ndef process_sales(**context):\n    return {\n        \"DATE\": context[\"ds\"],\n        \"BATCH_ID\": context[\"run_id\"]\n    }\n</code></pre>"},{"location":"integrations/airflow/#xcom-integration","title":"XCom Integration","text":"<p>Results are automatically pushed to XCom:</p> <pre><code>@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef pipeline():\n\n    @etlx_task(config=\"pipelines/extract.yml\")\n    def extract(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @task\n    def log_results(result):\n        print(f\"Rows processed: {result['rows_processed']}\")\n        print(f\"Duration: {result['duration_ms']}ms\")\n\n    result = extract()\n    log_results(result)\n</code></pre>"},{"location":"integrations/airflow/#dag-patterns","title":"DAG Patterns","text":""},{"location":"integrations/airflow/#sequential-pipeline","title":"Sequential Pipeline","text":"<pre><code>from airflow.decorators import dag, task\nfrom etlx.integrations.airflow import etlx_task\n\n@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef sequential_pipeline():\n\n    @etlx_task(config=\"pipelines/extract.yml\")\n    def extract(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/transform.yml\")\n    def transform(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/load.yml\")\n    def load(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    extract() &gt;&gt; transform() &gt;&gt; load()\n</code></pre>"},{"location":"integrations/airflow/#parallel-processing","title":"Parallel Processing","text":"<pre><code>@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef parallel_pipeline():\n\n    @etlx_task(config=\"pipelines/sales.yml\")\n    def process_sales(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/inventory.yml\")\n    def process_inventory(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/aggregate.yml\")\n    def aggregate(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    # Parallel tasks feed into aggregate\n    [process_sales(), process_inventory()] &gt;&gt; aggregate()\n</code></pre>"},{"location":"integrations/airflow/#dynamic-task-mapping","title":"Dynamic Task Mapping","text":"<pre><code>@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef dynamic_pipeline():\n\n    @task\n    def get_regions():\n        return [\"north\", \"south\", \"east\", \"west\"]\n\n    @etlx_task(config=\"pipelines/regional_sales.yml\")\n    def process_region(region, **context):\n        return {\n            \"DATE\": context[\"ds\"],\n            \"REGION\": region\n        }\n\n    regions = get_regions()\n    process_region.expand(region=regions)\n</code></pre>"},{"location":"integrations/airflow/#conditional-execution","title":"Conditional Execution","text":"<pre><code>from airflow.operators.python import BranchPythonOperator\n\n@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef conditional_pipeline():\n\n    def choose_path(**context):\n        day = context[\"execution_date\"].weekday()\n        if day == 0:  # Monday\n            return \"full_refresh\"\n        return \"incremental\"\n\n    branch = BranchPythonOperator(\n        task_id=\"choose_path\",\n        python_callable=choose_path\n    )\n\n    @etlx_task(config=\"pipelines/full_refresh.yml\")\n    def full_refresh(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @etlx_task(config=\"pipelines/incremental.yml\")\n    def incremental(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    branch &gt;&gt; [full_refresh(), incremental()]\n</code></pre>"},{"location":"integrations/airflow/#error-handling","title":"Error Handling","text":""},{"location":"integrations/airflow/#retry-configuration","title":"Retry Configuration","text":"<pre><code>from airflow.decorators import dag\nfrom datetime import timedelta\n\n@dag(\n    schedule=\"@daily\",\n    start_date=datetime(2025, 1, 1),\n    default_args={\n        \"retries\": 3,\n        \"retry_delay\": timedelta(minutes=5),\n        \"retry_exponential_backoff\": True,\n        \"max_retry_delay\": timedelta(hours=1)\n    }\n)\ndef pipeline_with_retries():\n    @etlx_task(config=\"pipelines/sales.yml\")\n    def process(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    process()\n</code></pre>"},{"location":"integrations/airflow/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>from airflow.decorators import dag, task\nfrom etlx import Pipeline\nfrom etlx.exceptions import QualityCheckError\n\n@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef pipeline_with_error_handling():\n\n    @task\n    def run_pipeline(**context):\n        pipeline = Pipeline.from_yaml(\"pipelines/sales.yml\")\n\n        try:\n            result = pipeline.run(\n                variables={\"DATE\": context[\"ds\"]},\n                fail_on_checks=True\n            )\n            return result.to_dict()\n\n        except QualityCheckError as e:\n            # Log warning but don't fail\n            print(f\"Quality checks failed: {e}\")\n            return {\"status\": \"WARNING\", \"checks_failed\": len(e.failed_checks)}\n\n    run_pipeline()\n</code></pre>"},{"location":"integrations/airflow/#alerting-on-failure","title":"Alerting on Failure","text":"<pre><code>from airflow.decorators import dag\nfrom airflow.operators.email import EmailOperator\n\n@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef pipeline_with_alerts():\n\n    @etlx_task(config=\"pipelines/sales.yml\")\n    def process(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    alert = EmailOperator(\n        task_id=\"send_alert\",\n        to=\"team@example.com\",\n        subject=\"Pipeline Failed: {{ dag.dag_id }}\",\n        html_content=\"Task failed at {{ ts }}\",\n        trigger_rule=\"one_failed\"\n    )\n\n    process() &gt;&gt; alert\n</code></pre>"},{"location":"integrations/airflow/#best-practices","title":"Best Practices","text":""},{"location":"integrations/airflow/#1-use-variables-for-configuration","title":"1. Use Variables for Configuration","text":"<pre><code># airflow variables\n# etlx_database_url = postgresql://...\n# etlx_s3_bucket = my-bucket\n\n@etlx_task(config=\"pipelines/sales.yml\")\ndef process(**context):\n    from airflow.models import Variable\n    return {\n        \"DATABASE_URL\": Variable.get(\"etlx_database_url\"),\n        \"S3_BUCKET\": Variable.get(\"etlx_s3_bucket\"),\n        \"DATE\": context[\"ds\"]\n    }\n</code></pre>"},{"location":"integrations/airflow/#2-use-connections-for-secrets","title":"2. Use Connections for Secrets","text":"<pre><code>@task\ndef run_with_connection(**context):\n    from airflow.hooks.base import BaseHook\n    from etlx import Pipeline\n\n    conn = BaseHook.get_connection(\"my_database\")\n\n    import os\n    os.environ[\"POSTGRES_HOST\"] = conn.host\n    os.environ[\"POSTGRES_USER\"] = conn.login\n    os.environ[\"POSTGRES_PASSWORD\"] = conn.password\n    os.environ[\"POSTGRES_DATABASE\"] = conn.schema\n\n    pipeline = Pipeline.from_yaml(\"pipelines/sales.yml\")\n    return pipeline.run(variables={\"DATE\": context[\"ds\"]}).to_dict()\n</code></pre>"},{"location":"integrations/airflow/#3-organize-pipeline-files","title":"3. Organize Pipeline Files","text":"<pre><code>dags/\n\u251c\u2500\u2500 sales_dag.py\n\u251c\u2500\u2500 inventory_dag.py\n\u2514\u2500\u2500 pipelines/\n    \u251c\u2500\u2500 sales/\n    \u2502   \u251c\u2500\u2500 extract.yml\n    \u2502   \u251c\u2500\u2500 transform.yml\n    \u2502   \u2514\u2500\u2500 load.yml\n    \u2514\u2500\u2500 inventory/\n        \u251c\u2500\u2500 daily.yml\n        \u2514\u2500\u2500 weekly.yml\n</code></pre>"},{"location":"integrations/airflow/#4-use-slas","title":"4. Use SLAs","text":"<pre><code>@dag(\n    schedule=\"@daily\",\n    start_date=datetime(2025, 1, 1),\n    sla_miss_callback=sla_alert\n)\ndef pipeline_with_sla():\n\n    @etlx_task(\n        config=\"pipelines/sales.yml\",\n        sla=timedelta(hours=2)\n    )\n    def process(**context):\n        return {\"DATE\": context[\"ds\"]}\n</code></pre>"},{"location":"integrations/airflow/#monitoring","title":"Monitoring","text":""},{"location":"integrations/airflow/#task-metrics","title":"Task Metrics","text":"<p>Access metrics from XCom:</p> <pre><code>@dag(schedule=\"@daily\", start_date=datetime(2025, 1, 1))\ndef monitored_pipeline():\n\n    @etlx_task(config=\"pipelines/sales.yml\")\n    def process(**context):\n        return {\"DATE\": context[\"ds\"]}\n\n    @task\n    def log_metrics(result):\n        # Send to monitoring system\n        print(f\"Pipeline: {result['pipeline_name']}\")\n        print(f\"Duration: {result['duration_ms']}ms\")\n        print(f\"Rows: {result['rows_processed']} \u2192 {result['rows_written']}\")\n        print(f\"Checks: {result['checks_passed']}/{result['checks_passed'] + result['checks_failed']}\")\n\n    result = process()\n    log_metrics(result)\n</code></pre>"},{"location":"integrations/airflow/#custom-callbacks","title":"Custom Callbacks","text":"<pre><code>def on_success(context):\n    result = context[\"task_instance\"].xcom_pull()\n    # Send to monitoring\n    print(f\"Success: {result['rows_written']} rows\")\n\ndef on_failure(context):\n    # Send alert\n    print(f\"Failed: {context['exception']}\")\n\n@etlx_task(\n    config=\"pipelines/sales.yml\",\n    on_success_callback=on_success,\n    on_failure_callback=on_failure\n)\ndef process(**context):\n    return {\"DATE\": context[\"ds\"]}\n</code></pre>"},{"location":"integrations/airflow/#related","title":"Related","text":"<ul> <li>Integrations Overview - Other integrations</li> <li>Python API - ETLX API reference</li> <li>Production Best Practices</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Technical reference documentation for ETLX.</p>"},{"location":"reference/#reference-guides","title":"Reference Guides","text":"Guide Description Expressions SQL expression syntax Data Types Type system and mapping Environment Variables Configuration variables Troubleshooting Common issues and solutions"},{"location":"reference/#expression-language","title":"Expression Language","text":"<p>ETLX uses SQL-compatible expressions for filters, derived columns, and checks:</p> <pre><code>transforms:\n  - op: filter\n    predicate: amount &gt; 100 AND status = 'active'\n\n  - op: derive_column\n    name: total\n    expr: quantity * price * (1 - discount)\n</code></pre> <p>Expression Reference \u2192</p>"},{"location":"reference/#data-types","title":"Data Types","text":"<p>Standard types that map across all backends:</p> Type Description <code>string</code> Text data <code>int</code> Integer numbers <code>float</code> Floating-point <code>bool</code> Boolean <code>date</code> Calendar date <code>timestamp</code> Date and time <code>decimal</code> Precise decimal <p>Data Types Reference \u2192</p>"},{"location":"reference/#environment-variables","title":"Environment Variables","text":"<p>Configure connections and credentials via environment:</p> <pre><code>export POSTGRES_HOST=localhost\nexport POSTGRES_USER=etlx\nexport AWS_REGION=us-east-1\n</code></pre> <p>Environment Variables \u2192</p>"},{"location":"reference/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ul> <li>Installation problems</li> <li>Configuration errors</li> <li>Runtime failures</li> <li>Performance issues</li> </ul> <p>Troubleshooting Guide \u2192</p>"},{"location":"reference/data-types/","title":"Data Types Reference","text":"<p>ETLX supports standard data types that map to backend-specific types. This reference covers type handling across backends.</p>"},{"location":"reference/data-types/#standard-types","title":"Standard Types","text":"ETLX Type Description Example Values <code>string</code> Text data <code>\"hello\"</code>, <code>\"John Doe\"</code> <code>int</code> Integer numbers <code>42</code>, <code>-100</code>, <code>1000000</code> <code>float</code> Floating-point numbers <code>3.14</code>, <code>-0.001</code>, <code>1e10</code> <code>bool</code> Boolean values <code>true</code>, <code>false</code> <code>date</code> Calendar date <code>2025-01-15</code> <code>timestamp</code> Date and time <code>2025-01-15 14:30:00</code> <code>decimal</code> Precise decimal <code>99.99</code>, <code>1234.56789</code>"},{"location":"reference/data-types/#type-mapping-by-backend","title":"Type Mapping by Backend","text":""},{"location":"reference/data-types/#duckdb","title":"DuckDB","text":"ETLX Type DuckDB Type <code>string</code> <code>VARCHAR</code> <code>int</code> <code>BIGINT</code> <code>float</code> <code>DOUBLE</code> <code>bool</code> <code>BOOLEAN</code> <code>date</code> <code>DATE</code> <code>timestamp</code> <code>TIMESTAMP</code> <code>decimal</code> <code>DECIMAL</code>"},{"location":"reference/data-types/#polars","title":"Polars","text":"ETLX Type Polars Type <code>string</code> <code>Utf8</code> <code>int</code> <code>Int64</code> <code>float</code> <code>Float64</code> <code>bool</code> <code>Boolean</code> <code>date</code> <code>Date</code> <code>timestamp</code> <code>Datetime</code> <code>decimal</code> <code>Decimal</code>"},{"location":"reference/data-types/#postgresql","title":"PostgreSQL","text":"ETLX Type PostgreSQL Type <code>string</code> <code>TEXT</code> / <code>VARCHAR</code> <code>int</code> <code>BIGINT</code> <code>float</code> <code>DOUBLE PRECISION</code> <code>bool</code> <code>BOOLEAN</code> <code>date</code> <code>DATE</code> <code>timestamp</code> <code>TIMESTAMP</code> <code>decimal</code> <code>NUMERIC</code>"},{"location":"reference/data-types/#snowflake","title":"Snowflake","text":"ETLX Type Snowflake Type <code>string</code> <code>VARCHAR</code> <code>int</code> <code>INTEGER</code> <code>float</code> <code>FLOAT</code> <code>bool</code> <code>BOOLEAN</code> <code>date</code> <code>DATE</code> <code>timestamp</code> <code>TIMESTAMP_NTZ</code> <code>decimal</code> <code>NUMBER</code>"},{"location":"reference/data-types/#bigquery","title":"BigQuery","text":"ETLX Type BigQuery Type <code>string</code> <code>STRING</code> <code>int</code> <code>INT64</code> <code>float</code> <code>FLOAT64</code> <code>bool</code> <code>BOOL</code> <code>date</code> <code>DATE</code> <code>timestamp</code> <code>TIMESTAMP</code> <code>decimal</code> <code>NUMERIC</code>"},{"location":"reference/data-types/#pandas","title":"Pandas","text":"ETLX Type Pandas Type <code>string</code> <code>object</code> / <code>string</code> <code>int</code> <code>int64</code> <code>float</code> <code>float64</code> <code>bool</code> <code>bool</code> <code>date</code> <code>datetime64[ns]</code> <code>timestamp</code> <code>datetime64[ns]</code> <code>decimal</code> <code>float64</code>"},{"location":"reference/data-types/#type-casting","title":"Type Casting","text":""},{"location":"reference/data-types/#cast-transform","title":"Cast Transform","text":"<p>Explicitly convert column types:</p> <pre><code>transforms:\n  - op: cast\n    columns:\n      id: int\n      amount: float\n      is_active: bool\n      created_date: date\n      updated_at: timestamp\n</code></pre>"},{"location":"reference/data-types/#supported-cast-targets","title":"Supported Cast Targets","text":"<pre><code>transforms:\n  - op: cast\n    columns:\n      # String types\n      name: string\n      code: varchar\n\n      # Numeric types\n      id: int\n      id: integer\n      id: bigint\n      amount: float\n      amount: double\n      price: decimal\n\n      # Boolean\n      is_active: bool\n      is_active: boolean\n\n      # Date/Time\n      order_date: date\n      created_at: timestamp\n</code></pre>"},{"location":"reference/data-types/#cast-in-expressions","title":"Cast in Expressions","text":"<pre><code>transforms:\n  - op: derive_column\n    name: amount_str\n    expr: cast(amount as varchar)\n\n  - op: derive_column\n    name: year\n    expr: cast(extract(year from date) as integer)\n</code></pre>"},{"location":"reference/data-types/#type-inference","title":"Type Inference","text":""},{"location":"reference/data-types/#csv-files","title":"CSV Files","text":"<p>CSV files don't have type information. ETLX infers types:</p> <pre><code>source:\n  type: file\n  path: data.csv\n  format: csv\n  # Types are inferred from data\n</code></pre> <p>To override inference:</p> <pre><code>source:\n  type: file\n  path: data.csv\n  format: csv\n  options:\n    dtype:\n      id: int\n      amount: float\n      date: date\n</code></pre>"},{"location":"reference/data-types/#parquet-files","title":"Parquet Files","text":"<p>Parquet preserves types from the schema:</p> <pre><code>source:\n  type: file\n  path: data.parquet\n  format: parquet\n  # Types come from Parquet schema\n</code></pre>"},{"location":"reference/data-types/#json-files","title":"JSON Files","text":"<p>JSON types map naturally:</p> JSON Type ETLX Type <code>string</code> <code>string</code> <code>number</code> (integer) <code>int</code> <code>number</code> (decimal) <code>float</code> <code>boolean</code> <code>bool</code> <code>null</code> NULL <code>array</code> Backend-specific <code>object</code> Backend-specific"},{"location":"reference/data-types/#null-handling","title":"NULL Handling","text":""},{"location":"reference/data-types/#null-values","title":"NULL Values","text":"<p>All types can be NULL:</p> <pre><code>transforms:\n  - op: filter\n    predicate: email IS NOT NULL\n\n  - op: fill_null\n    columns:\n      email: \"unknown@example.com\"\n      amount: 0\n      is_active: false\n</code></pre>"},{"location":"reference/data-types/#null-safe-operations","title":"NULL-Safe Operations","text":"<pre><code>transforms:\n  # COALESCE returns first non-NULL\n  - op: derive_column\n    name: display_name\n    expr: coalesce(nickname, first_name, 'Anonymous')\n\n  # NULLIF returns NULL if equal\n  - op: derive_column\n    name: safe_ratio\n    expr: a / nullif(b, 0)\n</code></pre>"},{"location":"reference/data-types/#date-and-time-formats","title":"Date and Time Formats","text":""},{"location":"reference/data-types/#date-strings","title":"Date Strings","text":"<p>Supported formats:</p> <pre><code>2025-01-15          # ISO date\n2025/01/15          # Slash separator\n01-15-2025          # US format (with config)\n15-01-2025          # EU format (with config)\n</code></pre>"},{"location":"reference/data-types/#timestamp-strings","title":"Timestamp Strings","text":"<pre><code>2025-01-15 14:30:00           # Date and time\n2025-01-15T14:30:00           # ISO 8601\n2025-01-15T14:30:00Z          # UTC\n2025-01-15T14:30:00+05:00     # With timezone\n</code></pre>"},{"location":"reference/data-types/#date-parsing","title":"Date Parsing","text":"<pre><code>transforms:\n  - op: derive_column\n    name: parsed_date\n    expr: date('2025-01-15')\n\n  - op: derive_column\n    name: parsed_timestamp\n    expr: timestamp('2025-01-15 14:30:00')\n</code></pre>"},{"location":"reference/data-types/#numeric-precision","title":"Numeric Precision","text":""},{"location":"reference/data-types/#integer-types","title":"Integer Types","text":"Type Range <code>int</code> / <code>bigint</code> -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 <code>int32</code> -2,147,483,648 to 2,147,483,647 <code>int16</code> -32,768 to 32,767"},{"location":"reference/data-types/#decimal-precision","title":"Decimal Precision","text":"<p>For financial calculations, use <code>decimal</code>:</p> <pre><code>transforms:\n  - op: cast\n    columns:\n      price: decimal  # Full precision\n\n  - op: derive_column\n    name: total\n    expr: cast(quantity * price as decimal(10, 2))\n</code></pre>"},{"location":"reference/data-types/#boolean-representations","title":"Boolean Representations","text":""},{"location":"reference/data-types/#in-csv-files","title":"In CSV Files","text":"<p>Common boolean representations:</p> True Values False Values <code>true</code>, <code>True</code>, <code>TRUE</code> <code>false</code>, <code>False</code>, <code>FALSE</code> <code>1</code> <code>0</code> <code>yes</code>, <code>Yes</code>, <code>YES</code> <code>no</code>, <code>No</code>, <code>NO</code> <code>t</code>, <code>T</code> <code>f</code>, <code>F</code>"},{"location":"reference/data-types/#casting-to-boolean","title":"Casting to Boolean","text":"<pre><code>transforms:\n  - op: derive_column\n    name: is_active\n    expr: |\n      case\n        when status = 'active' then true\n        else false\n      end\n</code></pre>"},{"location":"reference/data-types/#complex-types","title":"Complex Types","text":""},{"location":"reference/data-types/#arrays-backend-dependent","title":"Arrays (Backend-Dependent)","text":"<pre><code># DuckDB\ntransforms:\n  - op: derive_column\n    name: first_tag\n    expr: tags[1]  # Array indexing\n\n# BigQuery\ntransforms:\n  - op: derive_column\n    name: tag_count\n    expr: ARRAY_LENGTH(tags)\n</code></pre>"},{"location":"reference/data-types/#json-backend-dependent","title":"JSON (Backend-Dependent)","text":"<pre><code># DuckDB\ntransforms:\n  - op: derive_column\n    name: user_name\n    expr: json_extract(metadata, '$.user.name')\n\n# Snowflake\ntransforms:\n  - op: derive_column\n    name: user_name\n    expr: metadata:user:name::string\n</code></pre>"},{"location":"reference/data-types/#type-compatibility","title":"Type Compatibility","text":""},{"location":"reference/data-types/#implicit-conversions","title":"Implicit Conversions","text":"<p>Some conversions happen automatically:</p> <pre><code>transforms:\n  # int + float = float\n  - op: derive_column\n    name: total\n    expr: quantity + 0.5  # quantity (int) + 0.5 (float) = float\n\n  # Comparisons work across compatible types\n  - op: filter\n    predicate: id = '123'  # Compares int to string (may cast)\n</code></pre>"},{"location":"reference/data-types/#explicit-conversions","title":"Explicit Conversions","text":"<p>When implicit conversion fails, use explicit cast:</p> <pre><code>transforms:\n  - op: derive_column\n    name: id_str\n    expr: cast(id as varchar) || '-' || category\n</code></pre>"},{"location":"reference/data-types/#related","title":"Related","text":"<ul> <li>Cast Transform - Type conversion</li> <li>Expressions Reference - Expression syntax</li> <li>Backend Documentation - Backend-specific types</li> </ul>"},{"location":"reference/environment-variables/","title":"Environment Variables Reference","text":"<p>ETLX uses environment variables for configuration, database connections, and cloud credentials.</p>"},{"location":"reference/environment-variables/#pipeline-variables","title":"Pipeline Variables","text":""},{"location":"reference/environment-variables/#variable-substitution","title":"Variable Substitution","text":"<p>Use <code>${VAR}</code> syntax in pipeline YAML:</p> <pre><code>name: ${PIPELINE_NAME:-default}\nsource:\n  type: file\n  path: ${INPUT_PATH}/data.parquet\nsink:\n  type: database\n  connection: ${DB_CONNECTION}\n  table: ${SCHEMA}.${TABLE}\n</code></pre>"},{"location":"reference/environment-variables/#default-values","title":"Default Values","text":"<p>Provide defaults with <code>:-</code> syntax:</p> <pre><code>engine: ${ENGINE:-duckdb}\nsource:\n  type: file\n  path: ${INPUT:-data/default.parquet}\n</code></pre>"},{"location":"reference/environment-variables/#setting-variables","title":"Setting Variables","text":"<pre><code># Command line\netlx run pipeline.yml --var DATE=2025-01-15 --var REGION=north\n\n# Environment\nexport DATE=2025-01-15\nexport REGION=north\netlx run pipeline.yml\n\n# .env file\necho \"DATE=2025-01-15\" &gt;&gt; .env\necho \"REGION=north\" &gt;&gt; .env\netlx run pipeline.yml\n</code></pre>"},{"location":"reference/environment-variables/#database-connections","title":"Database Connections","text":""},{"location":"reference/environment-variables/#postgresql","title":"PostgreSQL","text":"Variable Description Example <code>POSTGRES_HOST</code> Database host <code>localhost</code> <code>POSTGRES_PORT</code> Database port <code>5432</code> <code>POSTGRES_USER</code> Username <code>etlx_user</code> <code>POSTGRES_PASSWORD</code> Password <code>secret</code> <code>POSTGRES_DATABASE</code> Database name <code>analytics</code> <code>POSTGRES_SSLMODE</code> SSL mode <code>require</code> <code>DATABASE_URL</code> Full connection URL <code>postgresql://user:pass@host:5432/db</code> <pre><code># Individual variables\nexport POSTGRES_HOST=db.example.com\nexport POSTGRES_PORT=5432\nexport POSTGRES_USER=etlx_user\nexport POSTGRES_PASSWORD=secret\nexport POSTGRES_DATABASE=analytics\n\n# Or connection URL\nexport DATABASE_URL=postgresql://etlx_user:secret@db.example.com:5432/analytics\n</code></pre>"},{"location":"reference/environment-variables/#mysql","title":"MySQL","text":"Variable Description Example <code>MYSQL_HOST</code> Database host <code>localhost</code> <code>MYSQL_PORT</code> Database port <code>3306</code> <code>MYSQL_USER</code> Username <code>etlx_user</code> <code>MYSQL_PASSWORD</code> Password <code>secret</code> <code>MYSQL_DATABASE</code> Database name <code>analytics</code>"},{"location":"reference/environment-variables/#snowflake","title":"Snowflake","text":"Variable Description Example <code>SNOWFLAKE_ACCOUNT</code> Account identifier <code>xy12345.us-east-1</code> <code>SNOWFLAKE_USER</code> Username <code>etlx_user</code> <code>SNOWFLAKE_PASSWORD</code> Password <code>secret</code> <code>SNOWFLAKE_DATABASE</code> Database <code>analytics</code> <code>SNOWFLAKE_SCHEMA</code> Schema <code>public</code> <code>SNOWFLAKE_WAREHOUSE</code> Warehouse <code>compute_wh</code> <code>SNOWFLAKE_ROLE</code> Role <code>analyst</code> <code>SNOWFLAKE_PRIVATE_KEY_PATH</code> Key file path <code>/path/to/key.p8</code> <pre><code>export SNOWFLAKE_ACCOUNT=xy12345.us-east-1\nexport SNOWFLAKE_USER=etlx_user\nexport SNOWFLAKE_PASSWORD=secret\nexport SNOWFLAKE_DATABASE=analytics\nexport SNOWFLAKE_SCHEMA=public\nexport SNOWFLAKE_WAREHOUSE=etl_wh\n</code></pre>"},{"location":"reference/environment-variables/#bigquery","title":"BigQuery","text":"Variable Description Example <code>GOOGLE_APPLICATION_CREDENTIALS</code> Service account JSON <code>/path/to/sa.json</code> <code>BIGQUERY_PROJECT</code> GCP project ID <code>my-project-123</code> <code>BIGQUERY_DATASET</code> Default dataset <code>analytics</code> <code>BIGQUERY_LOCATION</code> Location <code>US</code> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\nexport BIGQUERY_PROJECT=my-project-123\nexport BIGQUERY_DATASET=analytics\n</code></pre>"},{"location":"reference/environment-variables/#clickhouse","title":"ClickHouse","text":"Variable Description Example <code>CLICKHOUSE_HOST</code> Server host <code>localhost</code> <code>CLICKHOUSE_PORT</code> HTTP port <code>8123</code> <code>CLICKHOUSE_USER</code> Username <code>default</code> <code>CLICKHOUSE_PASSWORD</code> Password <code>secret</code> <code>CLICKHOUSE_DATABASE</code> Database <code>analytics</code>"},{"location":"reference/environment-variables/#cloud-storage","title":"Cloud Storage","text":""},{"location":"reference/environment-variables/#aws-s3","title":"AWS S3","text":"Variable Description Example <code>AWS_ACCESS_KEY_ID</code> Access key <code>AKIA...</code> <code>AWS_SECRET_ACCESS_KEY</code> Secret key <code>wJal...</code> <code>AWS_SESSION_TOKEN</code> Session token (STS) <code>FwoG...</code> <code>AWS_REGION</code> Default region <code>us-east-1</code> <code>AWS_DEFAULT_REGION</code> Alternative region var <code>us-east-1</code> <pre><code>export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_REGION=us-east-1\n</code></pre> <p>Or use AWS CLI profiles:</p> <pre><code>export AWS_PROFILE=production\n</code></pre>"},{"location":"reference/environment-variables/#google-cloud-storage","title":"Google Cloud Storage","text":"Variable Description Example <code>GOOGLE_APPLICATION_CREDENTIALS</code> Service account <code>/path/to/sa.json</code> <code>GOOGLE_CLOUD_PROJECT</code> Project ID <code>my-project-123</code> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre> <p>Or use gcloud CLI:</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"reference/environment-variables/#azure-blob-storage","title":"Azure Blob Storage","text":"Variable Description Example <code>AZURE_STORAGE_ACCOUNT</code> Storage account <code>mystorageaccount</code> <code>AZURE_STORAGE_KEY</code> Access key <code>abc123...</code> <code>AZURE_STORAGE_CONNECTION_STRING</code> Connection string <code>DefaultEndpoints...</code> <pre><code>export AZURE_STORAGE_ACCOUNT=mystorageaccount\nexport AZURE_STORAGE_KEY=abc123...\n</code></pre>"},{"location":"reference/environment-variables/#spark-configuration","title":"Spark Configuration","text":"Variable Description Example <code>SPARK_MASTER</code> Spark master URL <code>local[*]</code>, <code>spark://host:7077</code> <code>SPARK_EXECUTOR_MEMORY</code> Executor memory <code>4g</code> <code>SPARK_EXECUTOR_CORES</code> Executor cores <code>2</code> <code>SPARK_EXECUTOR_INSTANCES</code> Number of executors <code>10</code> <code>SPARK_DRIVER_MEMORY</code> Driver memory <code>2g</code> <pre><code>export SPARK_MASTER=local[*]\nexport SPARK_EXECUTOR_MEMORY=8g\nexport SPARK_EXECUTOR_CORES=4\n</code></pre>"},{"location":"reference/environment-variables/#etlx-configuration","title":"ETLX Configuration","text":"Variable Description Default <code>ETLX_LOG_LEVEL</code> Logging level <code>INFO</code> <code>ETLX_CONFIG_DIR</code> Config directory <code>.</code> <code>ETLX_CACHE_DIR</code> Cache directory <code>~/.cache/etlx</code> <pre><code>export ETLX_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/environment-variables/#using-env-files","title":"Using .env Files","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nDATABASE_URL=postgresql://user:pass@localhost:5432/db\nAWS_REGION=us-east-1\nSNOWFLAKE_ACCOUNT=xy12345.us-east-1\nSNOWFLAKE_USER=etlx_user\nSNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}  # Reference secrets manager\n</code></pre> <p>ETLX automatically loads <code>.env</code> files.</p>"},{"location":"reference/environment-variables/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"reference/environment-variables/#development","title":"Development","text":"<pre><code># .env.development\nDATABASE_URL=postgresql://dev:dev@localhost:5432/dev_db\nS3_BUCKET=dev-data-lake\nLOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/environment-variables/#production","title":"Production","text":"<pre><code># .env.production\nDATABASE_URL=postgresql://prod:${DB_PASSWORD}@prod-db:5432/prod_db\nS3_BUCKET=prod-data-lake\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"reference/environment-variables/#loading-environment-files","title":"Loading Environment Files","text":"<pre><code># Load specific env file\nexport $(cat .env.production | xargs)\netlx run pipeline.yml\n</code></pre>"},{"location":"reference/environment-variables/#secrets-management","title":"Secrets Management","text":""},{"location":"reference/environment-variables/#never-commit-secrets","title":"Never Commit Secrets","text":"<pre><code># .gitignore\n.env\n.env.*\n*.pem\n*.key\ncredentials.json\n</code></pre>"},{"location":"reference/environment-variables/#use-secret-references","title":"Use Secret References","text":"<pre><code># .env\nDATABASE_PASSWORD=${DB_PASSWORD_FROM_VAULT}\nSNOWFLAKE_PASSWORD=$(vault kv get -field=password secret/snowflake)\n</code></pre>"},{"location":"reference/environment-variables/#secret-managers","title":"Secret Managers","text":"<pre><code># AWS Secrets Manager\nexport DB_PASSWORD=$(aws secretsmanager get-secret-value \\\n  --secret-id prod/db-password \\\n  --query SecretString --output text)\n\n# HashiCorp Vault\nexport DB_PASSWORD=$(vault kv get -field=password secret/database)\n\n# Google Secret Manager\nexport DB_PASSWORD=$(gcloud secrets versions access latest --secret=db-password)\n</code></pre>"},{"location":"reference/environment-variables/#debugging","title":"Debugging","text":""},{"location":"reference/environment-variables/#view-current-environment","title":"View Current Environment","text":"<pre><code># Show all ETLX-related variables\nenv | grep -E '^(ETLX_|POSTGRES_|SNOWFLAKE_|AWS_|GOOGLE_)'\n\n# Test variable substitution\netlx validate pipeline.yml --verbose\n</code></pre>"},{"location":"reference/environment-variables/#common-issues","title":"Common Issues","text":"<p>Variable not expanding:</p> <pre><code># Wrong: Single quotes prevent expansion\nexport PATH='${HOME}/data'  # Literal ${HOME}\n\n# Right: Double quotes or no quotes\nexport PATH=\"${HOME}/data\"  # Expands to /home/user/data\n</code></pre> <p>Missing variable:</p> <pre><code># Use defaults to handle missing variables\npath: ${INPUT_PATH:-data/default.parquet}\n</code></pre>"},{"location":"reference/environment-variables/#related","title":"Related","text":"<ul> <li>Pipeline YAML - YAML syntax</li> <li>Variables - Variable substitution</li> <li>Production Best Practices - Secrets management</li> </ul>"},{"location":"reference/expressions/","title":"Expression Language Reference","text":"<p>ETLX uses SQL expressions for filters, derived columns, and quality checks. This reference covers the supported expression syntax.</p>"},{"location":"reference/expressions/#basic-syntax","title":"Basic Syntax","text":"<p>Expressions are written as SQL-compatible strings:</p> <pre><code>transforms:\n  - op: filter\n    predicate: amount &gt; 100\n\n  - op: derive_column\n    name: total\n    expr: quantity * price\n</code></pre>"},{"location":"reference/expressions/#operators","title":"Operators","text":""},{"location":"reference/expressions/#comparison-operators","title":"Comparison Operators","text":"Operator Description Example <code>=</code> Equal <code>status = 'active'</code> <code>!=</code> or <code>&lt;&gt;</code> Not equal <code>status != 'deleted'</code> <code>&gt;</code> Greater than <code>amount &gt; 100</code> <code>&gt;=</code> Greater than or equal <code>amount &gt;= 100</code> <code>&lt;</code> Less than <code>amount &lt; 1000</code> <code>&lt;=</code> Less than or equal <code>amount &lt;= 1000</code>"},{"location":"reference/expressions/#logical-operators","title":"Logical Operators","text":"Operator Description Example <code>AND</code> Logical AND <code>status = 'active' AND amount &gt; 0</code> <code>OR</code> Logical OR <code>status = 'pending' OR status = 'active'</code> <code>NOT</code> Logical NOT <code>NOT status = 'deleted'</code>"},{"location":"reference/expressions/#arithmetic-operators","title":"Arithmetic Operators","text":"Operator Description Example <code>+</code> Addition <code>price + tax</code> <code>-</code> Subtraction <code>gross - discount</code> <code>*</code> Multiplication <code>quantity * price</code> <code>/</code> Division <code>total / count</code> <code>%</code> Modulo <code>id % 10</code>"},{"location":"reference/expressions/#string-operators","title":"String Operators","text":"Operator Description Example <code>\\|\\|</code> Concatenation <code>first_name \\|\\| ' ' \\|\\| last_name</code> <code>LIKE</code> Pattern match <code>email LIKE '%@gmail.com'</code> <code>ILIKE</code> Case-insensitive pattern <code>name ILIKE '%smith%'</code>"},{"location":"reference/expressions/#null-handling","title":"NULL Handling","text":""},{"location":"reference/expressions/#check-for-null","title":"Check for NULL","text":"<pre><code>- op: filter\n  predicate: email IS NOT NULL\n\n- op: filter\n  predicate: phone IS NULL\n</code></pre>"},{"location":"reference/expressions/#coalesce","title":"COALESCE","text":"<p>Return first non-NULL value:</p> <pre><code>- op: derive_column\n  name: display_name\n  expr: coalesce(nickname, first_name, 'Unknown')\n</code></pre>"},{"location":"reference/expressions/#nullif","title":"NULLIF","text":"<p>Return NULL if values equal:</p> <pre><code>- op: derive_column\n  name: safe_divisor\n  expr: amount / nullif(count, 0)\n</code></pre>"},{"location":"reference/expressions/#case-expressions","title":"CASE Expressions","text":""},{"location":"reference/expressions/#simple-case","title":"Simple CASE","text":"<pre><code>- op: derive_column\n  name: status_label\n  expr: |\n    case status\n      when 'A' then 'Active'\n      when 'P' then 'Pending'\n      when 'D' then 'Deleted'\n      else 'Unknown'\n    end\n</code></pre>"},{"location":"reference/expressions/#searched-case","title":"Searched CASE","text":"<pre><code>- op: derive_column\n  name: size_category\n  expr: |\n    case\n      when amount &lt; 100 then 'Small'\n      when amount &lt; 1000 then 'Medium'\n      when amount &lt; 10000 then 'Large'\n      else 'Enterprise'\n    end\n</code></pre>"},{"location":"reference/expressions/#string-functions","title":"String Functions","text":"Function Description Example <code>upper(s)</code> Uppercase <code>upper(name)</code> <code>lower(s)</code> Lowercase <code>lower(email)</code> <code>trim(s)</code> Remove whitespace <code>trim(name)</code> <code>ltrim(s)</code> Left trim <code>ltrim(name)</code> <code>rtrim(s)</code> Right trim <code>rtrim(name)</code> <code>length(s)</code> String length <code>length(description)</code> <code>substring(s, start, len)</code> Extract substring <code>substring(phone, 1, 3)</code> <code>replace(s, old, new)</code> Replace text <code>replace(phone, '-', '')</code> <code>concat(s1, s2, ...)</code> Concatenate <code>concat(first, ' ', last)</code> <code>split_part(s, delim, n)</code> Split and get part <code>split_part(email, '@', 2)</code>"},{"location":"reference/expressions/#examples","title":"Examples","text":"<pre><code>transforms:\n  - op: derive_column\n    name: email_domain\n    expr: split_part(email, '@', 2)\n\n  - op: derive_column\n    name: full_name\n    expr: concat(upper(substring(first_name, 1, 1)), lower(substring(first_name, 2, 100)), ' ', last_name)\n\n  - op: derive_column\n    name: clean_phone\n    expr: replace(replace(phone, '-', ''), ' ', '')\n</code></pre>"},{"location":"reference/expressions/#numeric-functions","title":"Numeric Functions","text":"Function Description Example <code>abs(n)</code> Absolute value <code>abs(balance)</code> <code>round(n, d)</code> Round to decimals <code>round(amount, 2)</code> <code>floor(n)</code> Round down <code>floor(amount)</code> <code>ceil(n)</code> Round up <code>ceil(amount)</code> <code>sqrt(n)</code> Square root <code>sqrt(value)</code> <code>power(n, p)</code> Power <code>power(base, 2)</code> <code>mod(n, d)</code> Modulo <code>mod(id, 10)</code> <code>greatest(a, b, ...)</code> Maximum of values <code>greatest(a, b, c)</code> <code>least(a, b, ...)</code> Minimum of values <code>least(a, b, c)</code>"},{"location":"reference/expressions/#examples_1","title":"Examples","text":"<pre><code>transforms:\n  - op: derive_column\n    name: rounded_amount\n    expr: round(amount, 2)\n\n  - op: derive_column\n    name: percentage\n    expr: round(part / total * 100, 1)\n\n  - op: derive_column\n    name: capped_value\n    expr: least(amount, 1000)\n</code></pre>"},{"location":"reference/expressions/#date-and-time-functions","title":"Date and Time Functions","text":"Function Description Example <code>current_date</code> Today's date <code>current_date</code> <code>current_timestamp</code> Current timestamp <code>current_timestamp</code> <code>date(ts)</code> Extract date <code>date(created_at)</code> <code>year(d)</code> Extract year <code>year(order_date)</code> <code>month(d)</code> Extract month <code>month(order_date)</code> <code>day(d)</code> Extract day <code>day(order_date)</code> <code>hour(ts)</code> Extract hour <code>hour(created_at)</code> <code>minute(ts)</code> Extract minute <code>minute(created_at)</code> <code>extract(part from d)</code> Extract date part <code>extract(dow from date)</code> <code>date_trunc(part, d)</code> Truncate date <code>date_trunc('month', date)</code> <code>date_diff(part, d1, d2)</code> Date difference <code>date_diff('day', start, end)</code>"},{"location":"reference/expressions/#date-parts","title":"Date Parts","text":"<ul> <li><code>year</code>, <code>quarter</code>, <code>month</code>, <code>week</code>, <code>day</code></li> <li><code>hour</code>, <code>minute</code>, <code>second</code></li> <li><code>dow</code> (day of week), <code>doy</code> (day of year)</li> </ul>"},{"location":"reference/expressions/#examples_2","title":"Examples","text":"<pre><code>transforms:\n  - op: derive_column\n    name: order_month\n    expr: date_trunc('month', order_date)\n\n  - op: derive_column\n    name: days_since_signup\n    expr: date_diff('day', signup_date, current_date)\n\n  - op: derive_column\n    name: is_weekend\n    expr: extract(dow from date) in (0, 6)\n\n  - op: filter\n    predicate: order_date &gt;= current_date - interval '30 days'\n</code></pre>"},{"location":"reference/expressions/#aggregate-functions","title":"Aggregate Functions","text":"<p>Used in <code>aggregate</code> transforms:</p> Function Description Example <code>count(*)</code> Count rows <code>count(*)</code> <code>count(col)</code> Count non-NULL <code>count(email)</code> <code>count(distinct col)</code> Count unique <code>count(distinct customer_id)</code> <code>sum(col)</code> Sum values <code>sum(amount)</code> <code>avg(col)</code> Average <code>avg(amount)</code> <code>min(col)</code> Minimum <code>min(date)</code> <code>max(col)</code> Maximum <code>max(date)</code> <code>stddev(col)</code> Standard deviation <code>stddev(amount)</code> <code>variance(col)</code> Variance <code>variance(amount)</code>"},{"location":"reference/expressions/#examples_3","title":"Examples","text":"<pre><code>- op: aggregate\n  group_by: [category]\n  aggregations:\n    total_revenue: sum(amount)\n    avg_order: avg(amount)\n    order_count: count(*)\n    unique_customers: count(distinct customer_id)\n    first_order: min(order_date)\n    last_order: max(order_date)\n</code></pre>"},{"location":"reference/expressions/#conditional-aggregation","title":"Conditional Aggregation","text":"<pre><code>- op: aggregate\n  group_by: [region]\n  aggregations:\n    total_orders: count(*)\n    completed_orders: sum(case when status = 'completed' then 1 else 0 end)\n    completion_rate: avg(case when status = 'completed' then 1.0 else 0.0 end)\n    high_value_revenue: sum(case when amount &gt; 1000 then amount else 0 end)\n</code></pre>"},{"location":"reference/expressions/#in-and-between","title":"IN and BETWEEN","text":""},{"location":"reference/expressions/#in-operator","title":"IN Operator","text":"<pre><code>- op: filter\n  predicate: status IN ('active', 'pending', 'review')\n\n- op: filter\n  predicate: category NOT IN ('test', 'internal')\n</code></pre>"},{"location":"reference/expressions/#between-operator","title":"BETWEEN Operator","text":"<pre><code>- op: filter\n  predicate: amount BETWEEN 100 AND 1000\n\n- op: filter\n  predicate: date BETWEEN '2025-01-01' AND '2025-12-31'\n</code></pre>"},{"location":"reference/expressions/#type-casting","title":"Type Casting","text":""},{"location":"reference/expressions/#cast-function","title":"CAST Function","text":"<pre><code>- op: derive_column\n  name: amount_str\n  expr: cast(amount as varchar)\n\n- op: derive_column\n  name: amount_int\n  expr: cast(amount as integer)\n</code></pre>"},{"location":"reference/expressions/#shorthand-postgresql-style","title":"Shorthand (PostgreSQL-style)","text":"<pre><code>- op: derive_column\n  name: amount_int\n  expr: amount::integer\n</code></pre>"},{"location":"reference/expressions/#backend-specific-functions","title":"Backend-Specific Functions","text":"<p>Some functions are backend-specific. Check backend documentation:</p>"},{"location":"reference/expressions/#duckdb","title":"DuckDB","text":"<pre><code>- op: derive_column\n  name: json_value\n  expr: json_extract(data, '$.name')\n</code></pre>"},{"location":"reference/expressions/#bigquery","title":"BigQuery","text":"<pre><code>- op: derive_column\n  name: json_value\n  expr: JSON_EXTRACT_SCALAR(data, '$.name')\n</code></pre>"},{"location":"reference/expressions/#multi-line-expressions","title":"Multi-line Expressions","text":"<p>For complex expressions, use YAML multi-line syntax:</p> <pre><code>- op: derive_column\n  name: customer_tier\n  expr: |\n    case\n      when lifetime_value &gt;= 10000 then 'Platinum'\n      when lifetime_value &gt;= 5000 then 'Gold'\n      when lifetime_value &gt;= 1000 then 'Silver'\n      else 'Bronze'\n    end\n\n- op: filter\n  predicate: |\n    status = 'active'\n    AND created_at &gt;= current_date - interval '30 days'\n    AND (amount &gt; 100 OR is_premium = true)\n</code></pre>"},{"location":"reference/expressions/#related","title":"Related","text":"<ul> <li>Filter Transform</li> <li>Derive Column Transform</li> <li>Aggregate Transform</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions when working with ETLX.</p>"},{"location":"reference/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"reference/troubleshooting/#modulenotfounderror-no-module-named-etlx","title":"ModuleNotFoundError: No module named 'etlx'","text":"<p>Cause: ETLX not installed or wrong Python environment.</p> <p>Solution:</p> <pre><code># Install ETLX\npip install etlx\n\n# Or with specific backend\npip install etlx[duckdb]\n\n# Verify installation\npython -c \"import etlx; print(etlx.__version__)\"\n</code></pre>"},{"location":"reference/troubleshooting/#backend-not-installed","title":"Backend Not Installed","text":"<pre><code>ModuleNotFoundError: No module named 'duckdb'\n</code></pre> <p>Solution: Install the backend extra:</p> <pre><code>pip install etlx[duckdb]\npip install etlx[polars]\npip install etlx[spark]\npip install etlx[snowflake]\n</code></pre>"},{"location":"reference/troubleshooting/#python-version-error","title":"Python Version Error","text":"<pre><code>ERROR: Package 'etlx' requires a different Python: 3.8.0 not in '&gt;=3.10'\n</code></pre> <p>Solution: Upgrade Python to 3.10 or later:</p> <pre><code># Check version\npython --version\n\n# Use pyenv to install newer version\npyenv install 3.12.0\npyenv local 3.12.0\n</code></pre>"},{"location":"reference/troubleshooting/#configuration-errors","title":"Configuration Errors","text":""},{"location":"reference/troubleshooting/#invalid-yaml-syntax","title":"Invalid YAML Syntax","text":"<pre><code>yaml.scanner.ScannerError: mapping values are not allowed here\n</code></pre> <p>Cause: YAML indentation or syntax error.</p> <p>Solution: Check YAML syntax:</p> <pre><code># Wrong: Inconsistent indentation\ntransforms:\n- op: filter  # Missing space after -\n  predicate: amount &gt; 0\n\n# Right: Consistent indentation\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n</code></pre> <p>Use a YAML validator or <code>etlx validate</code>:</p> <pre><code>etlx validate pipeline.yml\n</code></pre>"},{"location":"reference/troubleshooting/#missing-required-field","title":"Missing Required Field","text":"<pre><code>Configuration is invalid\nErrors:\n  - sink: Field required\n</code></pre> <p>Solution: Add the required field:</p> <pre><code>name: my_pipeline\nsource:\n  type: file\n  path: data.csv\n  format: csv\nsink:  # Add missing sink\n  type: file\n  path: output.parquet\n  format: parquet\n</code></pre>"},{"location":"reference/troubleshooting/#invalid-transform-operation","title":"Invalid Transform Operation","text":"<pre><code>transforms -&gt; 0 -&gt; op: Input should be 'select', 'filter', 'rename', ...\n</code></pre> <p>Cause: Typo in transform operation name.</p> <p>Solution: Use correct operation name:</p> <pre><code># Wrong\ntransforms:\n  - op: filtter  # Typo!\n\n# Right\ntransforms:\n  - op: filter\n</code></pre> <p>Valid operations: <code>select</code>, <code>rename</code>, <code>filter</code>, <code>derive_column</code>, <code>cast</code>, <code>fill_null</code>, <code>dedup</code>, <code>sort</code>, <code>join</code>, <code>aggregate</code>, <code>union</code>, <code>limit</code></p>"},{"location":"reference/troubleshooting/#variable-not-found","title":"Variable Not Found","text":"<pre><code>KeyError: 'DATE'\n</code></pre> <p>Cause: Variable referenced but not provided.</p> <p>Solution: Provide the variable:</p> <pre><code>etlx run pipeline.yml --var DATE=2025-01-15\n</code></pre> <p>Or use defaults:</p> <pre><code>path: data/sales_${DATE:-2025-01-01}.csv\n</code></pre>"},{"location":"reference/troubleshooting/#runtime-errors","title":"Runtime Errors","text":""},{"location":"reference/troubleshooting/#file-not-found","title":"File Not Found","text":"<pre><code>FileNotFoundError: data/input.csv not found\n</code></pre> <p>Solution: Verify path:</p> <pre><code># Check file exists\nls -la data/input.csv\n\n# Check current directory\npwd\n\n# Use absolute path\netlx run pipeline.yml --var INPUT_PATH=/absolute/path/to/data.csv\n</code></pre>"},{"location":"reference/troubleshooting/#permission-denied","title":"Permission Denied","text":"<pre><code>PermissionError: [Errno 13] Permission denied: 'output/results.parquet'\n</code></pre> <p>Solution: Check permissions:</p> <pre><code># Check directory permissions\nls -la output/\n\n# Create directory with proper permissions\nmkdir -p output\nchmod 755 output\n</code></pre>"},{"location":"reference/troubleshooting/#out-of-memory","title":"Out of Memory","text":"<pre><code>MemoryError: Unable to allocate array\n</code></pre> <p>Solutions:</p> <ol> <li>Use a more memory-efficient backend:</li> </ol> <pre><code>etlx run pipeline.yml --engine polars  # Streaming support\n</code></pre> <ol> <li>Filter data early:</li> </ol> <pre><code>transforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'  # Reduce data first\n</code></pre> <ol> <li>Select only needed columns:</li> </ol> <pre><code>transforms:\n  - op: select\n    columns: [id, amount, date]\n</code></pre>"},{"location":"reference/troubleshooting/#database-connection-failed","title":"Database Connection Failed","text":"<pre><code>psycopg2.OperationalError: could not connect to server: Connection refused\n</code></pre> <p>Solutions:</p> <ol> <li>Verify database is running:</li> </ol> <pre><code>pg_isready -h localhost -p 5432\n</code></pre> <ol> <li>Check credentials:</li> </ol> <pre><code>psql -h localhost -U user -d database\n</code></pre> <ol> <li>Verify environment variables:</li> </ol> <pre><code>echo $POSTGRES_HOST\necho $POSTGRES_PORT\n</code></pre> <ol> <li>Check firewall/network:</li> </ol> <pre><code>telnet db.example.com 5432\n</code></pre>"},{"location":"reference/troubleshooting/#quality-check-failures","title":"Quality Check Failures","text":""},{"location":"reference/troubleshooting/#not-null-check-failed","title":"Not Null Check Failed","text":"<pre><code>Quality Checks: FAILED\n  \u2717 not_null: email (42 NULL values found)\n</code></pre> <p>Solutions:</p> <ol> <li>Fix data quality at source</li> <li>Fill NULL values:</li> </ol> <pre><code>transforms:\n  - op: fill_null\n    columns:\n      email: \"unknown@example.com\"\n</code></pre> <ol> <li>Filter out NULLs:</li> </ol> <pre><code>transforms:\n  - op: filter\n    predicate: email IS NOT NULL\n</code></pre>"},{"location":"reference/troubleshooting/#unique-check-failed","title":"Unique Check Failed","text":"<pre><code>\u2717 unique: id (152 duplicates found)\n</code></pre> <p>Solutions:</p> <ol> <li>Deduplicate:</li> </ol> <pre><code>transforms:\n  - op: dedup\n    columns: [id]\n    keep: first\n</code></pre> <ol> <li>Investigate source data for duplicates</li> </ol>"},{"location":"reference/troubleshooting/#row-count-check-failed","title":"Row Count Check Failed","text":"<pre><code>\u2717 row_count: min=1 (0 rows found)\n</code></pre> <p>Cause: Empty result after transforms.</p> <p>Solutions:</p> <ol> <li>Check filter conditions aren't too restrictive</li> <li>Verify source data isn't empty</li> <li>Check join conditions</li> </ol>"},{"location":"reference/troubleshooting/#backend-specific-issues","title":"Backend-Specific Issues","text":""},{"location":"reference/troubleshooting/#duckdb","title":"DuckDB","text":"<p>Large CSV parsing slow:</p> <pre><code># Convert to Parquet first\nduckdb -c \"COPY (SELECT * FROM 'large.csv') TO 'large.parquet'\"\n</code></pre>"},{"location":"reference/troubleshooting/#spark","title":"Spark","text":"<p>Java not found:</p> <pre><code>JAVA_HOME is not set\n</code></pre> <p>Solution:</p> <pre><code># macOS\nbrew install openjdk@17\nexport JAVA_HOME=/opt/homebrew/opt/openjdk@17\n\n# Ubuntu\nsudo apt install openjdk-17-jdk\nexport JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n</code></pre>"},{"location":"reference/troubleshooting/#snowflake","title":"Snowflake","text":"<p>Account not found:</p> <pre><code>Account 'xyz' not found\n</code></pre> <p>Solution: Use full account identifier:</p> <pre><code>export SNOWFLAKE_ACCOUNT=xy12345.us-east-1\n# Not just: SNOWFLAKE_ACCOUNT=xy12345\n</code></pre>"},{"location":"reference/troubleshooting/#bigquery","title":"BigQuery","text":"<p>Quota exceeded:</p> <pre><code>Quota exceeded: Your project exceeded quota for concurrent queries\n</code></pre> <p>Solution: Wait and retry, or request quota increase in GCP Console.</p>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"reference/troubleshooting/#pipeline-running-slowly","title":"Pipeline Running Slowly","text":"<p>Diagnosis:</p> <pre><code>etlx run pipeline.yml --verbose\n</code></pre> <p>Look for slow steps.</p> <p>Common causes and solutions:</p> <ol> <li> <p>Reading CSV: Use Parquet instead</p> </li> <li> <p>Late filtering: Move filters earlier</p> </li> <li> <p>Large joins: Filter before joining</p> </li> <li> <p>Wrong backend: Try DuckDB or Polars for local files</p> </li> </ol>"},{"location":"reference/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Solutions:</p> <ol> <li>Use Polars (streaming support):</li> </ol> <pre><code>etlx run pipeline.yml --engine polars\n</code></pre> <ol> <li> <p>Select fewer columns</p> </li> <li> <p>Process in date partitions:</p> </li> </ol> <pre><code>for date in 2025-01-{01..31}; do\n  etlx run pipeline.yml --var DATE=$date\ndone\n</code></pre>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"reference/troubleshooting/#check-version","title":"Check Version","text":"<pre><code>etlx --version\netlx info --backends --check\n</code></pre>"},{"location":"reference/troubleshooting/#verbose-output","title":"Verbose Output","text":"<pre><code>etlx run pipeline.yml --verbose\n</code></pre>"},{"location":"reference/troubleshooting/#validate-configuration","title":"Validate Configuration","text":"<pre><code>etlx validate pipeline.yml --verbose\n</code></pre>"},{"location":"reference/troubleshooting/#report-issues","title":"Report Issues","text":"<p>If you've found a bug:</p> <ol> <li>Check existing issues: https://github.com/your-org/etlx/issues</li> <li>Create minimal reproduction</li> <li>Include:</li> <li>ETLX version</li> <li>Python version</li> <li>Operating system</li> <li>Complete error message</li> <li>Minimal pipeline YAML</li> </ol>"},{"location":"reference/troubleshooting/#related","title":"Related","text":"<ul> <li>Error Handling - Error handling strategies</li> <li>Performance - Optimization tips</li> <li>Backend Selection - Choose the right backend</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This guide covers all aspects of building pipelines with ETLX.</p>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p> Configuration</p> <p>Learn YAML configuration, variable substitution, and IDE integration.</p> <p> Configuration</p> </li> <li> <p> Sources &amp; Sinks</p> <p>Read from and write to files, databases, and cloud storage.</p> <p> Sources &amp; Sinks</p> </li> <li> <p> Transforms</p> <p>All 12 data transformation operations.</p> <p> Transforms</p> </li> <li> <p> Quality Checks</p> <p>Validate data quality with built-in checks.</p> <p> Quality Checks</p> </li> <li> <p> Backends</p> <p>Choose the right compute engine for your workload.</p> <p> Backends</p> </li> </ul>"},{"location":"user-guide/#how-etlx-works","title":"How ETLX Works","text":"<p>ETLX pipelines follow a simple flow:</p> <pre><code>graph LR\n    A[Source] --&gt; B[Transforms]\n    B --&gt; C[Quality Checks]\n    C --&gt; D[Sink]</code></pre> <ol> <li>Source - Read data from files, databases, or cloud storage</li> <li>Transforms - Apply transformations in sequence</li> <li>Quality Checks - Validate the transformed data</li> <li>Sink - Write to the destination</li> </ol>"},{"location":"user-guide/#configuration-methods","title":"Configuration Methods","text":""},{"location":"user-guide/#yaml-configuration","title":"YAML Configuration","text":"<p>Define pipelines declaratively:</p> <pre><code>name: my_pipeline\nengine: duckdb\n\nsource:\n  type: file\n  path: input.parquet\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n\nsink:\n  type: file\n  path: output.parquet\n</code></pre>"},{"location":"user-guide/#python-api","title":"Python API","text":"<p>Build pipelines programmatically:</p> <pre><code>from etlx import Pipeline\nfrom etlx.config.models import FileSource, FileSink\nfrom etlx.config.transforms import FilterTransform\n\npipeline = (\n    Pipeline(\"my_pipeline\", engine=\"duckdb\")\n    .source(FileSource(path=\"input.parquet\"))\n    .transform(FilterTransform(predicate=\"amount &gt; 0\"))\n    .sink(FileSink(path=\"output.parquet\"))\n)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"user-guide/#quick-reference","title":"Quick Reference","text":""},{"location":"user-guide/#transform-operations","title":"Transform Operations","text":"Transform Purpose <code>select</code> Choose columns <code>rename</code> Rename columns <code>filter</code> Filter rows <code>derive_column</code> Add computed columns <code>cast</code> Convert types <code>fill_null</code> Replace nulls <code>dedup</code> Remove duplicates <code>sort</code> Order rows <code>join</code> Join datasets <code>aggregate</code> Group and aggregate <code>union</code> Combine datasets <code>limit</code> Limit rows"},{"location":"user-guide/#quality-checks","title":"Quality Checks","text":"Check Purpose <code>not_null</code> No null values <code>unique</code> Uniqueness constraint <code>row_count</code> Row count bounds <code>accepted_values</code> Value whitelist <code>expression</code> Custom validation"},{"location":"user-guide/#supported-backends","title":"Supported Backends","text":"Backend Type Default DuckDB Local Yes Polars Local Yes Spark Distributed No Snowflake Cloud DW No BigQuery Cloud DW No"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<p>Start with Configuration to understand how pipelines are structured, then explore Transforms to learn the available operations.</p>"},{"location":"user-guide/backends/","title":"Backends","text":"<p>ETLX supports multiple compute backends through Ibis. Choose the right backend for your workload.</p>"},{"location":"user-guide/backends/#available-backends","title":"Available Backends","text":"Backend Type Install Best For DuckDB Local Default Analytics, small-medium data Polars Local Default Fast DataFrames DataFusion Local <code>etlx[datafusion]</code> Arrow-native queries Spark Distributed <code>etlx[spark]</code> Large-scale processing pandas Local <code>etlx[pandas]</code> Legacy compatibility Snowflake Cloud DW <code>etlx[snowflake]</code> Enterprise analytics BigQuery Cloud DW <code>etlx[bigquery]</code> Google Cloud PostgreSQL Database <code>etlx[postgres]</code> Operational data MySQL Database <code>etlx[mysql]</code> Web applications ClickHouse Database <code>etlx[clickhouse]</code> Real-time analytics"},{"location":"user-guide/backends/#selecting-a-backend","title":"Selecting a Backend","text":"<p>Specify the backend in your pipeline:</p> <pre><code>engine: duckdb  # or polars, spark, snowflake, etc.\n</code></pre> <p>Or at runtime:</p> <pre><code>etlx run pipeline.yml --engine polars\n</code></pre>"},{"location":"user-guide/backends/#backend-comparison","title":"Backend Comparison","text":""},{"location":"user-guide/backends/#local-backends","title":"Local Backends","text":"<p>For data that fits on a single machine:</p> Backend Speed Memory SQL Support DuckDB Fast Efficient Yes Polars Very Fast Efficient Limited DataFusion Fast Arrow-native Yes pandas Slower Higher No <p>Recommendation: Start with DuckDB (default).</p>"},{"location":"user-guide/backends/#distributed-backends","title":"Distributed Backends","text":"<p>For data too large for a single machine:</p> Backend Scale Cost Complexity Spark Massive Moderate Higher Snowflake Large Pay-per-query Lower BigQuery Large Pay-per-query Lower <p>Recommendation: Use Spark for on-premise, Snowflake/BigQuery for cloud.</p>"},{"location":"user-guide/backends/#backend-features","title":"Backend Features","text":"Feature DuckDB Polars Spark Snowflake Local files Yes Yes Yes No Cloud storage Yes Yes Yes Yes SQL support Full Partial Full Full Memory efficiency High High Medium N/A Parallelism Multi-core Multi-core Distributed Distributed"},{"location":"user-guide/backends/#checking-availability","title":"Checking Availability","text":"<p>List installed backends:</p> <pre><code>etlx info --backends --check\n</code></pre> <p>Output:</p> <pre><code>Available Backends\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend    \u2503 Name            \u2503 Status         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 duckdb     \u2502 DuckDB          \u2502 OK             \u2502\n\u2502 polars     \u2502 Polars          \u2502 OK             \u2502\n\u2502 spark      \u2502 Apache Spark    \u2502 Not installed  \u2502\n\u2502 snowflake  \u2502 Snowflake       \u2502 Not installed  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/backends/#installation","title":"Installation","text":"<p>Install additional backends:</p> <pre><code># Individual backends\npip install etlx[spark]\npip install etlx[snowflake]\npip install etlx[bigquery]\n\n# Multiple backends\npip install etlx[spark,snowflake,bigquery]\n\n# All backends\npip install etlx[all]\n</code></pre>"},{"location":"user-guide/backends/#backend-parity","title":"Backend Parity","text":"<p>ETLX aims for consistent behavior across backends. The same pipeline should produce identical results regardless of backend:</p> <pre><code># Works on any backend\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total: sum(amount)\n</code></pre> <p>Backend Differences</p> <p>Some edge cases may differ (null handling, floating-point precision). Test important pipelines on your target backend.</p>"},{"location":"user-guide/backends/#python-api","title":"Python API","text":"<pre><code>from etlx import ETLXEngine\n\n# DuckDB (default)\nengine = ETLXEngine(backend=\"duckdb\")\n\n# Polars\nengine = ETLXEngine(backend=\"polars\")\n\n# Snowflake\nengine = ETLXEngine(\n    backend=\"snowflake\",\n    connection_string=\"snowflake://user:pass@account/db/schema\"\n)\n</code></pre>"},{"location":"user-guide/backends/#choosing-your-backend","title":"Choosing Your Backend","text":""},{"location":"user-guide/backends/#start-with-duckdb","title":"Start with DuckDB","text":"<p>DuckDB is the default because it:</p> <ul> <li>Requires no setup</li> <li>Handles most workloads</li> <li>Has excellent SQL support</li> <li>Is very fast for analytics</li> </ul>"},{"location":"user-guide/backends/#consider-alternatives-when","title":"Consider Alternatives When","text":"Scenario Consider Need maximum speed on DataFrames Polars Data doesn't fit in memory Spark Already using Snowflake/BigQuery Same warehouse Need database connectivity PostgreSQL/MySQL Real-time analytics ClickHouse"},{"location":"user-guide/backends/#next-steps","title":"Next Steps","text":"<ul> <li>DuckDB - Default backend details</li> <li>Spark - Distributed processing</li> <li>Snowflake - Cloud warehouse</li> </ul>"},{"location":"user-guide/backends/bigquery/","title":"BigQuery Backend","text":"<p>Google BigQuery is a serverless, highly scalable data warehouse. ETLX pushes transformations to BigQuery for efficient in-warehouse processing.</p>"},{"location":"user-guide/backends/bigquery/#installation","title":"Installation","text":"<pre><code>pip install etlx[bigquery]\n# or\nuv add etlx[bigquery]\n</code></pre>"},{"location":"user-guide/backends/bigquery/#when-to-use-bigquery","title":"When to Use BigQuery","text":"<p>Ideal for:</p> <ul> <li>Data already in Google Cloud</li> <li>Serverless, pay-per-query model</li> <li>Petabyte-scale analytics</li> <li>Integration with GCP services</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Data is outside GCP (egress costs)</li> <li>Predictable compute costs needed</li> <li>Real-time processing required</li> </ul>"},{"location":"user-guide/backends/bigquery/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/bigquery/#authentication","title":"Authentication","text":""},{"location":"user-guide/backends/bigquery/#service-account-recommended-for-production","title":"Service Account (Recommended for Production)","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\nexport BIGQUERY_PROJECT=my-project-id\n</code></pre>"},{"location":"user-guide/backends/bigquery/#application-default-credentials-development","title":"Application Default Credentials (Development)","text":"<pre><code>gcloud auth application-default login\nexport BIGQUERY_PROJECT=my-project-id\n</code></pre>"},{"location":"user-guide/backends/bigquery/#environment-variables","title":"Environment Variables","text":"<pre><code>export BIGQUERY_PROJECT=my-project-id\nexport BIGQUERY_DATASET=analytics\nexport BIGQUERY_LOCATION=US\n</code></pre>"},{"location":"user-guide/backends/bigquery/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>name: bigquery_etl\nengine: bigquery\n\nsource:\n  type: database\n  connection: bigquery\n  table: raw_data.sales\n\ntransforms:\n  - op: filter\n    predicate: transaction_date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [region, product_type]\n    aggregations:\n      total_revenue: sum(amount)\n      transaction_count: count(*)\n\nsink:\n  type: database\n  connection: bigquery\n  table: analytics.sales_summary\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/bigquery/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/bigquery/#transforms","title":"Transforms","text":"<p>All transforms are pushed to BigQuery SQL:</p> Transform Support BigQuery SQL select Full <code>SELECT</code> rename Full <code>AS alias</code> filter Full <code>WHERE</code> derive_column Full Expressions cast Full <code>CAST()</code> fill_null Full <code>COALESCE()</code> / <code>IFNULL()</code> dedup Full <code>QUALIFY ROW_NUMBER()</code> sort Full <code>ORDER BY</code> join Full <code>JOIN</code> aggregate Full <code>GROUP BY</code> union Full <code>UNION ALL</code> limit Full <code>LIMIT</code>"},{"location":"user-guide/backends/bigquery/#data-types","title":"Data Types","text":"ETLX Type BigQuery Type string STRING int INT64 float FLOAT64 bool BOOL date DATE timestamp TIMESTAMP decimal NUMERIC"},{"location":"user-guide/backends/bigquery/#reading-data","title":"Reading Data","text":""},{"location":"user-guide/backends/bigquery/#from-tables","title":"From Tables","text":"<pre><code>source:\n  type: database\n  connection: bigquery\n  table: project.dataset.table_name\n</code></pre>"},{"location":"user-guide/backends/bigquery/#from-queries","title":"From Queries","text":"<pre><code>source:\n  type: database\n  connection: bigquery\n  query: |\n    SELECT *\n    FROM `project.dataset.table`\n    WHERE _PARTITIONDATE &gt;= '2025-01-01'\n</code></pre>"},{"location":"user-guide/backends/bigquery/#from-external-tables-gcs","title":"From External Tables (GCS)","text":"<pre><code>source:\n  type: database\n  connection: bigquery\n  query: |\n    SELECT * FROM EXTERNAL_QUERY(\n      'us.my_connection',\n      'SELECT * FROM external_table'\n    )\n</code></pre>"},{"location":"user-guide/backends/bigquery/#writing-data","title":"Writing Data","text":""},{"location":"user-guide/backends/bigquery/#replace-mode","title":"Replace Mode","text":"<pre><code>sink:\n  type: database\n  connection: bigquery\n  table: project.dataset.output\n  mode: replace  # WRITE_TRUNCATE\n</code></pre>"},{"location":"user-guide/backends/bigquery/#append-mode","title":"Append Mode","text":"<pre><code>sink:\n  type: database\n  connection: bigquery\n  table: project.dataset.output\n  mode: append  # WRITE_APPEND\n</code></pre>"},{"location":"user-guide/backends/bigquery/#partitioned-tables","title":"Partitioned Tables","text":"<pre><code>sink:\n  type: database\n  connection: bigquery\n  table: project.dataset.output\n  mode: replace\n  options:\n    partition_field: date\n    partition_type: DAY\n</code></pre>"},{"location":"user-guide/backends/bigquery/#cost-optimization","title":"Cost Optimization","text":""},{"location":"user-guide/backends/bigquery/#1-use-partitioned-tables","title":"1. Use Partitioned Tables","text":"<p>Partition by date to reduce bytes scanned:</p> <pre><code>source:\n  type: database\n  connection: bigquery\n  query: |\n    SELECT * FROM `project.dataset.events`\n    WHERE _PARTITIONDATE BETWEEN '2025-01-01' AND '2025-01-31'\n</code></pre>"},{"location":"user-guide/backends/bigquery/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<p>BigQuery charges by bytes scanned:</p> <pre><code>transforms:\n  - op: select\n    columns: [id, amount, date]  # Don't SELECT *\n</code></pre>"},{"location":"user-guide/backends/bigquery/#3-use-clustering","title":"3. Use Clustering","text":"<p>For frequently filtered columns:</p> <pre><code>CREATE TABLE analytics.sales\nPARTITION BY date\nCLUSTER BY region, product_type\nAS SELECT * FROM raw.sales;\n</code></pre>"},{"location":"user-guide/backends/bigquery/#4-materialize-intermediate-results","title":"4. Materialize Intermediate Results","text":"<p>For complex pipelines, use temp tables:</p> <pre><code>sink:\n  type: database\n  connection: bigquery\n  table: project.dataset.temp_results\n  mode: replace\n  options:\n    expiration_hours: 24\n</code></pre>"},{"location":"user-guide/backends/bigquery/#example-analytics-pipeline","title":"Example: Analytics Pipeline","text":"<pre><code>name: daily_analytics\ndescription: Compute daily KPIs from event data\nengine: bigquery\n\nsource:\n  type: database\n  connection: bigquery\n  query: |\n    SELECT\n      user_id,\n      event_type,\n      event_timestamp,\n      JSON_EXTRACT_SCALAR(properties, '$.value') as value,\n      JSON_EXTRACT_SCALAR(properties, '$.category') as category\n    FROM `analytics.events`\n    WHERE DATE(event_timestamp) = CURRENT_DATE() - 1\n\ntransforms:\n  - op: cast\n    columns:\n      value: float\n\n  - op: filter\n    predicate: event_type IN ('purchase', 'signup', 'pageview')\n\n  - op: aggregate\n    group_by: [event_type, category]\n    aggregations:\n      total_value: sum(value)\n      unique_users: count(distinct user_id)\n      event_count: count(*)\n\nchecks:\n  - check: not_null\n    columns: [event_type, category]\n  - check: row_count\n    min: 1\n\nsink:\n  type: database\n  connection: bigquery\n  table: analytics.daily_kpis\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/bigquery/#streaming-inserts","title":"Streaming Inserts","text":"<p>For real-time data, use streaming:</p> <pre><code>sink:\n  type: database\n  connection: bigquery\n  table: project.dataset.stream_table\n  mode: stream\n</code></pre> <p>Streaming Costs</p> <p>Streaming inserts have additional costs. Use batch loading for large volumes.</p>"},{"location":"user-guide/backends/bigquery/#limitations","title":"Limitations","text":"<ol> <li>Query Costs: Pay per TB scanned</li> <li>DML Quotas: Limited UPDATE/DELETE operations per day</li> <li>Streaming Limits: 100,000 rows/second per table</li> <li>Latency: Query startup time ~2-5 seconds</li> </ol>"},{"location":"user-guide/backends/bigquery/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/bigquery/#authentication-failed","title":"Authentication Failed","text":"<pre><code>google.auth.exceptions.DefaultCredentialsError\n</code></pre> <p>Solutions: 1. Set <code>GOOGLE_APPLICATION_CREDENTIALS</code> 2. Run <code>gcloud auth application-default login</code> 3. Verify service account has BigQuery permissions</p>"},{"location":"user-guide/backends/bigquery/#dataset-not-found","title":"Dataset Not Found","text":"<pre><code>NotFound: 404 Not found: Dataset project:dataset\n</code></pre> <p>Solution: Verify dataset exists and you have access: <pre><code>bq ls project:dataset\n</code></pre></p>"},{"location":"user-guide/backends/bigquery/#query-quota-exceeded","title":"Query Quota Exceeded","text":"<pre><code>Quota exceeded: Your project exceeded quota for concurrent queries\n</code></pre> <p>Solution: Wait and retry, or request quota increase.</p>"},{"location":"user-guide/backends/bigquery/#bytes-billed-too-high","title":"Bytes Billed Too High","text":"<p>Prevention: 1. Always filter by partition column 2. Select only needed columns 3. Use <code>--dry-run</code> to estimate costs: <pre><code>bq query --dry_run \"SELECT * FROM dataset.table\"\n</code></pre></p>"},{"location":"user-guide/backends/bigquery/#permissions-required","title":"Permissions Required","text":"<p>The service account needs these IAM roles:</p> <ul> <li><code>roles/bigquery.dataViewer</code> - Read tables</li> <li><code>roles/bigquery.dataEditor</code> - Write tables</li> <li><code>roles/bigquery.jobUser</code> - Run queries</li> </ul> <pre><code>gcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"serviceAccount:etlx@PROJECT_ID.iam.gserviceaccount.com\" \\\n  --role=\"roles/bigquery.dataEditor\"\n</code></pre>"},{"location":"user-guide/backends/bigquery/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Cloud Storage - GCS integration</li> <li>Database Sources - Database configuration</li> </ul>"},{"location":"user-guide/backends/clickhouse/","title":"ClickHouse Backend","text":"<p>ClickHouse is a column-oriented OLAP database designed for real-time analytics. ETLX supports ClickHouse for high-performance analytical workloads.</p>"},{"location":"user-guide/backends/clickhouse/#installation","title":"Installation","text":"<pre><code>pip install etlx[clickhouse]\n# or\nuv add etlx[clickhouse]\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#when-to-use-clickhouse","title":"When to Use ClickHouse","text":"<p>Ideal for:</p> <ul> <li>Real-time analytics on large datasets</li> <li>Time-series data</li> <li>Log and event analytics</li> <li>High-speed aggregations</li> <li>Append-heavy workloads</li> </ul> <p>Consider alternatives when:</p> <ul> <li>ACID transactions needed (use PostgreSQL)</li> <li>Frequent updates/deletes</li> <li>Small datasets (use DuckDB)</li> </ul>"},{"location":"user-guide/backends/clickhouse/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/clickhouse/#connection-setup","title":"Connection Setup","text":"<pre><code>export CLICKHOUSE_HOST=localhost\nexport CLICKHOUSE_PORT=8123\nexport CLICKHOUSE_USER=default\nexport CLICKHOUSE_PASSWORD=your_password\nexport CLICKHOUSE_DATABASE=analytics\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#using-env","title":"Using <code>.env</code>","text":"<pre><code>CLICKHOUSE_HOST=clickhouse.example.com\nCLICKHOUSE_PORT=8123\nCLICKHOUSE_USER=etlx_user\nCLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}\nCLICKHOUSE_DATABASE=analytics\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>name: clickhouse_analytics\nengine: clickhouse\n\nsource:\n  type: database\n  connection: clickhouse\n  table: events\n\ntransforms:\n  - op: filter\n    predicate: event_date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [event_type, toDate(event_time)]\n    aggregations:\n      event_count: count(*)\n      unique_users: uniqExact(user_id)\n\nsink:\n  type: database\n  connection: clickhouse\n  table: event_summary\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/clickhouse/#transforms","title":"Transforms","text":"Transform Support ClickHouse SQL select Full <code>SELECT</code> rename Full <code>AS alias</code> filter Full <code>WHERE</code> / <code>PREWHERE</code> derive_column Full Expressions cast Full <code>CAST()</code> / <code>toType()</code> fill_null Full <code>COALESCE()</code> / <code>ifNull()</code> dedup Full <code>DISTINCT</code> sort Full <code>ORDER BY</code> join Full <code>JOIN</code> aggregate Full <code>GROUP BY</code> union Full <code>UNION ALL</code> limit Full <code>LIMIT</code>"},{"location":"user-guide/backends/clickhouse/#data-types","title":"Data Types","text":"ETLX Type ClickHouse Type string String int Int64 float Float64 bool UInt8 date Date timestamp DateTime decimal Decimal"},{"location":"user-guide/backends/clickhouse/#reading-data","title":"Reading Data","text":""},{"location":"user-guide/backends/clickhouse/#from-tables","title":"From Tables","text":"<pre><code>source:\n  type: database\n  connection: clickhouse\n  table: database.table_name\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#from-queries","title":"From Queries","text":"<pre><code>source:\n  type: database\n  connection: clickhouse\n  query: |\n    SELECT\n      toDate(event_time) as event_date,\n      event_type,\n      user_id,\n      properties\n    FROM events\n    WHERE event_date &gt;= today() - 7\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#writing-data","title":"Writing Data","text":""},{"location":"user-guide/backends/clickhouse/#append-mode-recommended","title":"Append Mode (Recommended)","text":"<p>ClickHouse is optimized for append-only writes:</p> <pre><code>sink:\n  type: database\n  connection: clickhouse\n  table: event_summary\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#replace-mode","title":"Replace Mode","text":"<p>Use with caution (drops and recreates):</p> <pre><code>sink:\n  type: database\n  connection: clickhouse\n  table: output_table\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/backends/clickhouse/#1-use-appropriate-table-engines","title":"1. Use Appropriate Table Engines","text":"<p>MergeTree for most use cases: <pre><code>CREATE TABLE events (\n    event_date Date,\n    event_time DateTime,\n    user_id UInt64,\n    event_type String\n) ENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, user_id);\n</code></pre></p>"},{"location":"user-guide/backends/clickhouse/#2-leverage-prewhere","title":"2. Leverage PREWHERE","text":"<p>Filter on indexed columns:</p> <pre><code>transforms:\n  - op: filter\n    predicate: event_date &gt;= '2025-01-01'  # Uses partition pruning\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#3-use-materialized-views","title":"3. Use Materialized Views","text":"<p>For common aggregations:</p> <pre><code>CREATE MATERIALIZED VIEW event_counts\nENGINE = SummingMergeTree()\nORDER BY (event_date, event_type)\nAS SELECT\n    toDate(event_time) as event_date,\n    event_type,\n    count() as event_count\nFROM events\nGROUP BY event_date, event_type;\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#4-batch-inserts","title":"4. Batch Inserts","text":"<p>ClickHouse performs best with large batches:</p> <pre><code>sink:\n  type: database\n  connection: clickhouse\n  table: events\n  options:\n    batch_size: 100000\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#example-real-time-analytics","title":"Example: Real-Time Analytics","text":"<pre><code>name: realtime_dashboard\ndescription: Compute metrics for real-time dashboard\nengine: clickhouse\n\nsource:\n  type: database\n  connection: clickhouse\n  query: |\n    SELECT\n      toStartOfMinute(event_time) as minute,\n      event_type,\n      countIf(event_type = 'pageview') as pageviews,\n      countIf(event_type = 'click') as clicks,\n      uniqExact(user_id) as unique_users\n    FROM events\n    WHERE event_time &gt;= now() - INTERVAL 1 HOUR\n    GROUP BY minute, event_type\n    ORDER BY minute DESC\n\ntransforms:\n  - op: derive_column\n    name: click_rate\n    expr: clicks / nullif(pageviews, 0)\n\nchecks:\n  - check: not_null\n    columns: [minute, event_type]\n\nsink:\n  type: database\n  connection: clickhouse\n  table: dashboard_metrics\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#clickhouse-specific-functions","title":"ClickHouse-Specific Functions","text":"<p>ETLX supports ClickHouse-specific functions in expressions:</p> <pre><code>transforms:\n  - op: derive_column\n    name: hour\n    expr: toHour(event_time)\n\n  - op: derive_column\n    name: day_of_week\n    expr: toDayOfWeek(event_date)\n\n  - op: aggregate\n    group_by: [event_type]\n    aggregations:\n      approx_unique: uniq(user_id)\n      exact_unique: uniqExact(user_id)\n      percentile_95: quantile(0.95)(response_time)\n</code></pre>"},{"location":"user-guide/backends/clickhouse/#limitations","title":"Limitations","text":"<ol> <li>No Updates: Designed for append-only (use ReplacingMergeTree for updates)</li> <li>No Transactions: No ACID guarantees</li> <li>Join Performance: Large joins can be slow</li> <li>Memory Usage: Aggregations load data into memory</li> </ol>"},{"location":"user-guide/backends/clickhouse/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/clickhouse/#connection-failed","title":"Connection Failed","text":"<pre><code>clickhouse_driver.errors.NetworkError: Connection refused\n</code></pre> <p>Solutions: 1. Verify ClickHouse is running 2. Check HTTP port (default 8123) or native port (9000) 3. Verify network connectivity</p>"},{"location":"user-guide/backends/clickhouse/#memory-limit-exceeded","title":"Memory Limit Exceeded","text":"<pre><code>Memory limit exceeded\n</code></pre> <p>Solutions: 1. Add <code>LIMIT</code> to queries 2. Use approximate functions (<code>uniq</code> instead of <code>uniqExact</code>) 3. Increase memory limits: <pre><code>SET max_memory_usage = 20000000000;\n</code></pre></p>"},{"location":"user-guide/backends/clickhouse/#too-many-parts","title":"Too Many Parts","text":"<pre><code>Too many parts in table\n</code></pre> <p>Solution: Wait for merges or optimize: <pre><code>OPTIMIZE TABLE events FINAL;\n</code></pre></p>"},{"location":"user-guide/backends/clickhouse/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Database Sources - Database configuration</li> <li>Performance Best Practices</li> </ul>"},{"location":"user-guide/backends/datafusion/","title":"DataFusion Backend","text":"<p>Apache DataFusion is a fast, extensible query engine written in Rust. ETLX uses DataFusion via Ibis for efficient in-memory analytics.</p>"},{"location":"user-guide/backends/datafusion/#installation","title":"Installation","text":"<pre><code>pip install etlx[datafusion]\n# or\nuv add etlx[datafusion]\n</code></pre>"},{"location":"user-guide/backends/datafusion/#when-to-use-datafusion","title":"When to Use DataFusion","text":"<p>Ideal for:</p> <ul> <li>Rust-powered performance</li> <li>SQL-based analytics on files</li> <li>Lightweight embedded analytics</li> <li>When you need both SQL and DataFrame APIs</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Need database connectivity (use DuckDB)</li> <li>Distributed processing required (use Spark)</li> <li>Maximum ecosystem compatibility needed</li> </ul>"},{"location":"user-guide/backends/datafusion/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/datafusion/#basic-usage","title":"Basic Usage","text":"<pre><code>name: datafusion_pipeline\nengine: datafusion\n\nsource:\n  type: file\n  path: data/events.parquet\n  format: parquet\n\ntransforms:\n  - op: filter\n    predicate: event_date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [event_type]\n    aggregations:\n      count: count(*)\n\nsink:\n  type: file\n  path: output/summary.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/datafusion/#cli-override","title":"CLI Override","text":"<pre><code>etlx run pipeline.yml --engine datafusion\n</code></pre>"},{"location":"user-guide/backends/datafusion/#performance-characteristics","title":"Performance Characteristics","text":"Metric Performance Startup time ~50ms Memory efficiency Excellent Parallelism Multi-threaded SQL support Full"},{"location":"user-guide/backends/datafusion/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/datafusion/#transforms","title":"Transforms","text":"Transform Support Notes select Full rename Full filter Full Predicate pushdown derive_column Full cast Full fill_null Full dedup Full sort Full join Full aggregate Full union Full limit Full"},{"location":"user-guide/backends/datafusion/#data-types","title":"Data Types","text":"ETLX Type DataFusion Type string Utf8 int Int64 float Float64 bool Boolean date Date32 timestamp Timestamp decimal Decimal128"},{"location":"user-guide/backends/datafusion/#file-format-support","title":"File Format Support","text":"<p>DataFusion excels at processing various file formats:</p>"},{"location":"user-guide/backends/datafusion/#parquet","title":"Parquet","text":"<pre><code>source:\n  type: file\n  path: data/*.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/datafusion/#csv","title":"CSV","text":"<pre><code>source:\n  type: file\n  path: data/input.csv\n  format: csv\n  options:\n    has_header: true\n    delimiter: \",\"\n</code></pre>"},{"location":"user-guide/backends/datafusion/#json","title":"JSON","text":"<pre><code>source:\n  type: file\n  path: data/records.json\n  format: json\n</code></pre>"},{"location":"user-guide/backends/datafusion/#optimization-tips","title":"Optimization Tips","text":""},{"location":"user-guide/backends/datafusion/#1-use-parquet-format","title":"1. Use Parquet Format","text":"<p>DataFusion is highly optimized for Parquet:</p> <pre><code>source:\n  type: file\n  path: data/input.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/datafusion/#2-enable-predicate-pushdown","title":"2. Enable Predicate Pushdown","text":"<p>Filters are automatically pushed to file readers:</p> <pre><code>transforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'  # Pushed to Parquet reader\n</code></pre>"},{"location":"user-guide/backends/datafusion/#3-partition-your-data","title":"3. Partition Your Data","text":"<p>Organize data by common filter columns:</p> <pre><code>data/\n  year=2024/\n    month=01/\n      data.parquet\n    month=02/\n      data.parquet\n  year=2025/\n    month=01/\n      data.parquet\n</code></pre> <pre><code>source:\n  type: file\n  path: data/year=2025/**/*.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/datafusion/#example-analytics-pipeline","title":"Example: Analytics Pipeline","text":"<pre><code>name: web_analytics\ndescription: Process web analytics events\nengine: datafusion\n\nsource:\n  type: file\n  path: data/events/*.parquet\n  format: parquet\n\ntransforms:\n  # Filter to relevant date range\n  - op: filter\n    predicate: event_date &gt;= '2025-01-01'\n\n  # Select needed columns\n  - op: select\n    columns: [user_id, event_type, page_url, event_time]\n\n  # Extract page path\n  - op: derive_column\n    name: page_path\n    expr: split_part(page_url, '?', 1)\n\n  # Aggregate by page and event\n  - op: aggregate\n    group_by: [page_path, event_type]\n    aggregations:\n      pageviews: count(*)\n      unique_users: count(distinct user_id)\n\n  # Sort by pageviews\n  - op: sort\n    by:\n      - column: pageviews\n        order: desc\n\nsink:\n  type: file\n  path: output/page_analytics.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/datafusion/#sql-expressions","title":"SQL Expressions","text":"<p>DataFusion supports standard SQL expressions:</p> <pre><code>transforms:\n  - op: derive_column\n    name: full_name\n    expr: concat(first_name, ' ', last_name)\n\n  - op: derive_column\n    name: year\n    expr: extract(year from event_date)\n\n  - op: filter\n    predicate: |\n      amount &gt; 100\n      AND status IN ('completed', 'shipped')\n      AND created_at &gt;= '2025-01-01'\n</code></pre>"},{"location":"user-guide/backends/datafusion/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature DataFusion DuckDB Polars Language Rust C++ Rust SQL support Full Full Via Ibis DB connectivity No Yes No Memory efficiency Excellent Excellent Excellent Arrow native Yes Yes Yes"},{"location":"user-guide/backends/datafusion/#limitations","title":"Limitations","text":"<ol> <li>No Database Connectivity: File-based only</li> <li>Ecosystem: Smaller than DuckDB</li> <li>Extensions: Fewer built-in extensions</li> </ol>"},{"location":"user-guide/backends/datafusion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/datafusion/#import-error","title":"Import Error","text":"<pre><code>ModuleNotFoundError: No module named 'datafusion'\n</code></pre> <p>Solution: <pre><code>pip install etlx[datafusion]\n</code></pre></p>"},{"location":"user-guide/backends/datafusion/#file-not-found","title":"File Not Found","text":"<pre><code>Error: No files found matching pattern\n</code></pre> <p>Solution: Verify file path and glob pattern: <pre><code>ls data/*.parquet\n</code></pre></p>"},{"location":"user-guide/backends/datafusion/#memory-issues","title":"Memory Issues","text":"<p>For large datasets: 1. Use Parquet format 2. Filter early in pipeline 3. Select only needed columns</p>"},{"location":"user-guide/backends/datafusion/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>DuckDB - Alternative with DB support</li> <li>Polars - Alternative Rust-based engine</li> </ul>"},{"location":"user-guide/backends/duckdb/","title":"DuckDB","text":"<p>DuckDB is the default backend in ETLX. It's a fast, embedded analytical database that requires no setup.</p>"},{"location":"user-guide/backends/duckdb/#overview","title":"Overview","text":"<ul> <li>Type: Embedded analytical database</li> <li>Installation: Included by default</li> <li>Best For: Analytics, small-to-medium datasets, local development</li> </ul>"},{"location":"user-guide/backends/duckdb/#configuration","title":"Configuration","text":"<pre><code>engine: duckdb\n</code></pre> <p>No additional configuration required for local files.</p>"},{"location":"user-guide/backends/duckdb/#features","title":"Features","text":"Feature Supported Local files Yes Cloud storage Yes (S3, GCS, Azure) SQL support Full Parallel execution Yes Memory efficiency High"},{"location":"user-guide/backends/duckdb/#file-support","title":"File Support","text":"<p>DuckDB can read/write:</p> <ul> <li>Parquet (fastest)</li> <li>CSV</li> <li>JSON</li> </ul> <pre><code>source:\n  type: file\n  path: data.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/duckdb/#cloud-storage","title":"Cloud Storage","text":"<p>DuckDB supports cloud storage directly:</p> <pre><code>source:\n  type: file\n  path: s3://bucket/data.parquet\n\nsink:\n  type: file\n  path: s3://bucket/output/\n</code></pre>"},{"location":"user-guide/backends/duckdb/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/backends/duckdb/#use-parquet","title":"Use Parquet","text":"<p>Parquet is significantly faster than CSV:</p> <pre><code># Fast\nformat: parquet\n\n# Slower\nformat: csv\n</code></pre>"},{"location":"user-guide/backends/duckdb/#filter-early","title":"Filter Early","text":"<p>DuckDB can push filters down:</p> <pre><code>transforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n  # Further transforms run on filtered data\n</code></pre>"},{"location":"user-guide/backends/duckdb/#select-columns","title":"Select Columns","text":"<p>With Parquet, only selected columns are read:</p> <pre><code>transforms:\n  - op: select\n    columns: [id, amount]\n</code></pre>"},{"location":"user-guide/backends/duckdb/#memory-management","title":"Memory Management","text":"<p>DuckDB is memory-efficient but for very large files:</p> <pre><code># Configure memory limit\nengine = ETLXEngine(\n    backend=\"duckdb\",\n    memory_limit=\"4GB\"\n)\n</code></pre>"},{"location":"user-guide/backends/duckdb/#python-api","title":"Python API","text":"<pre><code>from etlx import ETLXEngine\n\nengine = ETLXEngine(backend=\"duckdb\")\n\n# Read and process\ntable = engine.read_file(\"data.parquet\", \"parquet\")\nfiltered = engine.filter(table, \"amount &gt; 100\")\nresult = engine.to_pandas(filtered)\n</code></pre>"},{"location":"user-guide/backends/duckdb/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Local development</li> <li>Small-to-medium datasets (up to ~100GB)</li> <li>Analytics and reporting</li> <li>Quick data exploration</li> </ul> <p>Consider alternatives for:</p> <ul> <li>Very large datasets \u2192 Spark</li> <li>Existing cloud warehouse \u2192 Snowflake/BigQuery</li> <li>Real-time streaming \u2192 ClickHouse</li> </ul>"},{"location":"user-guide/backends/duckdb/#related","title":"Related","text":"<ul> <li>Backends Overview</li> <li>Polars - Alternative local backend</li> <li>Spark - For larger datasets</li> </ul>"},{"location":"user-guide/backends/mysql/","title":"MySQL Backend","text":"<p>MySQL is a widely-used open-source relational database. ETLX supports MySQL as both a data source and sink.</p>"},{"location":"user-guide/backends/mysql/#installation","title":"Installation","text":"<pre><code>pip install etlx[mysql]\n# or\nuv add etlx[mysql]\n</code></pre>"},{"location":"user-guide/backends/mysql/#when-to-use-mysql","title":"When to Use MySQL","text":"<p>Ideal for:</p> <ul> <li>Data already in MySQL</li> <li>OLTP workloads with ETL</li> <li>Integration with existing MySQL infrastructure</li> <li>Web application databases</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Heavy analytics workloads (use DuckDB or columnar DBs)</li> <li>Complex window functions needed</li> <li>Large-scale data warehousing</li> </ul>"},{"location":"user-guide/backends/mysql/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/mysql/#connection-setup","title":"Connection Setup","text":"<pre><code>export MYSQL_HOST=localhost\nexport MYSQL_PORT=3306\nexport MYSQL_USER=etlx_user\nexport MYSQL_PASSWORD=your_password\nexport MYSQL_DATABASE=analytics\n</code></pre>"},{"location":"user-guide/backends/mysql/#using-env","title":"Using <code>.env</code>","text":"<pre><code>MYSQL_HOST=db.example.com\nMYSQL_PORT=3306\nMYSQL_USER=etlx_user\nMYSQL_PASSWORD=${MYSQL_PASSWORD}\nMYSQL_DATABASE=analytics\n</code></pre>"},{"location":"user-guide/backends/mysql/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>name: mysql_etl\nengine: mysql\n\nsource:\n  type: database\n  connection: mysql\n  table: raw_transactions\n\ntransforms:\n  - op: filter\n    predicate: transaction_date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [customer_id]\n    aggregations:\n      total_amount: sum(amount)\n\nsink:\n  type: database\n  connection: mysql\n  table: customer_totals\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/mysql/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/mysql/#transforms","title":"Transforms","text":"Transform Support MySQL SQL select Full <code>SELECT</code> rename Full <code>AS alias</code> filter Full <code>WHERE</code> derive_column Full Expressions cast Full <code>CAST()</code> fill_null Full <code>COALESCE()</code> / <code>IFNULL()</code> dedup Full <code>GROUP BY</code> with aggregation sort Full <code>ORDER BY</code> join Full <code>JOIN</code> aggregate Full <code>GROUP BY</code> union Full <code>UNION ALL</code> limit Full <code>LIMIT</code>"},{"location":"user-guide/backends/mysql/#data-types","title":"Data Types","text":"ETLX Type MySQL Type string VARCHAR / TEXT int INT / BIGINT float DOUBLE bool TINYINT(1) date DATE timestamp DATETIME decimal DECIMAL"},{"location":"user-guide/backends/mysql/#reading-data","title":"Reading Data","text":""},{"location":"user-guide/backends/mysql/#from-tables","title":"From Tables","text":"<pre><code>source:\n  type: database\n  connection: mysql\n  table: database.table_name\n</code></pre>"},{"location":"user-guide/backends/mysql/#from-queries","title":"From Queries","text":"<pre><code>source:\n  type: database\n  connection: mysql\n  query: |\n    SELECT o.*, c.name as customer_name\n    FROM orders o\n    INNER JOIN customers c ON o.customer_id = c.id\n    WHERE o.created_at &gt;= DATE_SUB(NOW(), INTERVAL 30 DAY)\n</code></pre>"},{"location":"user-guide/backends/mysql/#writing-data","title":"Writing Data","text":""},{"location":"user-guide/backends/mysql/#replace-mode","title":"Replace Mode","text":"<pre><code>sink:\n  type: database\n  connection: mysql\n  table: output_table\n  mode: replace  # TRUNCATE + INSERT\n</code></pre>"},{"location":"user-guide/backends/mysql/#append-mode","title":"Append Mode","text":"<pre><code>sink:\n  type: database\n  connection: mysql\n  table: output_table\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/mysql/#upsert-mode","title":"Upsert Mode","text":"<pre><code>sink:\n  type: database\n  connection: mysql\n  table: output_table\n  mode: upsert\n  upsert_keys: [id]\n</code></pre> <p>Uses <code>INSERT ... ON DUPLICATE KEY UPDATE</code>.</p>"},{"location":"user-guide/backends/mysql/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/backends/mysql/#1-use-indexes","title":"1. Use Indexes","text":"<pre><code>CREATE INDEX idx_orders_date ON orders(order_date);\nCREATE INDEX idx_orders_customer ON orders(customer_id);\n</code></pre>"},{"location":"user-guide/backends/mysql/#2-batch-inserts","title":"2. Batch Inserts","text":"<p>Configure batch size for large writes:</p> <pre><code>sink:\n  type: database\n  connection: mysql\n  table: output_table\n  options:\n    batch_size: 5000\n</code></pre>"},{"location":"user-guide/backends/mysql/#3-disable-foreign-key-checks-for-bulk-loads","title":"3. Disable Foreign Key Checks for Bulk Loads","text":"<pre><code>sink:\n  type: database\n  connection: mysql\n  table: output_table\n  mode: replace\n  options:\n    disable_foreign_keys: true\n</code></pre>"},{"location":"user-guide/backends/mysql/#example-etl-pipeline","title":"Example: ETL Pipeline","text":"<pre><code>name: sales_summary\ndescription: Aggregate daily sales\nengine: mysql\n\nsource:\n  type: database\n  connection: mysql\n  query: |\n    SELECT\n      s.sale_id,\n      s.product_id,\n      p.category,\n      s.quantity,\n      s.unit_price,\n      s.sale_date\n    FROM sales s\n    JOIN products p ON s.product_id = p.id\n    WHERE s.sale_date &gt;= CURDATE() - INTERVAL 7 DAY\n\ntransforms:\n  - op: derive_column\n    name: sale_amount\n    expr: quantity * unit_price\n\n  - op: aggregate\n    group_by: [category, sale_date]\n    aggregations:\n      total_sales: sum(sale_amount)\n      total_quantity: sum(quantity)\n      order_count: count(*)\n\nsink:\n  type: database\n  connection: mysql\n  table: sales_summary\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/mysql/#limitations","title":"Limitations","text":"<ol> <li>Window Functions: Limited compared to PostgreSQL</li> <li>CTEs: Supported in MySQL 8.0+ only</li> <li>JSON: Less mature than PostgreSQL</li> <li>Concurrent Writes: Table-level locking in MyISAM</li> </ol>"},{"location":"user-guide/backends/mysql/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/mysql/#connection-refused","title":"Connection Refused","text":"<pre><code>mysql.connector.errors.InterfaceError: 2003: Can't connect to MySQL server\n</code></pre> <p>Solutions: 1. Verify MySQL is running 2. Check host and port 3. Verify <code>bind-address</code> in MySQL config</p>"},{"location":"user-guide/backends/mysql/#access-denied","title":"Access Denied","text":"<pre><code>Access denied for user 'etlx_user'@'host'\n</code></pre> <p>Solution: <pre><code>GRANT SELECT, INSERT, UPDATE, DELETE ON database.* TO 'etlx_user'@'%';\nFLUSH PRIVILEGES;\n</code></pre></p>"},{"location":"user-guide/backends/mysql/#character-set-issues","title":"Character Set Issues","text":"<pre><code>Incorrect string value for column\n</code></pre> <p>Solution: Use UTF-8: <pre><code>export MYSQL_CHARSET=utf8mb4\n</code></pre></p>"},{"location":"user-guide/backends/mysql/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Database Sources - Database configuration</li> <li>Database Sinks - Writing to databases</li> </ul>"},{"location":"user-guide/backends/pandas/","title":"Pandas Backend","text":"<p>Pandas is the most widely-used Python data analysis library. ETLX supports Pandas via Ibis for compatibility with existing workflows.</p>"},{"location":"user-guide/backends/pandas/#installation","title":"Installation","text":"<p>Pandas is included with the base ETLX installation:</p> <pre><code>pip install etlx\n# or\nuv add etlx\n</code></pre>"},{"location":"user-guide/backends/pandas/#when-to-use-pandas","title":"When to Use Pandas","text":"<p>Ideal for:</p> <ul> <li>Compatibility with existing Pandas code</li> <li>Datasets that fit in memory</li> <li>When you need extensive Pandas ecosystem</li> <li>Quick prototyping</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Performance is critical (use DuckDB or Polars)</li> <li>Large datasets (use Spark or chunked processing)</li> <li>Production workloads (use DuckDB)</li> </ul>"},{"location":"user-guide/backends/pandas/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/pandas/#basic-usage","title":"Basic Usage","text":"<pre><code>name: pandas_pipeline\nengine: pandas\n\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 0\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n\nsink:\n  type: file\n  path: output/summary.csv\n  format: csv\n</code></pre>"},{"location":"user-guide/backends/pandas/#cli-override","title":"CLI Override","text":"<pre><code>etlx run pipeline.yml --engine pandas\n</code></pre>"},{"location":"user-guide/backends/pandas/#performance-characteristics","title":"Performance Characteristics","text":"Metric Performance Startup time ~200ms Memory efficiency Moderate Parallelism Single-threaded* Ecosystem Extensive <p>*Some operations use NumPy's parallel operations.</p>"},{"location":"user-guide/backends/pandas/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/pandas/#transforms","title":"Transforms","text":"Transform Support Notes select Full rename Full filter Full derive_column Full cast Full fill_null Full dedup Full sort Full join Full aggregate Full union Full limit Full"},{"location":"user-guide/backends/pandas/#data-types","title":"Data Types","text":"ETLX Type Pandas Type string object / string int int64 float float64 bool bool date datetime64[ns] timestamp datetime64[ns] decimal float64"},{"location":"user-guide/backends/pandas/#file-format-support","title":"File Format Support","text":""},{"location":"user-guide/backends/pandas/#csv","title":"CSV","text":"<pre><code>source:\n  type: file\n  path: data/input.csv\n  format: csv\n  options:\n    encoding: utf-8\n    delimiter: \",\"\n</code></pre>"},{"location":"user-guide/backends/pandas/#excel","title":"Excel","text":"<pre><code>source:\n  type: file\n  path: data/report.xlsx\n  format: excel\n  options:\n    sheet_name: Sheet1\n</code></pre>"},{"location":"user-guide/backends/pandas/#parquet","title":"Parquet","text":"<pre><code>source:\n  type: file\n  path: data/input.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/pandas/#json","title":"JSON","text":"<pre><code>source:\n  type: file\n  path: data/records.json\n  format: json\n  options:\n    orient: records\n</code></pre>"},{"location":"user-guide/backends/pandas/#memory-optimization","title":"Memory Optimization","text":""},{"location":"user-guide/backends/pandas/#1-use-appropriate-data-types","title":"1. Use Appropriate Data Types","text":"<p>Pandas can use significant memory. Optimize with type casting:</p> <pre><code>transforms:\n  - op: cast\n    columns:\n      id: int32  # Instead of int64\n      amount: float32  # Instead of float64\n      category: category  # Pandas categorical\n</code></pre>"},{"location":"user-guide/backends/pandas/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<pre><code>transforms:\n  - op: select\n    columns: [id, name, amount]\n</code></pre>"},{"location":"user-guide/backends/pandas/#3-filter-early","title":"3. Filter Early","text":"<pre><code>transforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  # Then do memory-intensive operations\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/backends/pandas/#example-data-analysis-pipeline","title":"Example: Data Analysis Pipeline","text":"<pre><code>name: sales_analysis\ndescription: Analyze sales data with Pandas\nengine: pandas\n\nsource:\n  type: file\n  path: data/sales.csv\n  format: csv\n\ntransforms:\n  # Clean data\n  - op: filter\n    predicate: amount &gt; 0\n\n  # Fill missing values\n  - op: fill_null\n    columns:\n      category: Unknown\n      discount: 0\n\n  # Calculate derived metrics\n  - op: derive_column\n    name: net_amount\n    expr: amount - discount\n\n  - op: derive_column\n    name: profit_margin\n    expr: (amount - cost) / amount\n\n  # Aggregate by category\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total_sales: sum(net_amount)\n      avg_margin: avg(profit_margin)\n      order_count: count(*)\n\n  # Sort by total sales\n  - op: sort\n    by:\n      - column: total_sales\n        order: desc\n\nchecks:\n  - check: not_null\n    columns: [category, total_sales]\n  - check: expression\n    expr: total_sales &gt;= 0\n\nsink:\n  type: file\n  path: output/category_analysis.csv\n  format: csv\n</code></pre>"},{"location":"user-guide/backends/pandas/#chunked-processing","title":"Chunked Processing","text":"<p>For files larger than memory, Pandas can read in chunks. However, this is handled automatically by ETLX when possible. For very large files, consider using DuckDB or Polars instead.</p>"},{"location":"user-guide/backends/pandas/#pandas-specific-features","title":"Pandas-Specific Features","text":"<p>While ETLX provides a backend-agnostic API, you can leverage Pandas features:</p>"},{"location":"user-guide/backends/pandas/#datetime-operations","title":"DateTime Operations","text":"<pre><code>transforms:\n  - op: derive_column\n    name: month\n    expr: extract(month from order_date)\n\n  - op: derive_column\n    name: day_of_week\n    expr: extract(dow from order_date)\n</code></pre>"},{"location":"user-guide/backends/pandas/#string-operations","title":"String Operations","text":"<pre><code>transforms:\n  - op: derive_column\n    name: name_upper\n    expr: upper(name)\n\n  - op: derive_column\n    name: email_domain\n    expr: split_part(email, '@', 2)\n</code></pre>"},{"location":"user-guide/backends/pandas/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature Pandas DuckDB Polars Ecosystem Extensive Growing Growing Performance Moderate Fast Fast Memory usage High Low Low Learning curve Low Low Medium Production ready Yes* Yes Yes <p>*With appropriate memory management.</p>"},{"location":"user-guide/backends/pandas/#limitations","title":"Limitations","text":"<ol> <li>Memory Usage: Loads entire dataset into memory</li> <li>Performance: Single-threaded for most operations</li> <li>Large Files: Not suitable for files &gt; available RAM</li> <li>Production: Consider DuckDB/Polars for better performance</li> </ol>"},{"location":"user-guide/backends/pandas/#when-to-migrate-to-other-backends","title":"When to Migrate to Other Backends","text":"<p>Consider switching from Pandas when:</p> <ul> <li>Processing takes too long</li> <li>Memory errors occur</li> <li>Dataset size exceeds 1GB</li> <li>Running in production</li> </ul> <p>Migration is easy - just change the engine:</p> <pre><code># Before\nengine: pandas\n\n# After\nengine: duckdb  # or polars\n</code></pre>"},{"location":"user-guide/backends/pandas/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/pandas/#memory-error","title":"Memory Error","text":"<pre><code>MemoryError: Unable to allocate array\n</code></pre> <p>Solutions: 1. Use DuckDB or Polars instead 2. Read fewer columns 3. Filter data at source 4. Use chunked processing</p>"},{"location":"user-guide/backends/pandas/#slow-performance","title":"Slow Performance","text":"<p>If pipeline is slow:</p> <ol> <li>Profile with <code>--verbose</code></li> <li>Consider switching to DuckDB or Polars</li> <li>Optimize data types</li> <li>Filter early</li> </ol>"},{"location":"user-guide/backends/pandas/#type-inference-issues","title":"Type Inference Issues","text":"<pre><code>DtypeWarning: Columns have mixed types\n</code></pre> <p>Solution: Specify types explicitly: <pre><code>source:\n  type: file\n  path: data.csv\n  format: csv\n  options:\n    dtype:\n      id: int64\n      amount: float64\n</code></pre></p>"},{"location":"user-guide/backends/pandas/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>DuckDB - Faster alternative</li> <li>Polars - Modern DataFrame library</li> </ul>"},{"location":"user-guide/backends/polars/","title":"Polars Backend","text":"<p>Polars is a blazing fast DataFrame library written in Rust. ETLX uses Polars via Ibis for excellent single-machine performance.</p>"},{"location":"user-guide/backends/polars/#installation","title":"Installation","text":"<pre><code>pip install etlx[polars]\n# or\nuv add etlx[polars]\n</code></pre>"},{"location":"user-guide/backends/polars/#when-to-use-polars","title":"When to Use Polars","text":"<p>Ideal for:</p> <ul> <li>Large files that don't fit in memory (streaming support)</li> <li>CPU-intensive transformations</li> <li>When you need Rust-level performance</li> <li>Single-machine workloads with millions of rows</li> </ul> <p>Consider alternatives when:</p> <ul> <li>You need SQL features like window functions (use DuckDB)</li> <li>Distributed processing is required (use Spark)</li> <li>Working with databases directly (use DuckDB)</li> </ul>"},{"location":"user-guide/backends/polars/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/polars/#basic-usage","title":"Basic Usage","text":"<pre><code>name: polars_pipeline\nengine: polars\n\nsource:\n  type: file\n  path: data/large_dataset.parquet\n  format: parquet\n\ntransforms:\n  - op: filter\n    predicate: amount &gt; 100\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n\nsink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/polars/#cli-override","title":"CLI Override","text":"<pre><code>etlx run pipeline.yml --engine polars\n</code></pre>"},{"location":"user-guide/backends/polars/#performance-characteristics","title":"Performance Characteristics","text":"Metric Performance Startup time ~100ms Memory efficiency Excellent (lazy evaluation) Parallelism Multi-threaded Streaming Supported"},{"location":"user-guide/backends/polars/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/polars/#transforms","title":"Transforms","text":"Transform Support Notes select Full rename Full filter Full derive_column Full cast Full fill_null Full dedup Full sort Full join Full aggregate Full union Full limit Full"},{"location":"user-guide/backends/polars/#data-types","title":"Data Types","text":"ETLX Type Polars Type string Utf8 int Int64 float Float64 bool Boolean date Date timestamp Datetime decimal Decimal"},{"location":"user-guide/backends/polars/#optimization-tips","title":"Optimization Tips","text":""},{"location":"user-guide/backends/polars/#1-use-parquet-format","title":"1. Use Parquet Format","text":"<p>Polars is highly optimized for Parquet:</p> <pre><code>source:\n  type: file\n  path: data/input.parquet\n  format: parquet\n\nsink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/polars/#2-filter-early","title":"2. Filter Early","text":"<p>Push filters as early as possible in your pipeline:</p> <pre><code>transforms:\n  # Filter first to reduce data volume\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  # Then do expensive operations\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/backends/polars/#3-select-only-needed-columns","title":"3. Select Only Needed Columns","text":"<pre><code>transforms:\n  - op: select\n    columns: [id, name, amount]  # Reduces memory usage\n</code></pre>"},{"location":"user-guide/backends/polars/#streaming-large-files","title":"Streaming Large Files","text":"<p>Polars can process files larger than memory using streaming:</p> <pre><code>name: stream_large_file\nengine: polars\n\nsource:\n  type: file\n  path: data/huge_file.csv\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: status = 'active'\n  - op: aggregate\n    group_by: [region]\n    aggregations:\n      count: count(*)\n\nsink:\n  type: file\n  path: output/summary.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/polars/#limitations","title":"Limitations","text":"<ol> <li>SQL Functions: Some advanced SQL functions may not be available</li> <li>Database Connections: Less efficient than DuckDB for direct DB queries</li> <li>Window Functions: Limited compared to SQL-based backends</li> </ol>"},{"location":"user-guide/backends/polars/#comparison-with-duckdb","title":"Comparison with DuckDB","text":"Feature Polars DuckDB In-memory performance Excellent Excellent SQL support Via Ibis Native Streaming Built-in Limited Database connectivity Via connectors Native Memory efficiency Excellent Good"},{"location":"user-guide/backends/polars/#example-high-performance-analytics","title":"Example: High-Performance Analytics","text":"<pre><code>name: analytics_pipeline\ndescription: Process large analytics dataset\nengine: polars\n\nsource:\n  type: file\n  path: data/events.parquet\n  format: parquet\n\ntransforms:\n  # Filter to relevant date range\n  - op: filter\n    predicate: event_date &gt;= '2025-01-01'\n\n  # Select needed columns\n  - op: select\n    columns: [user_id, event_type, value, event_date]\n\n  # Aggregate by user and event type\n  - op: aggregate\n    group_by: [user_id, event_type]\n    aggregations:\n      total_value: sum(value)\n      event_count: count(*)\n\n  # Filter significant users\n  - op: filter\n    predicate: event_count &gt;= 10\n\nsink:\n  type: file\n  path: output/user_analytics.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/polars/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/polars/#import-error","title":"Import Error","text":"<pre><code>ModuleNotFoundError: No module named 'polars'\n</code></pre> <p>Solution: Install Polars extra: <pre><code>pip install etlx[polars]\n</code></pre></p>"},{"location":"user-guide/backends/polars/#memory-issues","title":"Memory Issues","text":"<p>If you encounter memory errors with large files:</p> <ol> <li>Use Parquet format (columnar, efficient)</li> <li>Filter early in the pipeline</li> <li>Select only needed columns</li> <li>Consider using streaming mode</li> </ol>"},{"location":"user-guide/backends/polars/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>DuckDB - Alternative for SQL-heavy workloads</li> <li>Performance Best Practices</li> </ul>"},{"location":"user-guide/backends/postgresql/","title":"PostgreSQL Backend","text":"<p>PostgreSQL is a powerful open-source relational database. ETLX can execute transformations directly in PostgreSQL or use it as a source/sink.</p>"},{"location":"user-guide/backends/postgresql/#installation","title":"Installation","text":"<pre><code>pip install etlx[postgres]\n# or\nuv add etlx[postgres]\n</code></pre>"},{"location":"user-guide/backends/postgresql/#when-to-use-postgresql","title":"When to Use PostgreSQL","text":"<p>Ideal for:</p> <ul> <li>Data already in PostgreSQL</li> <li>ACID-compliant transactions needed</li> <li>Complex SQL with CTEs, window functions</li> <li>Integration with existing PostgreSQL infrastructure</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Analytics on large datasets (use DuckDB or columnar DBs)</li> <li>File-based processing</li> <li>No existing PostgreSQL infrastructure</li> </ul>"},{"location":"user-guide/backends/postgresql/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/postgresql/#connection-setup","title":"Connection Setup","text":"<p>Set environment variables:</p> <pre><code>export POSTGRES_HOST=localhost\nexport POSTGRES_PORT=5432\nexport POSTGRES_USER=etlx_user\nexport POSTGRES_PASSWORD=your_password\nexport POSTGRES_DATABASE=analytics\n</code></pre> <p>Or use a connection URL:</p> <pre><code>export DATABASE_URL=postgresql://user:password@localhost:5432/analytics\n</code></pre>"},{"location":"user-guide/backends/postgresql/#using-env","title":"Using <code>.env</code>","text":"<pre><code>POSTGRES_HOST=db.example.com\nPOSTGRES_PORT=5432\nPOSTGRES_USER=etlx_user\nPOSTGRES_PASSWORD=${POSTGRES_PASSWORD}\nPOSTGRES_DATABASE=analytics\nPOSTGRES_SSLMODE=require\n</code></pre>"},{"location":"user-guide/backends/postgresql/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>name: postgres_etl\nengine: postgres\n\nsource:\n  type: database\n  connection: postgres\n  table: raw_orders\n\ntransforms:\n  - op: filter\n    predicate: order_date &gt;= '2025-01-01'\n  - op: derive_column\n    name: order_total\n    expr: quantity * unit_price\n\nsink:\n  type: database\n  connection: postgres\n  table: processed_orders\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/postgresql/#connection-options","title":"Connection Options","text":""},{"location":"user-guide/backends/postgresql/#ssl-configuration","title":"SSL Configuration","text":"<pre><code>export POSTGRES_SSLMODE=require\nexport POSTGRES_SSLROOTCERT=/path/to/ca.crt\nexport POSTGRES_SSLCERT=/path/to/client.crt\nexport POSTGRES_SSLKEY=/path/to/client.key\n</code></pre>"},{"location":"user-guide/backends/postgresql/#connection-pooling","title":"Connection Pooling","text":"<p>For production, use connection pooling:</p> <pre><code># With PgBouncer\nexport POSTGRES_HOST=pgbouncer.example.com\nexport POSTGRES_PORT=6432\n</code></pre>"},{"location":"user-guide/backends/postgresql/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/postgresql/#transforms","title":"Transforms","text":"Transform Support PostgreSQL SQL select Full <code>SELECT</code> rename Full <code>AS alias</code> filter Full <code>WHERE</code> derive_column Full Expressions cast Full <code>CAST()</code> / <code>::</code> fill_null Full <code>COALESCE()</code> dedup Full <code>DISTINCT ON</code> sort Full <code>ORDER BY</code> join Full <code>JOIN</code> aggregate Full <code>GROUP BY</code> union Full <code>UNION ALL</code> limit Full <code>LIMIT</code>"},{"location":"user-guide/backends/postgresql/#data-types","title":"Data Types","text":"ETLX Type PostgreSQL Type string TEXT / VARCHAR int INTEGER / BIGINT float DOUBLE PRECISION bool BOOLEAN date DATE timestamp TIMESTAMP decimal NUMERIC"},{"location":"user-guide/backends/postgresql/#reading-data","title":"Reading Data","text":""},{"location":"user-guide/backends/postgresql/#from-tables","title":"From Tables","text":"<pre><code>source:\n  type: database\n  connection: postgres\n  table: schema.table_name\n</code></pre>"},{"location":"user-guide/backends/postgresql/#from-queries","title":"From Queries","text":"<pre><code>source:\n  type: database\n  connection: postgres\n  query: |\n    SELECT o.*, c.name as customer_name\n    FROM orders o\n    JOIN customers c ON o.customer_id = c.id\n    WHERE o.created_at &gt;= NOW() - INTERVAL '30 days'\n</code></pre>"},{"location":"user-guide/backends/postgresql/#with-ctes","title":"With CTEs","text":"<pre><code>source:\n  type: database\n  connection: postgres\n  query: |\n    WITH recent_orders AS (\n      SELECT * FROM orders\n      WHERE created_at &gt;= NOW() - INTERVAL '7 days'\n    ),\n    order_totals AS (\n      SELECT customer_id, SUM(amount) as total\n      FROM recent_orders\n      GROUP BY customer_id\n    )\n    SELECT * FROM order_totals\n</code></pre>"},{"location":"user-guide/backends/postgresql/#writing-data","title":"Writing Data","text":""},{"location":"user-guide/backends/postgresql/#replace-mode","title":"Replace Mode","text":"<pre><code>sink:\n  type: database\n  connection: postgres\n  table: output_table\n  mode: replace  # TRUNCATE + INSERT\n</code></pre>"},{"location":"user-guide/backends/postgresql/#append-mode","title":"Append Mode","text":"<pre><code>sink:\n  type: database\n  connection: postgres\n  table: output_table\n  mode: append  # INSERT only\n</code></pre>"},{"location":"user-guide/backends/postgresql/#upsert-mode","title":"Upsert Mode","text":"<pre><code>sink:\n  type: database\n  connection: postgres\n  table: output_table\n  mode: upsert\n  upsert_keys: [id]\n</code></pre> <p>Uses <code>INSERT ... ON CONFLICT DO UPDATE</code>.</p>"},{"location":"user-guide/backends/postgresql/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/backends/postgresql/#1-index-your-tables","title":"1. Index Your Tables","text":"<pre><code>CREATE INDEX idx_orders_date ON orders(order_date);\nCREATE INDEX idx_orders_customer ON orders(customer_id);\n</code></pre>"},{"location":"user-guide/backends/postgresql/#2-use-explain-analyze","title":"2. Use EXPLAIN ANALYZE","text":"<p>Test your queries:</p> <pre><code>EXPLAIN ANALYZE\nSELECT * FROM orders WHERE order_date &gt;= '2025-01-01';\n</code></pre>"},{"location":"user-guide/backends/postgresql/#3-partition-large-tables","title":"3. Partition Large Tables","text":"<pre><code>CREATE TABLE orders (\n  id SERIAL,\n  order_date DATE,\n  amount NUMERIC\n) PARTITION BY RANGE (order_date);\n\nCREATE TABLE orders_2025_q1 PARTITION OF orders\n  FOR VALUES FROM ('2025-01-01') TO ('2025-04-01');\n</code></pre>"},{"location":"user-guide/backends/postgresql/#4-batch-inserts","title":"4. Batch Inserts","text":"<p>For large writes, data is batched automatically. Configure batch size:</p> <pre><code>sink:\n  type: database\n  connection: postgres\n  table: output_table\n  options:\n    batch_size: 10000\n</code></pre>"},{"location":"user-guide/backends/postgresql/#example-data-warehouse-load","title":"Example: Data Warehouse Load","text":"<pre><code>name: warehouse_load\ndescription: Load transformed data into warehouse\nengine: postgres\n\nsource:\n  type: database\n  connection: postgres\n  query: |\n    SELECT\n      o.id,\n      o.customer_id,\n      c.name as customer_name,\n      c.region,\n      o.order_date,\n      o.amount,\n      o.status\n    FROM staging.orders o\n    JOIN staging.customers c ON o.customer_id = c.id\n    WHERE o.processed = false\n\ntransforms:\n  - op: filter\n    predicate: status = 'completed'\n\n  - op: derive_column\n    name: order_month\n    expr: date_trunc('month', order_date)\n\n  - op: derive_column\n    name: amount_with_tax\n    expr: amount * 1.1\n\nchecks:\n  - check: not_null\n    columns: [id, customer_id, amount]\n  - check: unique\n    columns: [id]\n\nsink:\n  type: database\n  connection: postgres\n  table: warehouse.fact_orders\n  mode: append\n</code></pre>"},{"location":"user-guide/backends/postgresql/#using-postgresql-extensions","title":"Using PostgreSQL Extensions","text":""},{"location":"user-guide/backends/postgresql/#postgis-spatial","title":"PostGIS (Spatial)","text":"<pre><code>source:\n  type: database\n  connection: postgres\n  query: |\n    SELECT id, name, ST_AsText(location) as location_wkt\n    FROM stores\n    WHERE ST_DWithin(location, ST_MakePoint(-122.4, 37.8), 10000)\n</code></pre>"},{"location":"user-guide/backends/postgresql/#timescaledb-time-series","title":"TimescaleDB (Time Series)","text":"<pre><code>source:\n  type: database\n  connection: postgres\n  query: |\n    SELECT time_bucket('1 hour', timestamp) as hour,\n           avg(temperature) as avg_temp\n    FROM sensor_data\n    WHERE timestamp &gt;= NOW() - INTERVAL '24 hours'\n    GROUP BY hour\n</code></pre>"},{"location":"user-guide/backends/postgresql/#limitations","title":"Limitations","text":"<ol> <li>Performance: Row-based storage less efficient for analytics</li> <li>Scale: Single-node limits</li> <li>Concurrent Writes: Lock contention on heavy writes</li> </ol>"},{"location":"user-guide/backends/postgresql/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/postgresql/#connection-refused","title":"Connection Refused","text":"<pre><code>psycopg2.OperationalError: could not connect to server: Connection refused\n</code></pre> <p>Solutions: 1. Verify PostgreSQL is running 2. Check host and port 3. Verify firewall rules 4. Check <code>pg_hba.conf</code> allows connections</p>"},{"location":"user-guide/backends/postgresql/#permission-denied","title":"Permission Denied","text":"<pre><code>permission denied for table orders\n</code></pre> <p>Solution: <pre><code>GRANT SELECT ON orders TO etlx_user;\nGRANT INSERT, UPDATE ON processed_orders TO etlx_user;\n</code></pre></p>"},{"location":"user-guide/backends/postgresql/#ssl-required","title":"SSL Required","text":"<pre><code>FATAL: no pg_hba.conf entry for host\n</code></pre> <p>Solution: <pre><code>export POSTGRES_SSLMODE=require\n</code></pre></p>"},{"location":"user-guide/backends/postgresql/#timeout","title":"Timeout","text":"<pre><code>canceling statement due to statement timeout\n</code></pre> <p>Solution: Increase timeout: <pre><code>SET statement_timeout = '300s';\n</code></pre></p> <p>Or configure in connection: <pre><code>export POSTGRES_OPTIONS=\"-c statement_timeout=300s\"\n</code></pre></p>"},{"location":"user-guide/backends/postgresql/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Database Sources - Database configuration</li> <li>Database Sinks - Writing to databases</li> </ul>"},{"location":"user-guide/backends/snowflake/","title":"Snowflake Backend","text":"<p>Snowflake is a cloud-native data warehouse. ETLX can push transformations directly to Snowflake for efficient in-warehouse processing.</p>"},{"location":"user-guide/backends/snowflake/#installation","title":"Installation","text":"<pre><code>pip install etlx[snowflake]\n# or\nuv add etlx[snowflake]\n</code></pre>"},{"location":"user-guide/backends/snowflake/#when-to-use-snowflake","title":"When to Use Snowflake","text":"<p>Ideal for:</p> <ul> <li>Data already in Snowflake</li> <li>Complex SQL transformations</li> <li>Scalable compute without infrastructure management</li> <li>Enterprise data warehouse workloads</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Data is local files (use DuckDB)</li> <li>Cost is a primary concern</li> <li>Real-time processing needed</li> </ul>"},{"location":"user-guide/backends/snowflake/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/snowflake/#connection-setup","title":"Connection Setup","text":"<p>Set environment variables:</p> <pre><code>export SNOWFLAKE_ACCOUNT=xy12345.us-east-1\nexport SNOWFLAKE_USER=etlx_user\nexport SNOWFLAKE_PASSWORD=your_password\nexport SNOWFLAKE_DATABASE=analytics\nexport SNOWFLAKE_SCHEMA=public\nexport SNOWFLAKE_WAREHOUSE=compute_wh\n</code></pre> <p>Or use <code>.env</code>:</p> <pre><code>SNOWFLAKE_ACCOUNT=xy12345.us-east-1\nSNOWFLAKE_USER=etlx_user\nSNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}\nSNOWFLAKE_DATABASE=analytics\nSNOWFLAKE_SCHEMA=public\nSNOWFLAKE_WAREHOUSE=compute_wh\nSNOWFLAKE_ROLE=analyst_role\n</code></pre>"},{"location":"user-guide/backends/snowflake/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>name: snowflake_etl\nengine: snowflake\n\nsource:\n  type: database\n  connection: snowflake\n  table: raw_sales\n\ntransforms:\n  - op: filter\n    predicate: sale_date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [region, product_category]\n    aggregations:\n      total_revenue: sum(amount)\n      order_count: count(*)\n\nsink:\n  type: database\n  connection: snowflake\n  table: sales_summary\n  mode: replace\n</code></pre>"},{"location":"user-guide/backends/snowflake/#key-pair-authentication","title":"Key-Pair Authentication","text":"<p>For production, use key-pair authentication:</p> <pre><code>export SNOWFLAKE_PRIVATE_KEY_PATH=/path/to/rsa_key.p8\nexport SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=your_passphrase\n</code></pre> <pre><code>source:\n  type: database\n  connection: snowflake\n  auth: key_pair\n  table: source_table\n</code></pre>"},{"location":"user-guide/backends/snowflake/#warehouse-configuration","title":"Warehouse Configuration","text":""},{"location":"user-guide/backends/snowflake/#specify-warehouse-size","title":"Specify Warehouse Size","text":"<p>Use warehouse parameters for compute scaling:</p> <pre><code># Use larger warehouse for heavy transformations\nexport SNOWFLAKE_WAREHOUSE=compute_xl\n</code></pre>"},{"location":"user-guide/backends/snowflake/#auto-suspend","title":"Auto-Suspend","text":"<p>Configure warehouse to auto-suspend:</p> <pre><code>ALTER WAREHOUSE compute_wh SET AUTO_SUSPEND = 60;\n</code></pre>"},{"location":"user-guide/backends/snowflake/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/snowflake/#transforms","title":"Transforms","text":"<p>All transforms are pushed to Snowflake SQL:</p> Transform Support Snowflake SQL select Full <code>SELECT columns</code> rename Full <code>AS alias</code> filter Full <code>WHERE</code> derive_column Full Expressions cast Full <code>CAST()</code> / <code>::</code> fill_null Full <code>COALESCE()</code> / <code>IFNULL()</code> dedup Full <code>QUALIFY ROW_NUMBER()</code> sort Full <code>ORDER BY</code> join Full <code>JOIN</code> aggregate Full <code>GROUP BY</code> union Full <code>UNION ALL</code> limit Full <code>LIMIT</code>"},{"location":"user-guide/backends/snowflake/#data-types","title":"Data Types","text":"ETLX Type Snowflake Type string VARCHAR int INTEGER float FLOAT bool BOOLEAN date DATE timestamp TIMESTAMP_NTZ decimal NUMBER(38,9)"},{"location":"user-guide/backends/snowflake/#reading-from-stages","title":"Reading from Stages","text":"<p>Load data from Snowflake stages:</p> <pre><code>source:\n  type: database\n  connection: snowflake\n  query: |\n    SELECT * FROM @my_stage/data/\n    (FILE_FORMAT =&gt; 'my_csv_format')\n</code></pre>"},{"location":"user-guide/backends/snowflake/#writing-to-tables","title":"Writing to Tables","text":""},{"location":"user-guide/backends/snowflake/#replace-mode","title":"Replace Mode","text":"<pre><code>sink:\n  type: database\n  connection: snowflake\n  table: output_table\n  mode: replace  # TRUNCATE + INSERT\n</code></pre>"},{"location":"user-guide/backends/snowflake/#append-mode","title":"Append Mode","text":"<pre><code>sink:\n  type: database\n  connection: snowflake\n  table: output_table\n  mode: append  # INSERT only\n</code></pre>"},{"location":"user-guide/backends/snowflake/#merge-mode","title":"Merge Mode","text":"<pre><code>sink:\n  type: database\n  connection: snowflake\n  table: output_table\n  mode: merge\n  merge_keys: [id]\n</code></pre>"},{"location":"user-guide/backends/snowflake/#cost-optimization","title":"Cost Optimization","text":""},{"location":"user-guide/backends/snowflake/#1-use-appropriate-warehouse-size","title":"1. Use Appropriate Warehouse Size","text":"<pre><code># Small warehouse for light transformations\n# XS: 1 credit/hour\n# S:  2 credits/hour\n# M:  4 credits/hour\n# L:  8 credits/hour\n</code></pre>"},{"location":"user-guide/backends/snowflake/#2-filter-early","title":"2. Filter Early","text":"<p>Reduce data scanned:</p> <pre><code>transforms:\n  # Filter first to reduce compute\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/backends/snowflake/#3-use-clustering-keys","title":"3. Use Clustering Keys","text":"<p>For large tables, define clustering:</p> <pre><code>ALTER TABLE sales CLUSTER BY (date, region);\n</code></pre>"},{"location":"user-guide/backends/snowflake/#example-data-warehouse-etl","title":"Example: Data Warehouse ETL","text":"<pre><code>name: warehouse_etl\ndescription: Transform raw data into analytics tables\nengine: snowflake\n\nsource:\n  type: database\n  connection: snowflake\n  query: |\n    SELECT\n      o.order_id,\n      o.customer_id,\n      o.order_date,\n      oi.product_id,\n      oi.quantity,\n      oi.unit_price,\n      p.category,\n      c.region\n    FROM raw.orders o\n    JOIN raw.order_items oi ON o.order_id = oi.order_id\n    JOIN raw.products p ON oi.product_id = p.product_id\n    JOIN raw.customers c ON o.customer_id = c.customer_id\n    WHERE o.order_date &gt;= DATEADD(day, -30, CURRENT_DATE())\n\ntransforms:\n  - op: derive_column\n    name: line_total\n    expr: quantity * unit_price\n\n  - op: aggregate\n    group_by: [region, category, order_date]\n    aggregations:\n      total_revenue: sum(line_total)\n      total_orders: count(distinct order_id)\n      total_items: sum(quantity)\n\nchecks:\n  - check: not_null\n    columns: [region, category, total_revenue]\n  - check: row_count\n    min: 1\n\nsink:\n  type: database\n  connection: snowflake\n  table: analytics.daily_sales\n  mode: merge\n  merge_keys: [region, category, order_date]\n</code></pre>"},{"location":"user-guide/backends/snowflake/#limitations","title":"Limitations","text":"<ol> <li>Cost: Pay per compute second</li> <li>Latency: Connection overhead (~1-2s)</li> <li>Local Data: Must stage local files first</li> <li>Concurrent Queries: Subject to warehouse queueing</li> </ol>"},{"location":"user-guide/backends/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/snowflake/#connection-failed","title":"Connection Failed","text":"<pre><code>snowflake.connector.errors.DatabaseError: Connection failed\n</code></pre> <p>Solutions: 1. Verify account identifier format: <code>account.region</code> 2. Check network connectivity 3. Verify credentials</p>"},{"location":"user-guide/backends/snowflake/#warehouse-suspended","title":"Warehouse Suspended","text":"<pre><code>Warehouse 'COMPUTE_WH' is suspended\n</code></pre> <p>Solution: Warehouse auto-resumes, or manually: <pre><code>ALTER WAREHOUSE compute_wh RESUME;\n</code></pre></p>"},{"location":"user-guide/backends/snowflake/#permission-denied","title":"Permission Denied","text":"<pre><code>SQL access control error\n</code></pre> <p>Solution: Grant necessary permissions: <pre><code>GRANT USAGE ON WAREHOUSE compute_wh TO ROLE etlx_role;\nGRANT SELECT ON TABLE raw.sales TO ROLE etlx_role;\nGRANT INSERT ON TABLE analytics.summary TO ROLE etlx_role;\n</code></pre></p>"},{"location":"user-guide/backends/snowflake/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Database Sources - Database configuration</li> <li>Database Sinks - Writing to databases</li> </ul>"},{"location":"user-guide/backends/spark/","title":"Apache Spark Backend","text":"<p>Apache Spark is a distributed computing framework for big data processing. Use Spark for datasets that exceed single-machine capacity.</p>"},{"location":"user-guide/backends/spark/#installation","title":"Installation","text":"<pre><code>pip install etlx[spark]\n# or\nuv add etlx[spark]\n</code></pre> <p>Java Required</p> <p>Spark requires Java 8, 11, or 17. Verify with <code>java -version</code>.</p>"},{"location":"user-guide/backends/spark/#when-to-use-spark","title":"When to Use Spark","text":"<p>Ideal for:</p> <ul> <li>Datasets that don't fit on a single machine</li> <li>Distributed cluster environments (YARN, Kubernetes, EMR)</li> <li>Integration with Hadoop ecosystem</li> <li>Production data lake pipelines</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Data fits in memory (use DuckDB or Polars)</li> <li>Low latency is critical (Spark has startup overhead)</li> <li>Simple transformations (Spark is overkill)</li> </ul>"},{"location":"user-guide/backends/spark/#configuration","title":"Configuration","text":""},{"location":"user-guide/backends/spark/#basic-usage","title":"Basic Usage","text":"<pre><code>name: spark_pipeline\nengine: spark\n\nsource:\n  type: file\n  path: s3://bucket/data/*.parquet\n  format: parquet\n\ntransforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n  - op: aggregate\n    group_by: [region, category]\n    aggregations:\n      revenue: sum(amount)\n\nsink:\n  type: file\n  path: s3://bucket/output/\n  format: parquet\n</code></pre>"},{"location":"user-guide/backends/spark/#spark-configuration","title":"Spark Configuration","text":"<p>Configure Spark session via environment variables:</p> <pre><code>export SPARK_MASTER=spark://master:7077\nexport SPARK_EXECUTOR_MEMORY=4g\nexport SPARK_EXECUTOR_CORES=2\n</code></pre> <p>Or in <code>.env</code>:</p> <pre><code>SPARK_MASTER=local[*]\nSPARK_EXECUTOR_MEMORY=8g\nSPARK_DRIVER_MEMORY=4g\n</code></pre>"},{"location":"user-guide/backends/spark/#deployment-modes","title":"Deployment Modes","text":""},{"location":"user-guide/backends/spark/#local-mode","title":"Local Mode","text":"<p>For development and testing:</p> <pre><code>export SPARK_MASTER=local[*]\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"user-guide/backends/spark/#standalone-cluster","title":"Standalone Cluster","text":"<pre><code>export SPARK_MASTER=spark://master:7077\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"user-guide/backends/spark/#yarn","title":"YARN","text":"<pre><code>export SPARK_MASTER=yarn\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"user-guide/backends/spark/#kubernetes","title":"Kubernetes","text":"<pre><code>export SPARK_MASTER=k8s://https://kubernetes:443\nexport SPARK_KUBERNETES_CONTAINER_IMAGE=spark:3.5.0\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"user-guide/backends/spark/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/backends/spark/#transforms","title":"Transforms","text":"Transform Support Notes select Full rename Full filter Full derive_column Full cast Full fill_null Full dedup Full Uses <code>dropDuplicates</code> sort Full Distributed sort join Full Broadcast/shuffle join aggregate Full union Full limit Full"},{"location":"user-guide/backends/spark/#data-sources","title":"Data Sources","text":"<p>Spark excels at reading from distributed storage:</p> <pre><code># S3\nsource:\n  type: file\n  path: s3a://bucket/data/*.parquet\n  format: parquet\n\n# HDFS\nsource:\n  type: file\n  path: hdfs://namenode/data/*.parquet\n  format: parquet\n\n# Delta Lake\nsource:\n  type: file\n  path: s3://bucket/delta-table/\n  format: delta\n</code></pre>"},{"location":"user-guide/backends/spark/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/backends/spark/#1-partition-data","title":"1. Partition Data","text":"<p>For large datasets, ensure data is partitioned:</p> <pre><code>sink:\n  type: file\n  path: s3://bucket/output/\n  format: parquet\n  options:\n    partition_by: [date, region]\n</code></pre>"},{"location":"user-guide/backends/spark/#2-use-appropriate-file-formats","title":"2. Use Appropriate File Formats","text":"<ul> <li>Parquet: Best for analytics (columnar, compressed)</li> <li>Delta: ACID transactions, time travel</li> <li>ORC: Hive compatibility</li> </ul>"},{"location":"user-guide/backends/spark/#3-broadcast-small-tables","title":"3. Broadcast Small Tables","text":"<p>For joins with small dimension tables:</p> <pre><code>transforms:\n  - op: join\n    right:\n      type: file\n      path: s3://bucket/dim_products.parquet  # Small table\n      format: parquet\n    on: [product_id]\n    how: left\n</code></pre>"},{"location":"user-guide/backends/spark/#4-filter-early","title":"4. Filter Early","text":"<p>Push predicates as early as possible:</p> <pre><code>transforms:\n  # Filter first - reduces shuffle\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  # Then aggregate\n  - op: aggregate\n    group_by: [category]\n    aggregations:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/backends/spark/#cloud-integration","title":"Cloud Integration","text":""},{"location":"user-guide/backends/spark/#aws-emr","title":"AWS EMR","text":"<pre><code># Submit to EMR\nexport SPARK_MASTER=yarn\nexport AWS_REGION=us-east-1\n\netlx run pipeline.yml --engine spark\n</code></pre>"},{"location":"user-guide/backends/spark/#databricks","title":"Databricks","text":"<p>Use Databricks-specific configuration:</p> <pre><code>export SPARK_MASTER=databricks\nexport DATABRICKS_HOST=https://xxx.cloud.databricks.com\nexport DATABRICKS_TOKEN=dapi...\n</code></pre>"},{"location":"user-guide/backends/spark/#google-dataproc","title":"Google Dataproc","text":"<pre><code>export SPARK_MASTER=yarn\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json\n</code></pre>"},{"location":"user-guide/backends/spark/#example-large-scale-etl","title":"Example: Large-Scale ETL","text":"<pre><code>name: daily_sales_etl\ndescription: Process daily sales across all regions\nengine: spark\n\nsource:\n  type: file\n  path: s3://data-lake/raw/sales/date=${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  # Filter valid records\n  - op: filter\n    predicate: amount &gt; 0 AND status = 'completed'\n\n  # Enrich with product data\n  - op: join\n    right:\n      type: file\n      path: s3://data-lake/dim/products/\n      format: parquet\n    on: [product_id]\n    how: left\n\n  # Aggregate by region and category\n  - op: aggregate\n    group_by: [region, category, date]\n    aggregations:\n      total_revenue: sum(amount)\n      order_count: count(*)\n      avg_order: avg(amount)\n\nsink:\n  type: file\n  path: s3://data-lake/processed/sales_summary/\n  format: parquet\n  options:\n    partition_by: [date, region]\n    mode: overwrite\n</code></pre>"},{"location":"user-guide/backends/spark/#limitations","title":"Limitations","text":"<ol> <li>Startup Overhead: 5-30 seconds for session initialization</li> <li>Small Data: Inefficient for datasets under 1GB</li> <li>Complexity: Requires cluster management</li> <li>Cost: Cloud clusters can be expensive</li> </ol>"},{"location":"user-guide/backends/spark/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/backends/spark/#java-not-found","title":"Java Not Found","text":"<pre><code>JAVA_HOME is not set\n</code></pre> <p>Solution: Install Java and set JAVA_HOME: <pre><code># macOS\nbrew install openjdk@17\nexport JAVA_HOME=/opt/homebrew/opt/openjdk@17\n\n# Ubuntu\nsudo apt install openjdk-17-jdk\nexport JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n</code></pre></p>"},{"location":"user-guide/backends/spark/#out-of-memory","title":"Out of Memory","text":"<pre><code>java.lang.OutOfMemoryError: Java heap space\n</code></pre> <p>Solution: Increase executor memory: <pre><code>export SPARK_EXECUTOR_MEMORY=8g\nexport SPARK_DRIVER_MEMORY=4g\n</code></pre></p>"},{"location":"user-guide/backends/spark/#s3-access-denied","title":"S3 Access Denied","text":"<p>Ensure AWS credentials are configured: <pre><code>export AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\n# Or use IAM role\n</code></pre></p>"},{"location":"user-guide/backends/spark/#related","title":"Related","text":"<ul> <li>Backend Selection - Choosing the right backend</li> <li>Cloud Storage - S3, GCS configuration</li> <li>Production Best Practices</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>ETLX pipelines can be configured using YAML files or the Python API. This section covers configuration in detail.</p>"},{"location":"user-guide/configuration/#overview","title":"Overview","text":"<ul> <li> <p> Pipeline YAML</p> <p>Complete YAML schema reference.</p> <p> Pipeline YAML</p> </li> <li> <p> Variable Substitution</p> <p>Dynamic configuration with variables.</p> <p> Variables</p> </li> <li> <p> JSON Schema</p> <p>IDE autocompletion and validation.</p> <p> JSON Schema</p> </li> </ul>"},{"location":"user-guide/configuration/#pipeline-structure","title":"Pipeline Structure","text":"<p>Every pipeline has this basic structure:</p> <pre><code>name: pipeline_name           # Required: Unique identifier\ndescription: What it does     # Optional: Human description\nengine: duckdb                # Optional: Compute backend (default: duckdb)\n\nsource:                       # Required: Where to read data\n  type: file\n  path: input.parquet\n\ntransforms:                   # Optional: List of transformations\n  - op: filter\n    predicate: amount &gt; 0\n\nchecks:                       # Optional: Quality validations\n  - type: not_null\n    columns: [id]\n\nsink:                         # Required: Where to write data\n  type: file\n  path: output.parquet\n</code></pre>"},{"location":"user-guide/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>ETLX validates configurations using Pydantic:</p> <ul> <li>Type checking - Correct types for all fields</li> <li>Required fields - Missing fields are reported</li> <li>Unknown fields - Extra fields cause errors</li> <li>Value constraints - Invalid values are rejected</li> </ul> <p>Validate without running:</p> <pre><code>etlx validate pipeline.yml\n</code></pre>"},{"location":"user-guide/configuration/#yaml-vs-python","title":"YAML vs Python","text":"Feature YAML Python Simplicity Simple, declarative More verbose Variables <code>${VAR}</code> syntax Dict or env Dynamic logic Limited Full Python Reusability Copy/paste Functions, classes Version control Easy diff Easy diff IDE support JSON Schema Type hints <p>Recommendation: Use YAML for most pipelines. Use Python when you need:</p> <ul> <li>Complex conditional logic</li> <li>Dynamic pipeline generation</li> <li>Integration with existing Python code</li> <li>Custom transforms or checks</li> </ul>"},{"location":"user-guide/configuration/json-schema/","title":"JSON Schema for IDEs","text":"<p>ETLX provides JSON Schema support for IDE autocompletion and validation.</p>"},{"location":"user-guide/configuration/json-schema/#generate-schema","title":"Generate Schema","text":"<p>Export the JSON schema:</p> <pre><code># To stdout\netlx schema\n\n# To file\netlx schema -o .etlx-schema.json\n\n# With custom indentation\netlx schema -o .etlx-schema.json --indent 4\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#vs-code-setup","title":"VS Code Setup","text":""},{"location":"user-guide/configuration/json-schema/#option-1-yaml-extension-settings","title":"Option 1: YAML Extension Settings","text":"<ol> <li>Install the YAML extension</li> <li>Generate the schema: <code>etlx schema -o .etlx-schema.json</code></li> <li>Add to <code>.vscode/settings.json</code>:</li> </ol> <pre><code>{\n  \"yaml.schemas\": {\n    \".etlx-schema.json\": [\"pipelines/*.yml\", \"pipelines/**/*.yml\"]\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#option-2-inline-schema-reference","title":"Option 2: Inline Schema Reference","text":"<p>Add a schema reference at the top of your YAML file:</p> <pre><code># yaml-language-server: $schema=.etlx-schema.json\nname: my_pipeline\nengine: duckdb\n...\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#option-3-project-wide-schema","title":"Option 3: Project-Wide Schema","text":"<p>Create <code>.vscode/settings.json</code> in your project:</p> <pre><code>{\n  \"yaml.schemas\": {\n    \"https://raw.githubusercontent.com/etlx/etlx/main/schema.json\": [\n      \"pipelines/*.yml\"\n    ]\n  },\n  \"yaml.customTags\": [\n    \"!include scalar\"\n  ]\n}\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#pycharm-intellij-setup","title":"PyCharm / IntelliJ Setup","text":"<ol> <li>Generate the schema: <code>etlx schema -o .etlx-schema.json</code></li> <li>Go to Settings \u2192 Languages &amp; Frameworks \u2192 Schemas and DTDs \u2192 JSON Schema Mappings</li> <li>Click + to add a new mapping:</li> <li>Name: ETLX Pipeline</li> <li>Schema file or URL: Select <code>.etlx-schema.json</code></li> <li>File path pattern: <code>pipelines/*.yml</code></li> </ol>"},{"location":"user-guide/configuration/json-schema/#features","title":"Features","text":"<p>With JSON Schema enabled, you get:</p>"},{"location":"user-guide/configuration/json-schema/#autocompletion","title":"Autocompletion","text":"<p>Type and see suggestions:</p> <pre><code>source:\n  type: f  # Suggests: file\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#validation","title":"Validation","text":"<p>Errors are highlighted:</p> <pre><code>source:\n  type: invalid  # Error: must be 'file' or 'database'\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#documentation","title":"Documentation","text":"<p>Hover over fields to see descriptions:</p> <pre><code>transforms:\n  - op: filter  # Hover shows: \"Filter rows using a SQL-like predicate\"\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#required-fields","title":"Required Fields","text":"<p>Missing required fields are highlighted:</p> <pre><code>source:\n  type: file\n  # Error: 'path' is required\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#schema-contents","title":"Schema Contents","text":"<p>The generated schema includes:</p> <ul> <li>All source types (<code>file</code>, <code>database</code>)</li> <li>All sink types (<code>file</code>, <code>database</code>)</li> <li>All 12 transform operations</li> <li>All 5 quality checks</li> <li>All configuration options</li> <li>Descriptions for every field</li> </ul> <p>Example schema structure:</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"ETLX Pipeline Configuration\",\n  \"type\": \"object\",\n  \"required\": [\"name\", \"source\", \"sink\"],\n  \"properties\": {\n    \"name\": {\n      \"type\": \"string\",\n      \"description\": \"Unique pipeline identifier\"\n    },\n    \"engine\": {\n      \"type\": \"string\",\n      \"enum\": [\"duckdb\", \"polars\", \"datafusion\", \"spark\", \"pandas\"],\n      \"default\": \"duckdb\"\n    },\n    \"transforms\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"oneOf\": [\n          { \"$ref\": \"#/definitions/SelectTransform\" },\n          { \"$ref\": \"#/definitions/FilterTransform\" },\n          ...\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#keeping-schema-updated","title":"Keeping Schema Updated","text":"<p>Regenerate the schema when ETLX is updated:</p> <pre><code># After upgrading ETLX\npip install --upgrade etlx\netlx schema -o .etlx-schema.json\n</code></pre> <p>Add to your <code>Makefile</code> or scripts:</p> <pre><code>.PHONY: schema\nschema:\n    etlx schema -o .etlx-schema.json\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#cicd-validation","title":"CI/CD Validation","text":"<p>Validate pipelines in CI:</p> .github/workflows/validate.yml<pre><code>name: Validate Pipelines\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install ETLX\n        run: pip install etlx\n\n      - name: Validate all pipelines\n        run: |\n          for f in pipelines/*.yml; do\n            echo \"Validating $f\"\n            etlx validate \"$f\"\n          done\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/json-schema/#schema-not-loading","title":"Schema Not Loading","text":"<ol> <li>Check the file path in settings matches your pipeline location</li> <li>Ensure the schema file exists and is valid JSON</li> <li>Restart your IDE after changing settings</li> </ol>"},{"location":"user-guide/configuration/json-schema/#outdated-completions","title":"Outdated Completions","text":"<p>Regenerate the schema after updating ETLX:</p> <pre><code>etlx schema -o .etlx-schema.json\n</code></pre>"},{"location":"user-guide/configuration/json-schema/#custom-transforms","title":"Custom Transforms","text":"<p>If you add custom transforms, they won't appear in the schema. The schema covers built-in operations only.</p>"},{"location":"user-guide/configuration/json-schema/#related","title":"Related","text":"<ul> <li>Pipeline YAML - Configuration reference</li> <li>CLI: schema - Schema command details</li> </ul>"},{"location":"user-guide/configuration/pipeline-yaml/","title":"Pipeline YAML Reference","text":"<p>Complete reference for YAML pipeline configuration.</p>"},{"location":"user-guide/configuration/pipeline-yaml/#full-schema","title":"Full Schema","text":"<pre><code># Pipeline metadata\nname: string                    # Required: Pipeline identifier\ndescription: string             # Optional: Human-readable description\nengine: string                  # Optional: Compute backend (default: \"duckdb\")\n                               # Options: duckdb, polars, datafusion, spark, pandas\n\n# Data source\nsource:\n  type: file | database         # Required: Source type\n\n  # For type: file\n  path: string                  # Required: File path or cloud URI\n  format: csv | parquet | json  # Optional: File format (default: parquet)\n  options: object               # Optional: Format-specific options\n\n  # For type: database\n  connection: string            # Required: Connection string\n  query: string                 # Optional: SQL query (mutually exclusive with table)\n  table: string                 # Optional: Table name (mutually exclusive with query)\n\n# Transformations (applied in order)\ntransforms:\n  - op: select | rename | filter | derive_column | cast | fill_null |\n        dedup | sort | join | aggregate | union | limit\n    # ... operation-specific fields\n\n# Quality checks (run after transforms)\nchecks:\n  - type: not_null | unique | row_count | accepted_values | expression\n    # ... check-specific fields\n\n# Data sink\nsink:\n  type: file | database         # Required: Sink type\n\n  # For type: file\n  path: string                  # Required: Output path\n  format: parquet | csv         # Optional: Output format (default: parquet)\n  partition_by: [string]        # Optional: Partition columns\n  mode: overwrite | append      # Optional: Write mode (default: overwrite)\n\n  # For type: database\n  connection: string            # Required: Connection string\n  table: string                 # Required: Target table\n  mode: append | truncate       # Optional: Write mode (default: append)\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#source-configuration","title":"Source Configuration","text":""},{"location":"user-guide/configuration/pipeline-yaml/#file-source","title":"File Source","text":"<p>Read from local files or cloud storage:</p> <pre><code>source:\n  type: file\n  path: data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#csv-options","title":"CSV Options","text":"<pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n  options:\n    delimiter: \",\"              # Field delimiter\n    header: true                # First row is header\n    skip_rows: 0                # Rows to skip\n    null_values: [\"\", \"NULL\"]   # Values to treat as null\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#cloud-storage-paths","title":"Cloud Storage Paths","text":"<pre><code># Amazon S3\nsource:\n  type: file\n  path: s3://my-bucket/data/sales.parquet\n\n# Google Cloud Storage\nsource:\n  type: file\n  path: gs://my-bucket/data/sales.parquet\n\n# Azure ADLS\nsource:\n  type: file\n  path: abfs://container@account.dfs.core.windows.net/data/sales.parquet\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#database-source","title":"Database Source","text":"<p>Read from databases:</p> <pre><code># Using a query\nsource:\n  type: database\n  connection: postgresql://user:pass@localhost:5432/db\n  query: SELECT * FROM sales WHERE date &gt; '2025-01-01'\n\n# Using a table\nsource:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#transform-configuration","title":"Transform Configuration","text":"<p>Each transform has an <code>op</code> field that determines its type:</p>"},{"location":"user-guide/configuration/pipeline-yaml/#select","title":"select","text":"<pre><code>- op: select\n  columns: [id, name, amount]   # Columns to keep (in order)\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#rename","title":"rename","text":"<pre><code>- op: rename\n  mapping:                      # Old name -&gt; new name\n    old_column: new_column\n    another_old: another_new\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#filter","title":"filter","text":"<pre><code>- op: filter\n  predicate: amount &gt; 100 AND status = 'active'\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#derive_column","title":"derive_column","text":"<pre><code>- op: derive_column\n  name: total_with_tax          # New column name\n  expr: amount * 1.1            # Expression\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#cast","title":"cast","text":"<pre><code>- op: cast\n  columns:\n    id: string\n    amount: float64\n    created_at: datetime\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#fill_null","title":"fill_null","text":"<pre><code>- op: fill_null\n  columns:\n    amount: 0\n    status: \"unknown\"\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#dedup","title":"dedup","text":"<pre><code>- op: dedup\n  columns: [customer_id, order_id]  # Optional: specific columns\n                                    # If omitted, uses all columns\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#sort","title":"sort","text":"<pre><code>- op: sort\n  by: [amount, created_at]\n  descending: true              # Optional: default false\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#join","title":"join","text":"<pre><code>- op: join\n  right: other_dataset          # Reference to another dataset\n  on: [customer_id]             # Join columns\n  how: left                     # Optional: inner, left, right, outer\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#aggregate","title":"aggregate","text":"<pre><code>- op: aggregate\n  group_by: [category, region]\n  aggs:\n    total_sales: sum(amount)\n    avg_order: avg(amount)\n    order_count: count(*)\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#union","title":"union","text":"<pre><code>- op: union\n  sources: [dataset1, dataset2]  # References to datasets\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#limit","title":"limit","text":"<pre><code>- op: limit\n  n: 1000                       # Maximum rows\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#check-configuration","title":"Check Configuration","text":""},{"location":"user-guide/configuration/pipeline-yaml/#not_null","title":"not_null","text":"<pre><code>- type: not_null\n  columns: [id, name, amount]\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#unique","title":"unique","text":"<pre><code>- type: unique\n  columns: [id]                 # Single column\n  # OR\n  columns: [customer_id, order_id]  # Composite unique\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#row_count","title":"row_count","text":"<pre><code>- type: row_count\n  min: 1                        # Optional: minimum rows\n  max: 1000000                  # Optional: maximum rows\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#accepted_values","title":"accepted_values","text":"<pre><code>- type: accepted_values\n  column: status\n  values: [pending, active, completed, cancelled]\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#expression","title":"expression","text":"<pre><code>- type: expression\n  expr: amount &gt;= 0             # Must be true for all rows\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#sink-configuration","title":"Sink Configuration","text":""},{"location":"user-guide/configuration/pipeline-yaml/#file-sink","title":"File Sink","text":"<pre><code>sink:\n  type: file\n  path: output/sales.parquet\n  format: parquet\n  mode: overwrite\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#partitioned-output","title":"Partitioned Output","text":"<pre><code>sink:\n  type: file\n  path: output/sales/\n  format: parquet\n  partition_by: [year, month]\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#database-sink","title":"Database Sink","text":"<pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales_summary\n  mode: truncate\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#complete-example","title":"Complete Example","text":"<pre><code>name: daily_sales_etl\ndescription: Daily sales aggregation pipeline\nengine: duckdb\n\nsource:\n  type: file\n  path: s3://data-lake/raw/sales/${DATE}/*.parquet\n  format: parquet\n\ntransforms:\n  # Clean data\n  - op: filter\n    predicate: amount &gt; 0 AND status != 'cancelled'\n\n  # Standardize columns\n  - op: rename\n    mapping:\n      order_total: amount\n      cust_id: customer_id\n\n  # Add metrics\n  - op: derive_column\n    name: net_amount\n    expr: amount - COALESCE(discount, 0)\n\n  # Aggregate by region\n  - op: aggregate\n    group_by: [region, category]\n    aggs:\n      total_sales: sum(net_amount)\n      order_count: count(*)\n      avg_order: avg(net_amount)\n\n  # Sort for reporting\n  - op: sort\n    by: [total_sales]\n    descending: true\n\nchecks:\n  - type: not_null\n    columns: [region, category, total_sales]\n  - type: row_count\n    min: 1\n  - type: expression\n    expr: total_sales &gt;= 0\n\nsink:\n  type: file\n  path: s3://data-lake/processed/sales_summary/${DATE}/\n  format: parquet\n  partition_by: [region]\n</code></pre>"},{"location":"user-guide/configuration/pipeline-yaml/#related","title":"Related","text":"<ul> <li>Variable Substitution - Use <code>${VAR}</code> syntax</li> <li>JSON Schema - IDE support</li> <li>Transforms Reference - Detailed transform docs</li> </ul>"},{"location":"user-guide/configuration/variables/","title":"Variable Substitution","text":"<p>ETLX supports variable substitution in YAML configurations, allowing dynamic values at runtime.</p>"},{"location":"user-guide/configuration/variables/#basic-syntax","title":"Basic Syntax","text":"<p>Reference variables using <code>${VAR_NAME}</code>:</p> <pre><code>source:\n  type: file\n  path: data/${DATE}/sales.parquet\n</code></pre>"},{"location":"user-guide/configuration/variables/#default-values","title":"Default Values","text":"<p>Provide defaults with <code>${VAR_NAME:-default}</code>:</p> <pre><code>engine: ${ENGINE:-duckdb}\n\nsource:\n  type: file\n  path: ${INPUT_PATH:-data/sales.parquet}\n</code></pre>"},{"location":"user-guide/configuration/variables/#setting-variables","title":"Setting Variables","text":""},{"location":"user-guide/configuration/variables/#cli-flag","title":"CLI Flag","text":"<p>Pass variables with <code>--var</code>:</p> <pre><code>etlx run pipeline.yml --var DATE=2025-01-15\netlx run pipeline.yml --var DATE=2025-01-15 --var REGION=north\n</code></pre>"},{"location":"user-guide/configuration/variables/#environment-variables","title":"Environment Variables","text":"<p>Variables are resolved from the environment:</p> <pre><code>export DATE=2025-01-15\nexport DATABASE_URL=postgresql://localhost/db\n\netlx run pipeline.yml\n</code></pre>"},{"location":"user-guide/configuration/variables/#python-api","title":"Python API","text":"<p>Pass variables to <code>Pipeline.from_yaml()</code> or <code>run()</code>:</p> <pre><code># At load time\npipeline = Pipeline.from_yaml(\n    \"pipeline.yml\",\n    variables={\"DATE\": \"2025-01-15\"}\n)\n\n# At run time\nresult = pipeline.run(variables={\"DATE\": \"2025-01-15\"})\n</code></pre>"},{"location":"user-guide/configuration/variables/#resolution-order","title":"Resolution Order","text":"<p>Variables are resolved in this order:</p> <ol> <li>Explicit variables - <code>--var</code> flag or <code>variables</code> parameter</li> <li>Environment variables - <code>os.environ</code></li> <li>Default value - From <code>${VAR:-default}</code> syntax</li> <li>Error - If no value found and no default</li> </ol> <pre><code># Uses --var DATE if provided\n# Falls back to $DATE environment variable\n# Falls back to \"2025-01-01\" if neither exists\npath: data/${DATE:-2025-01-01}/sales.parquet\n</code></pre>"},{"location":"user-guide/configuration/variables/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/configuration/variables/#date-based-paths","title":"Date-Based Paths","text":"<pre><code>source:\n  type: file\n  path: s3://bucket/data/${YEAR}/${MONTH}/${DAY}/\n\nsink:\n  type: file\n  path: s3://bucket/output/${RUN_DATE}/\n</code></pre> <pre><code>etlx run pipeline.yml --var YEAR=2025 --var MONTH=01 --var DAY=15\n# OR\netlx run pipeline.yml --var RUN_DATE=2025-01-15\n</code></pre>"},{"location":"user-guide/configuration/variables/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n\nsink:\n  type: file\n  path: ${OUTPUT_BUCKET}/results/\n</code></pre> <pre><code># Development\nexport DATABASE_URL=postgresql://localhost/dev\nexport OUTPUT_BUCKET=./data/output\n\n# Production\nexport DATABASE_URL=postgresql://prod-server/db\nexport OUTPUT_BUCKET=s3://prod-bucket\n</code></pre>"},{"location":"user-guide/configuration/variables/#dynamic-filtering","title":"Dynamic Filtering","text":"<pre><code>transforms:\n  - op: filter\n    predicate: region = '${REGION}'\n\n  - op: filter\n    predicate: date &gt;= '${START_DATE}' AND date &lt;= '${END_DATE}'\n</code></pre> <pre><code>etlx run pipeline.yml \\\n  --var REGION=north \\\n  --var START_DATE=2025-01-01 \\\n  --var END_DATE=2025-01-31\n</code></pre>"},{"location":"user-guide/configuration/variables/#optional-features","title":"Optional Features","text":"<pre><code># Only aggregate if GROUP_BY is set\ntransforms:\n  - op: aggregate\n    group_by: [${GROUP_BY:-category}]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/configuration/variables/#airflow-integration","title":"Airflow Integration","text":"<p>When using the <code>@etlx_task</code> decorator, return variables from your task:</p> <pre><code>from etlx.integrations.airflow import etlx_task\n\n@etlx_task(config_path=\"pipelines/daily.yml\")\ndef run_daily_pipeline(**context):\n    return {\n        \"DATE\": context[\"ds\"],                    # Airflow execution date\n        \"BUCKET\": context[\"params\"].get(\"bucket\", \"default-bucket\"),\n    }\n</code></pre> <p>Access Airflow variables:</p> <pre><code>source:\n  type: file\n  path: s3://${BUCKET}/data/${DATE}/\n</code></pre>"},{"location":"user-guide/configuration/variables/#the-env-file","title":"The .env File","text":"<p>Store variables in <code>.env</code> for local development:</p> .env<pre><code>DATABASE_URL=postgresql://localhost/dev\nOUTPUT_PATH=./data/output\nDEFAULT_ENGINE=duckdb\n</code></pre> <p>Security</p> <p>Never commit <code>.env</code> files with secrets. Add <code>.env</code> to <code>.gitignore</code>.</p>"},{"location":"user-guide/configuration/variables/#escaping","title":"Escaping","text":"<p>To use a literal <code>${</code>, escape with <code>$${</code>:</p> <pre><code># Outputs: The variable syntax is ${VAR}\ndescription: The variable syntax is $${VAR}\n</code></pre>"},{"location":"user-guide/configuration/variables/#validation","title":"Validation","text":"<p>Unresolved required variables cause validation errors:</p> <pre><code>$ etlx run pipeline.yml\nError: Variable 'DATABASE_URL' is not set and has no default value\n</code></pre> <p>Use <code>etlx validate</code> to check variables:</p> <pre><code>etlx validate pipeline.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"user-guide/configuration/variables/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/variables/#1-use-descriptive-names","title":"1. Use Descriptive Names","text":"<pre><code># Good\npath: ${INPUT_SALES_PATH}\npath: ${S3_OUTPUT_BUCKET}\n\n# Bad\npath: ${PATH}\npath: ${P1}\n</code></pre>"},{"location":"user-guide/configuration/variables/#2-provide-sensible-defaults","title":"2. Provide Sensible Defaults","text":"<pre><code># Good - works out of the box for development\nengine: ${ENGINE:-duckdb}\npath: ${INPUT:-data/sample.parquet}\n\n# Less good - requires variables to be set\npath: ${INPUT}\n</code></pre>"},{"location":"user-guide/configuration/variables/#3-document-required-variables","title":"3. Document Required Variables","text":"<pre><code># Required variables:\n#   DATABASE_URL: PostgreSQL connection string\n#   DATE: Run date in YYYY-MM-DD format\n#\n# Optional variables:\n#   ENGINE: Compute backend (default: duckdb)\n#   OUTPUT_PATH: Output location (default: ./output/)\n\nname: documented_pipeline\n</code></pre>"},{"location":"user-guide/configuration/variables/#4-use-envexample","title":"4. Use .env.example","text":"<p>Create a template for required variables:</p> .env.example<pre><code># Copy this file to .env and fill in values\n\n# Required\nDATABASE_URL=postgresql://user:pass@host:5432/db\n\n# Optional\nENGINE=duckdb\nOUTPUT_PATH=./output/\n</code></pre>"},{"location":"user-guide/configuration/variables/#related","title":"Related","text":"<ul> <li>Pipeline YAML - Full YAML reference</li> <li>Airflow Integration - Using with Airflow</li> <li>Project Structure - Organizing variables</li> </ul>"},{"location":"user-guide/io/","title":"Sources &amp; Sinks","text":"<p>ETLX reads data from sources and writes data to sinks. This section covers all supported source and sink types.</p>"},{"location":"user-guide/io/#overview","title":"Overview","text":"<ul> <li> <p> File Sources</p> <p>Read CSV, Parquet, and JSON files.</p> <p> File Sources</p> </li> <li> <p> Database Sources</p> <p>Read from PostgreSQL, MySQL, and other databases.</p> <p> Database Sources</p> </li> <li> <p> Cloud Storage</p> <p>Read from S3, GCS, and Azure.</p> <p> Cloud Storage</p> </li> <li> <p> File Sinks</p> <p>Write Parquet and CSV files.</p> <p> File Sinks</p> </li> <li> <p> Database Sinks</p> <p>Write to databases.</p> <p> Database Sinks</p> </li> </ul>"},{"location":"user-guide/io/#source-types","title":"Source Types","text":"Type Description Formats <code>file</code> Local or cloud files CSV, Parquet, JSON <code>database</code> Relational databases SQL query or table"},{"location":"user-guide/io/#sink-types","title":"Sink Types","text":"Type Description Formats <code>file</code> Local or cloud files Parquet, CSV <code>database</code> Relational databases Table writes"},{"location":"user-guide/io/#quick-reference","title":"Quick Reference","text":""},{"location":"user-guide/io/#file-source","title":"File Source","text":"<pre><code>source:\n  type: file\n  path: data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/#database-source","title":"Database Source","text":"<pre><code>source:\n  type: database\n  connection: postgresql://localhost/db\n  table: sales\n</code></pre>"},{"location":"user-guide/io/#file-sink","title":"File Sink","text":"<pre><code>sink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/#database-sink","title":"Database Sink","text":"<pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: results\n  mode: truncate\n</code></pre>"},{"location":"user-guide/io/#cloud-storage","title":"Cloud Storage","text":"<p>All file sources and sinks support cloud URIs:</p> Provider URI Format AWS S3 <code>s3://bucket/path/file.parquet</code> Google Cloud <code>gs://bucket/path/file.parquet</code> Azure ADLS <code>abfs://container@account.dfs.core.windows.net/path/file.parquet</code> <p>See Cloud Storage for authentication setup.</p>"},{"location":"user-guide/io/cloud-storage/","title":"Cloud Storage","text":"<p>Read and write files from AWS S3, Google Cloud Storage, and Azure ADLS.</p>"},{"location":"user-guide/io/cloud-storage/#overview","title":"Overview","text":"<p>ETLX supports cloud storage through fsspec, providing a unified interface for all cloud providers.</p>"},{"location":"user-guide/io/cloud-storage/#aws-s3","title":"AWS S3","text":""},{"location":"user-guide/io/cloud-storage/#installation","title":"Installation","text":"<pre><code>pip install etlx[aws]\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#configuration","title":"Configuration","text":"<pre><code>source:\n  type: file\n  path: s3://my-bucket/data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#authentication","title":"Authentication","text":""},{"location":"user-guide/io/cloud-storage/#environment-variables-recommended","title":"Environment Variables (Recommended)","text":"<pre><code>export AWS_ACCESS_KEY_ID=AKIA...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#aws-profile","title":"AWS Profile","text":"<pre><code>export AWS_PROFILE=my-profile\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#iam-role","title":"IAM Role","text":"<p>When running on AWS (EC2, ECS, Lambda), IAM roles are automatically used.</p>"},{"location":"user-guide/io/cloud-storage/#examples","title":"Examples","text":"<p>Read from S3:</p> <pre><code>source:\n  type: file\n  path: s3://data-lake/raw/sales/2025/01/15/data.parquet\n</code></pre> <p>Write to S3:</p> <pre><code>sink:\n  type: file\n  path: s3://data-lake/processed/sales/\n  format: parquet\n</code></pre> <p>With variables:</p> <pre><code>source:\n  type: file\n  path: s3://${BUCKET}/data/${DATE}/sales.parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#google-cloud-storage","title":"Google Cloud Storage","text":""},{"location":"user-guide/io/cloud-storage/#installation_1","title":"Installation","text":"<pre><code>pip install etlx[gcp]\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#configuration_1","title":"Configuration","text":"<pre><code>source:\n  type: file\n  path: gs://my-bucket/data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#authentication_1","title":"Authentication","text":""},{"location":"user-guide/io/cloud-storage/#service-account-key","title":"Service Account Key","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#application-default-credentials","title":"Application Default Credentials","text":"<pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#examples_1","title":"Examples","text":"<p>Read from GCS:</p> <pre><code>source:\n  type: file\n  path: gs://data-lake/raw/sales.parquet\n</code></pre> <p>Write to GCS:</p> <pre><code>sink:\n  type: file\n  path: gs://data-lake/processed/\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#azure-adls","title":"Azure ADLS","text":""},{"location":"user-guide/io/cloud-storage/#installation_2","title":"Installation","text":"<pre><code>pip install etlx[azure]\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#configuration_2","title":"Configuration","text":"<pre><code>source:\n  type: file\n  path: abfs://container@account.dfs.core.windows.net/data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#authentication_2","title":"Authentication","text":""},{"location":"user-guide/io/cloud-storage/#connection-string","title":"Connection String","text":"<pre><code>export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;...\"\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#account-key","title":"Account Key","text":"<pre><code>export AZURE_STORAGE_ACCOUNT_NAME=myaccount\nexport AZURE_STORAGE_ACCOUNT_KEY=...\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#service-principal","title":"Service Principal","text":"<pre><code>export AZURE_TENANT_ID=...\nexport AZURE_CLIENT_ID=...\nexport AZURE_CLIENT_SECRET=...\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#examples_2","title":"Examples","text":"<p>Read from Azure:</p> <pre><code>source:\n  type: file\n  path: abfs://datalake@myaccount.dfs.core.windows.net/raw/sales.parquet\n</code></pre> <p>Write to Azure:</p> <pre><code>sink:\n  type: file\n  path: abfs://datalake@myaccount.dfs.core.windows.net/processed/\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#uri-formats","title":"URI Formats","text":"Provider Format AWS S3 <code>s3://bucket/path/file.parquet</code> GCS <code>gs://bucket/path/file.parquet</code> Azure ADLS Gen2 <code>abfs://container@account.dfs.core.windows.net/path/file.parquet</code> Azure Blob <code>az://container/path/file.parquet</code>"},{"location":"user-guide/io/cloud-storage/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/io/cloud-storage/#date-partitioned-data","title":"Date-Partitioned Data","text":"<pre><code>source:\n  type: file\n  path: s3://bucket/data/year=${YEAR}/month=${MONTH}/day=${DAY}/\n\nsink:\n  type: file\n  path: s3://bucket/output/${DATE}/\n  partition_by: [region]\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#cross-cloud-transfer","title":"Cross-Cloud Transfer","text":"<p>Read from one provider, write to another:</p> <pre><code>source:\n  type: file\n  path: s3://source-bucket/data.parquet\n\nsink:\n  type: file\n  path: gs://dest-bucket/data.parquet\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#environment-specific-buckets","title":"Environment-Specific Buckets","text":"<pre><code>source:\n  type: file\n  path: ${DATA_BUCKET}/raw/sales.parquet\n\nsink:\n  type: file\n  path: ${OUTPUT_BUCKET}/processed/\n</code></pre> <pre><code># Development\nexport DATA_BUCKET=s3://dev-data\nexport OUTPUT_BUCKET=s3://dev-output\n\n# Production\nexport DATA_BUCKET=s3://prod-data\nexport OUTPUT_BUCKET=s3://prod-output\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/io/cloud-storage/#use-parquet","title":"Use Parquet","text":"<p>Parquet files are faster to read from cloud storage due to:</p> <ul> <li>Columnar format (read only needed columns)</li> <li>Built-in compression</li> <li>Predicate pushdown support</li> </ul>"},{"location":"user-guide/io/cloud-storage/#regional-proximity","title":"Regional Proximity","text":"<p>Place compute near your data:</p> <ul> <li>Use same region for storage and compute</li> <li>Consider multi-region buckets for global access</li> </ul>"},{"location":"user-guide/io/cloud-storage/#compression","title":"Compression","text":"<p>Parquet is already compressed. For CSV:</p> <pre><code>source:\n  type: file\n  path: s3://bucket/data.csv.gz\n  format: csv\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/io/cloud-storage/#access-denied","title":"Access Denied","text":"<pre><code>Error: Access Denied\n</code></pre> <ul> <li>Verify credentials are set correctly</li> <li>Check bucket/object permissions</li> <li>Ensure IAM role has required permissions</li> </ul>"},{"location":"user-guide/io/cloud-storage/#bucket-not-found","title":"Bucket Not Found","text":"<pre><code>Error: Bucket not found\n</code></pre> <ul> <li>Check bucket name spelling</li> <li>Verify bucket exists in the expected region</li> <li>Check credentials have access to the bucket</li> </ul>"},{"location":"user-guide/io/cloud-storage/#slow-performance","title":"Slow Performance","text":"<ul> <li>Check network connectivity</li> <li>Verify data is in the same region as compute</li> <li>Consider using larger instance types</li> <li>Use Parquet instead of CSV</li> </ul>"},{"location":"user-guide/io/cloud-storage/#missing-credentials","title":"Missing Credentials","text":"<pre><code>Error: No credentials found\n</code></pre> <ul> <li>Set environment variables</li> <li>Configure AWS profile/GCP service account/Azure credentials</li> <li>When running locally, ensure credentials file exists</li> </ul>"},{"location":"user-guide/io/cloud-storage/#security-best-practices","title":"Security Best Practices","text":""},{"location":"user-guide/io/cloud-storage/#use-iam-roles","title":"Use IAM Roles","text":"<p>Prefer IAM roles over access keys:</p> <pre><code># Running on AWS EC2/ECS with IAM role\nsource:\n  type: file\n  path: s3://bucket/data.parquet\n  # No credentials needed - uses instance role\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#dont-commit-credentials","title":"Don't Commit Credentials","text":"<p>Add to <code>.gitignore</code>:</p> <pre><code>.env\n*.json  # Service account keys\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#use-secrets-managers","title":"Use Secrets Managers","text":"<p>For production, use secrets managers:</p> <ul> <li>AWS Secrets Manager</li> <li>Google Secret Manager</li> <li>Azure Key Vault</li> </ul>"},{"location":"user-guide/io/cloud-storage/#least-privilege","title":"Least Privilege","text":"<p>Grant minimal permissions:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"s3:GetObject\",\n    \"s3:PutObject\"\n  ],\n  \"Resource\": [\n    \"arn:aws:s3:::my-bucket/data/*\"\n  ]\n}\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#related","title":"Related","text":"<ul> <li>File Sources - File format options</li> <li>File Sinks - Writing files</li> <li>Environment Variables - Credential configuration</li> </ul>"},{"location":"user-guide/io/database-sinks/","title":"Database Sinks","text":"<p>Write transformed data to relational databases.</p>"},{"location":"user-guide/io/database-sinks/#basic-usage","title":"Basic Usage","text":"<pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales_summary\n</code></pre>"},{"location":"user-guide/io/database-sinks/#configuration","title":"Configuration","text":"Field Required Default Description <code>type</code> Yes - Must be <code>database</code> <code>connection</code> Yes - Database connection string <code>table</code> Yes - Target table name <code>mode</code> No <code>append</code> Write mode: <code>append</code>, <code>truncate</code>"},{"location":"user-guide/io/database-sinks/#write-modes","title":"Write Modes","text":""},{"location":"user-guide/io/database-sinks/#append-default","title":"Append (Default)","text":"<p>Add new rows to existing data:</p> <pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales_summary\n  mode: append\n</code></pre>"},{"location":"user-guide/io/database-sinks/#truncate","title":"Truncate","text":"<p>Delete existing data before writing:</p> <pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales_summary\n  mode: truncate\n</code></pre> <p>Truncate is Destructive</p> <p>Truncate deletes all existing data in the table before inserting new data.</p>"},{"location":"user-guide/io/database-sinks/#connection-strings","title":"Connection Strings","text":""},{"location":"user-guide/io/database-sinks/#postgresql","title":"PostgreSQL","text":"<pre><code>sink:\n  type: database\n  connection: postgresql://user:password@host:5432/database\n  table: results\n</code></pre>"},{"location":"user-guide/io/database-sinks/#mysql","title":"MySQL","text":"<pre><code>sink:\n  type: database\n  connection: mysql://user:password@host:3306/database\n  table: results\n</code></pre>"},{"location":"user-guide/io/database-sinks/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>sink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: ${TARGET_TABLE}\n</code></pre> <pre><code>export DATABASE_URL=postgresql://user:pass@localhost/db\nexport TARGET_TABLE=sales_summary\n</code></pre>"},{"location":"user-guide/io/database-sinks/#examples","title":"Examples","text":""},{"location":"user-guide/io/database-sinks/#daily-summary-table","title":"Daily Summary Table","text":"<pre><code>name: daily_sales_summary\nengine: duckdb\n\nsource:\n  type: file\n  path: s3://data-lake/raw/sales/${DATE}/*.parquet\n\ntransforms:\n  - op: aggregate\n    group_by: [region, category]\n    aggs:\n      total_sales: sum(amount)\n      order_count: count(*)\n\nsink:\n  type: database\n  connection: ${POSTGRES_URL}\n  table: daily_summaries\n  mode: append\n</code></pre>"},{"location":"user-guide/io/database-sinks/#incremental-load","title":"Incremental Load","text":"<pre><code>name: incremental_load\nengine: duckdb\n\nsource:\n  type: database\n  connection: ${SOURCE_DB}\n  query: |\n    SELECT * FROM orders\n    WHERE updated_at &gt; '${LAST_RUN}'\n\ntransforms:\n  - op: select\n    columns: [id, customer_id, amount, status, updated_at]\n\nsink:\n  type: database\n  connection: ${TARGET_DB}\n  table: orders_replica\n  mode: append\n</code></pre>"},{"location":"user-guide/io/database-sinks/#full-refresh","title":"Full Refresh","text":"<pre><code>name: full_refresh\nengine: duckdb\n\nsource:\n  type: file\n  path: data/products.csv\n  format: csv\n\ntransforms:\n  - op: filter\n    predicate: active = true\n\nsink:\n  type: database\n  connection: ${DATABASE_URL}\n  table: active_products\n  mode: truncate\n</code></pre>"},{"location":"user-guide/io/database-sinks/#table-requirements","title":"Table Requirements","text":""},{"location":"user-guide/io/database-sinks/#table-must-exist","title":"Table Must Exist","text":"<p>ETLX does not create tables automatically. Create the table first:</p> <pre><code>CREATE TABLE sales_summary (\n    region VARCHAR(50),\n    category VARCHAR(50),\n    total_sales DECIMAL(12,2),\n    order_count INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"user-guide/io/database-sinks/#column-matching","title":"Column Matching","text":"<p>Output columns must match table columns:</p> <pre><code>transforms:\n  # Ensure output matches table schema\n  - op: select\n    columns: [region, category, total_sales, order_count]\n</code></pre>"},{"location":"user-guide/io/database-sinks/#data-types","title":"Data Types","text":"<p>ETLX attempts to convert types automatically. For best results:</p> <ul> <li>Use compatible types</li> <li>Cast explicitly if needed:</li> </ul> <pre><code>transforms:\n  - op: cast\n    columns:\n      total_sales: float64\n      order_count: int64\n</code></pre>"},{"location":"user-guide/io/database-sinks/#python-api","title":"Python API","text":"<pre><code>from etlx.config.models import DatabaseSink\n\n# Basic\nsink = DatabaseSink(\n    connection=\"postgresql://localhost/db\",\n    table=\"results\"\n)\n\n# With truncate\nsink = DatabaseSink(\n    connection=\"${DATABASE_URL}\",\n    table=\"sales_summary\",\n    mode=\"truncate\"\n)\n</code></pre>"},{"location":"user-guide/io/database-sinks/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/io/database-sinks/#batch-size","title":"Batch Size","text":"<p>For large datasets, writes are batched automatically. Performance varies by database.</p>"},{"location":"user-guide/io/database-sinks/#indexes","title":"Indexes","text":"<p>Disable indexes before large inserts, re-enable after:</p> <pre><code>-- Before pipeline\nALTER INDEX idx_sales_date DISABLE;\n\n-- After pipeline\nALTER INDEX idx_sales_date REBUILD;\n</code></pre>"},{"location":"user-guide/io/database-sinks/#truncate-vs-delete","title":"Truncate vs Delete","text":"<p><code>truncate</code> is faster than deleting all rows:</p> <pre><code>mode: truncate  # Fast - drops and recreates\n</code></pre>"},{"location":"user-guide/io/database-sinks/#connection-pooling","title":"Connection Pooling","text":"<p>For high-frequency pipelines, consider connection pooling at the database level.</p>"},{"location":"user-guide/io/database-sinks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/io/database-sinks/#table-not-found","title":"Table Not Found","text":"<pre><code>Error: Table 'results' does not exist\n</code></pre> <p>Create the table before running the pipeline.</p>"},{"location":"user-guide/io/database-sinks/#column-mismatch","title":"Column Mismatch","text":"<pre><code>Error: Column 'extra_col' does not exist in table\n</code></pre> <p>Ensure your output columns match the table schema:</p> <pre><code>transforms:\n  - op: select\n    columns: [col1, col2, col3]  # Only columns in target table\n</code></pre>"},{"location":"user-guide/io/database-sinks/#type-mismatch","title":"Type Mismatch","text":"<pre><code>Error: Cannot convert 'abc' to integer\n</code></pre> <p>Cast columns to correct types:</p> <pre><code>transforms:\n  - op: cast\n    columns:\n      amount: float64\n</code></pre>"},{"location":"user-guide/io/database-sinks/#permission-denied","title":"Permission Denied","text":"<pre><code>Error: Permission denied for table 'results'\n</code></pre> <p>Verify the database user has INSERT (and TRUNCATE if using truncate mode) permissions.</p>"},{"location":"user-guide/io/database-sinks/#limitations","title":"Limitations","text":""},{"location":"user-guide/io/database-sinks/#no-upsert-yet","title":"No Upsert (Yet)","text":"<p>Upsert/merge operations are planned for a future release. Currently, use:</p> <ul> <li><code>append</code> for incremental loads</li> <li><code>truncate</code> for full refreshes</li> </ul>"},{"location":"user-guide/io/database-sinks/#no-schema-management","title":"No Schema Management","text":"<p>ETLX does not create or modify table schemas. Manage schemas separately.</p>"},{"location":"user-guide/io/database-sinks/#related","title":"Related","text":"<ul> <li>Database Sources - Reading from databases</li> <li>File Sinks - Alternative: write to files</li> <li>Backends - Database backend details</li> </ul>"},{"location":"user-guide/io/database-sources/","title":"Database Sources","text":"<p>Read data from relational databases using SQL queries or table references.</p>"},{"location":"user-guide/io/database-sources/#basic-usage","title":"Basic Usage","text":"<pre><code>source:\n  type: database\n  connection: postgresql://user:pass@localhost:5432/mydb\n  table: sales\n</code></pre>"},{"location":"user-guide/io/database-sources/#configuration","title":"Configuration","text":"Field Required Default Description <code>type</code> Yes - Must be <code>database</code> <code>connection</code> Yes - Database connection string <code>table</code> No* - Table name to read <code>query</code> No* - SQL query to execute <p>*Either <code>table</code> or <code>query</code> is required, but not both.</p>"},{"location":"user-guide/io/database-sources/#connection-strings","title":"Connection Strings","text":""},{"location":"user-guide/io/database-sources/#postgresql","title":"PostgreSQL","text":"<pre><code>source:\n  type: database\n  connection: postgresql://user:password@host:5432/database\n  table: sales\n</code></pre>"},{"location":"user-guide/io/database-sources/#mysql","title":"MySQL","text":"<pre><code>source:\n  type: database\n  connection: mysql://user:password@host:3306/database\n  table: sales\n</code></pre>"},{"location":"user-guide/io/database-sources/#sqlite","title":"SQLite","text":"<pre><code>source:\n  type: database\n  connection: sqlite:///path/to/database.db\n  table: sales\n</code></pre>"},{"location":"user-guide/io/database-sources/#using-environment-variables","title":"Using Environment Variables","text":"<p>Store connection strings securely:</p> <pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales\n</code></pre> <pre><code>export DATABASE_URL=postgresql://user:pass@localhost/db\n</code></pre>"},{"location":"user-guide/io/database-sources/#reading-tables","title":"Reading Tables","text":"<p>Read an entire table:</p> <pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n  table: sales\n</code></pre> <p>This is equivalent to <code>SELECT * FROM sales</code>.</p>"},{"location":"user-guide/io/database-sources/#using-queries","title":"Using Queries","text":"<p>For more control, use a SQL query:</p> <pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n  query: |\n    SELECT\n      id,\n      customer_id,\n      amount,\n      created_at\n    FROM sales\n    WHERE created_at &gt;= '2025-01-01'\n      AND status = 'completed'\n</code></pre>"},{"location":"user-guide/io/database-sources/#complex-queries","title":"Complex Queries","text":"<pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n  query: |\n    SELECT\n      s.id,\n      s.amount,\n      c.name as customer_name,\n      c.tier as customer_tier\n    FROM sales s\n    JOIN customers c ON s.customer_id = c.id\n    WHERE s.created_at &gt;= '2025-01-01'\n</code></pre>"},{"location":"user-guide/io/database-sources/#parameterized-queries","title":"Parameterized Queries","text":"<p>Use variables in queries:</p> <pre><code>source:\n  type: database\n  connection: ${DATABASE_URL}\n  query: |\n    SELECT *\n    FROM sales\n    WHERE created_at &gt;= '${START_DATE}'\n      AND created_at &lt; '${END_DATE}'\n      AND region = '${REGION}'\n</code></pre> <pre><code>etlx run pipeline.yml \\\n  --var START_DATE=2025-01-01 \\\n  --var END_DATE=2025-02-01 \\\n  --var REGION=north\n</code></pre>"},{"location":"user-guide/io/database-sources/#supported-databases","title":"Supported Databases","text":"Database Connection Prefix Install Extra PostgreSQL <code>postgresql://</code> <code>etlx[postgres]</code> MySQL <code>mysql://</code> <code>etlx[mysql]</code> SQLite <code>sqlite:///</code> Built-in ClickHouse <code>clickhouse://</code> <code>etlx[clickhouse]</code> Snowflake See below <code>etlx[snowflake]</code> BigQuery See below <code>etlx[bigquery]</code>"},{"location":"user-guide/io/database-sources/#snowflake","title":"Snowflake","text":"<pre><code>source:\n  type: database\n  connection: snowflake://user:pass@account/database/schema?warehouse=compute_wh\n  table: sales\n</code></pre> <p>Or use environment variables:</p> <pre><code>export SNOWFLAKE_ACCOUNT=abc123.us-east-1\nexport SNOWFLAKE_USER=myuser\nexport SNOWFLAKE_PASSWORD=mypass\nexport SNOWFLAKE_DATABASE=mydb\nexport SNOWFLAKE_SCHEMA=public\nexport SNOWFLAKE_WAREHOUSE=compute_wh\n</code></pre>"},{"location":"user-guide/io/database-sources/#bigquery","title":"BigQuery","text":"<pre><code>source:\n  type: database\n  connection: bigquery://project-id\n  query: SELECT * FROM `project.dataset.table`\n</code></pre> <p>Set credentials via environment:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json\n</code></pre>"},{"location":"user-guide/io/database-sources/#python-api","title":"Python API","text":"<pre><code>from etlx.config.models import DatabaseSource\n\n# Using table\nsource = DatabaseSource(\n    connection=\"postgresql://localhost/db\",\n    table=\"sales\"\n)\n\n# Using query\nsource = DatabaseSource(\n    connection=\"${DATABASE_URL}\",\n    query=\"SELECT * FROM sales WHERE amount &gt; 100\"\n)\n</code></pre>"},{"location":"user-guide/io/database-sources/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/io/database-sources/#select-only-needed-columns","title":"Select Only Needed Columns","text":"<p>Instead of <code>SELECT *</code>, specify columns:</p> <pre><code>source:\n  type: database\n  query: SELECT id, amount, date FROM sales\n</code></pre>"},{"location":"user-guide/io/database-sources/#use-query-filters","title":"Use Query Filters","text":"<p>Filter in the database rather than after loading:</p> <pre><code># Good - filters in database\nsource:\n  type: database\n  query: SELECT * FROM sales WHERE date &gt;= '2025-01-01'\n\n# Less efficient - loads all then filters\nsource:\n  type: database\n  table: sales\ntransforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n</code></pre>"},{"location":"user-guide/io/database-sources/#limit-rows-for-testing","title":"Limit Rows for Testing","text":"<pre><code>source:\n  type: database\n  query: SELECT * FROM sales LIMIT 1000\n</code></pre>"},{"location":"user-guide/io/database-sources/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/io/database-sources/#connection-refused","title":"Connection Refused","text":"<pre><code>Error: Connection refused\n</code></pre> <ul> <li>Check the host and port are correct</li> <li>Verify the database server is running</li> <li>Check firewall rules</li> </ul>"},{"location":"user-guide/io/database-sources/#authentication-failed","title":"Authentication Failed","text":"<pre><code>Error: Authentication failed\n</code></pre> <ul> <li>Verify username and password</li> <li>Check the user has access to the database</li> <li>For cloud databases, check credentials are configured</li> </ul>"},{"location":"user-guide/io/database-sources/#missing-driver","title":"Missing Driver","text":"<pre><code>Error: No module named 'psycopg2'\n</code></pre> <p>Install the required extra:</p> <pre><code>pip install etlx[postgres]\n</code></pre>"},{"location":"user-guide/io/database-sources/#timeout","title":"Timeout","text":"<p>For long-running queries, consider:</p> <ul> <li>Adding query timeouts in your database</li> <li>Breaking into smaller queries</li> <li>Using incremental extraction patterns</li> </ul>"},{"location":"user-guide/io/database-sources/#related","title":"Related","text":"<ul> <li>Cloud Storage - Alternative to database reads</li> <li>Database Sinks - Writing to databases</li> <li>Backends - Backend-specific features</li> </ul>"},{"location":"user-guide/io/file-sinks/","title":"File Sinks","text":"<p>Write data to Parquet and CSV files.</p>"},{"location":"user-guide/io/file-sinks/#basic-usage","title":"Basic Usage","text":"<pre><code>sink:\n  type: file\n  path: output/results.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/file-sinks/#configuration","title":"Configuration","text":"Field Required Default Description <code>type</code> Yes - Must be <code>file</code> <code>path</code> Yes - Output path or cloud URI <code>format</code> No <code>parquet</code> Output format: <code>parquet</code>, <code>csv</code> <code>partition_by</code> No <code>[]</code> Columns to partition by <code>mode</code> No <code>overwrite</code> Write mode: <code>overwrite</code>, <code>append</code>"},{"location":"user-guide/io/file-sinks/#formats","title":"Formats","text":""},{"location":"user-guide/io/file-sinks/#parquet-recommended","title":"Parquet (Recommended)","text":"<pre><code>sink:\n  type: file\n  path: output/sales.parquet\n  format: parquet\n</code></pre> <p>Parquet advantages:</p> <ul> <li>Efficient columnar storage</li> <li>Built-in compression (snappy by default)</li> <li>Schema preservation</li> <li>Fast analytical queries</li> </ul>"},{"location":"user-guide/io/file-sinks/#csv","title":"CSV","text":"<pre><code>sink:\n  type: file\n  path: output/sales.csv\n  format: csv\n</code></pre>"},{"location":"user-guide/io/file-sinks/#write-modes","title":"Write Modes","text":""},{"location":"user-guide/io/file-sinks/#overwrite-default","title":"Overwrite (Default)","text":"<p>Replace existing data:</p> <pre><code>sink:\n  type: file\n  path: output/sales.parquet\n  mode: overwrite\n</code></pre>"},{"location":"user-guide/io/file-sinks/#append","title":"Append","text":"<p>Add to existing data:</p> <pre><code>sink:\n  type: file\n  path: output/sales.parquet\n  mode: append\n</code></pre> <p>Append with Parquet</p> <p>Appending creates additional files in the output directory. Consider using partitioning for incremental writes.</p>"},{"location":"user-guide/io/file-sinks/#partitioning","title":"Partitioning","text":"<p>Partition output by column values:</p> <pre><code>sink:\n  type: file\n  path: output/sales/\n  format: parquet\n  partition_by: [year, month]\n</code></pre> <p>This creates a directory structure:</p> <pre><code>output/sales/\n\u251c\u2500\u2500 year=2025/\n\u2502   \u251c\u2500\u2500 month=01/\n\u2502   \u2502   \u2514\u2500\u2500 data.parquet\n\u2502   \u2514\u2500\u2500 month=02/\n\u2502       \u2514\u2500\u2500 data.parquet\n\u2514\u2500\u2500 year=2024/\n    \u2514\u2500\u2500 month=12/\n        \u2514\u2500\u2500 data.parquet\n</code></pre>"},{"location":"user-guide/io/file-sinks/#common-partitioning-patterns","title":"Common Partitioning Patterns","text":""},{"location":"user-guide/io/file-sinks/#by-date","title":"By Date","text":"<pre><code># First, derive date parts\ntransforms:\n  - op: derive_column\n    name: year\n    expr: extract(year from date)\n  - op: derive_column\n    name: month\n    expr: extract(month from date)\n\nsink:\n  type: file\n  path: output/data/\n  partition_by: [year, month]\n</code></pre>"},{"location":"user-guide/io/file-sinks/#by-region","title":"By Region","text":"<pre><code>sink:\n  type: file\n  path: output/sales/\n  partition_by: [region]\n</code></pre>"},{"location":"user-guide/io/file-sinks/#multiple-levels","title":"Multiple Levels","text":"<pre><code>sink:\n  type: file\n  path: output/sales/\n  partition_by: [region, category, year]\n</code></pre>"},{"location":"user-guide/io/file-sinks/#partitioning-benefits","title":"Partitioning Benefits","text":"<ul> <li>Query performance: Only read relevant partitions</li> <li>Incremental updates: Update specific partitions</li> <li>Parallel processing: Process partitions independently</li> <li>Data management: Delete old partitions easily</li> </ul>"},{"location":"user-guide/io/file-sinks/#cloud-storage","title":"Cloud Storage","text":"<p>Write to cloud storage:</p> <pre><code># S3\nsink:\n  type: file\n  path: s3://my-bucket/output/sales.parquet\n\n# GCS\nsink:\n  type: file\n  path: gs://my-bucket/output/sales.parquet\n\n# Azure\nsink:\n  type: file\n  path: abfs://container@account.dfs.core.windows.net/output/sales.parquet\n</code></pre> <p>With partitioning:</p> <pre><code>sink:\n  type: file\n  path: s3://data-lake/processed/sales/\n  format: parquet\n  partition_by: [date]\n</code></pre>"},{"location":"user-guide/io/file-sinks/#variables-in-paths","title":"Variables in Paths","text":"<p>Use runtime variables:</p> <pre><code>sink:\n  type: file\n  path: output/${DATE}/sales.parquet\n</code></pre> <pre><code>etlx run pipeline.yml --var DATE=2025-01-15\n</code></pre> <p>For daily outputs:</p> <pre><code>sink:\n  type: file\n  path: s3://bucket/output/date=${RUN_DATE}/\n</code></pre>"},{"location":"user-guide/io/file-sinks/#python-api","title":"Python API","text":"<pre><code>from etlx.config.models import FileSink\n\n# Basic\nsink = FileSink(path=\"output/sales.parquet\")\n\n# With partitioning\nsink = FileSink(\n    path=\"output/sales/\",\n    format=\"parquet\",\n    partition_by=[\"year\", \"month\"]\n)\n\n# CSV\nsink = FileSink(\n    path=\"output/sales.csv\",\n    format=\"csv\"\n)\n</code></pre>"},{"location":"user-guide/io/file-sinks/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/io/file-sinks/#use-parquet-for-analytics","title":"Use Parquet for Analytics","text":"<p>Parquet is significantly better for analytical workloads:</p> Aspect Parquet CSV File size ~4x smaller Larger Read speed ~10x faster Slower Schema Preserved Lost Types Full support String only"},{"location":"user-guide/io/file-sinks/#partition-large-datasets","title":"Partition Large Datasets","text":"<p>For datasets over 1 million rows, use partitioning:</p> <pre><code>sink:\n  type: file\n  path: output/large_dataset/\n  partition_by: [date]\n</code></pre>"},{"location":"user-guide/io/file-sinks/#use-descriptive-paths","title":"Use Descriptive Paths","text":"<pre><code># Good\npath: output/sales_summary/date=${DATE}/\n\n# Bad\npath: output/data/\n</code></pre>"},{"location":"user-guide/io/file-sinks/#include-metadata","title":"Include Metadata","text":"<p>Consider including run metadata:</p> <pre><code># Add run date to filename\nsink:\n  type: file\n  path: output/sales_${RUN_DATE}_${RUN_ID}.parquet\n</code></pre>"},{"location":"user-guide/io/file-sinks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/io/file-sinks/#permission-denied","title":"Permission Denied","text":"<pre><code>Error: Permission denied\n</code></pre> <ul> <li>Check write permissions on the output directory</li> <li>For cloud storage, verify credentials have write access</li> <li>Ensure the output path is writable</li> </ul>"},{"location":"user-guide/io/file-sinks/#path-not-found","title":"Path Not Found","text":"<pre><code>Error: Directory not found\n</code></pre> <p>ETLX creates directories automatically. If this error occurs:</p> <ul> <li>Check the parent path is valid</li> <li>Verify cloud bucket exists</li> </ul>"},{"location":"user-guide/io/file-sinks/#disk-full","title":"Disk Full","text":"<pre><code>Error: No space left on device\n</code></pre> <ul> <li>Check available disk space</li> <li>Use cloud storage for large outputs</li> <li>Enable compression (automatic with Parquet)</li> </ul>"},{"location":"user-guide/io/file-sinks/#related","title":"Related","text":"<ul> <li>File Sources - Reading files</li> <li>Cloud Storage - Cloud provider setup</li> <li>Performance - Optimization tips</li> </ul>"},{"location":"user-guide/io/file-sources/","title":"File Sources","text":"<p>Read data from CSV, Parquet, and JSON files.</p>"},{"location":"user-guide/io/file-sources/#basic-usage","title":"Basic Usage","text":"<pre><code>source:\n  type: file\n  path: data/sales.parquet\n  format: parquet\n</code></pre>"},{"location":"user-guide/io/file-sources/#configuration","title":"Configuration","text":"Field Required Default Description <code>type</code> Yes - Must be <code>file</code> <code>path</code> Yes - File path or cloud URI <code>format</code> No <code>parquet</code> File format: <code>csv</code>, <code>parquet</code>, <code>json</code> <code>options</code> No <code>{}</code> Format-specific options"},{"location":"user-guide/io/file-sources/#formats","title":"Formats","text":""},{"location":"user-guide/io/file-sources/#parquet","title":"Parquet","text":"<p>Apache Parquet is the recommended format for performance:</p> <pre><code>source:\n  type: file\n  path: data/sales.parquet\n  format: parquet\n</code></pre> <p>Parquet benefits:</p> <ul> <li>Columnar storage (efficient for analytics)</li> <li>Built-in compression</li> <li>Schema preserved</li> <li>Fast reads with predicate pushdown</li> </ul>"},{"location":"user-guide/io/file-sources/#csv","title":"CSV","text":"<p>Read CSV files with options:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n  options:\n    delimiter: \",\"\n    header: true\n</code></pre>"},{"location":"user-guide/io/file-sources/#csv-options","title":"CSV Options","text":"Option Default Description <code>delimiter</code> <code>,</code> Field separator <code>header</code> <code>true</code> First row contains column names <code>skip_rows</code> <code>0</code> Number of rows to skip <code>null_values</code> <code>[\"\"]</code> Values to interpret as null <code>quote_char</code> <code>\"</code> Quote character"},{"location":"user-guide/io/file-sources/#csv-examples","title":"CSV Examples","text":"<p>Tab-separated file:</p> <pre><code>source:\n  type: file\n  path: data/sales.tsv\n  format: csv\n  options:\n    delimiter: \"\\t\"\n</code></pre> <p>No header row:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n  options:\n    header: false\n</code></pre> <p>Custom null values:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n  options:\n    null_values: [\"\", \"NULL\", \"N/A\", \"-\"]\n</code></pre>"},{"location":"user-guide/io/file-sources/#json","title":"JSON","text":"<p>Read JSON Lines (newline-delimited JSON):</p> <pre><code>source:\n  type: file\n  path: data/events.json\n  format: json\n</code></pre> <p>JSON Lines Format</p> <p>ETLX expects JSON Lines format where each line is a valid JSON object: <pre><code>{\"id\": 1, \"name\": \"Alice\"}\n{\"id\": 2, \"name\": \"Bob\"}\n</code></pre></p>"},{"location":"user-guide/io/file-sources/#path-patterns","title":"Path Patterns","text":""},{"location":"user-guide/io/file-sources/#local-files","title":"Local Files","text":"<pre><code>source:\n  type: file\n  path: data/sales.parquet\n</code></pre>"},{"location":"user-guide/io/file-sources/#absolute-paths","title":"Absolute Paths","text":"<pre><code>source:\n  type: file\n  path: /home/user/data/sales.parquet\n</code></pre>"},{"location":"user-guide/io/file-sources/#cloud-storage","title":"Cloud Storage","text":"<p>See Cloud Storage for detailed setup.</p> <pre><code># S3\nsource:\n  type: file\n  path: s3://my-bucket/data/sales.parquet\n\n# GCS\nsource:\n  type: file\n  path: gs://my-bucket/data/sales.parquet\n\n# Azure\nsource:\n  type: file\n  path: abfs://container@account.dfs.core.windows.net/data/sales.parquet\n</code></pre>"},{"location":"user-guide/io/file-sources/#variables-in-paths","title":"Variables in Paths","text":"<p>Use variable substitution for dynamic paths:</p> <pre><code>source:\n  type: file\n  path: data/${DATE}/sales.parquet\n</code></pre> <pre><code>etlx run pipeline.yml --var DATE=2025-01-15\n</code></pre>"},{"location":"user-guide/io/file-sources/#glob-patterns","title":"Glob Patterns","text":"<p>Coming in v0.2</p> <p>Glob patterns for reading multiple files are planned for a future release.</p>"},{"location":"user-guide/io/file-sources/#python-api","title":"Python API","text":"<pre><code>from etlx.config.models import FileSource\n\n# Parquet\nsource = FileSource(path=\"data/sales.parquet\")\n\n# CSV with options\nsource = FileSource(\n    path=\"data/sales.csv\",\n    format=\"csv\",\n    options={\"delimiter\": \";\", \"header\": True}\n)\n\n# Cloud storage\nsource = FileSource(path=\"s3://bucket/data/sales.parquet\")\n</code></pre>"},{"location":"user-guide/io/file-sources/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/io/file-sources/#use-parquet","title":"Use Parquet","text":"<p>Parquet is significantly faster than CSV for analytical workloads:</p> Format Read Time (1M rows) File Size CSV ~2.5s 100 MB Parquet ~0.3s 25 MB"},{"location":"user-guide/io/file-sources/#column-selection","title":"Column Selection","text":"<p>With Parquet, only required columns are read. Use <code>select</code> transform early:</p> <pre><code>transforms:\n  - op: select\n    columns: [id, amount, date]  # Only reads these columns\n</code></pre>"},{"location":"user-guide/io/file-sources/#compression","title":"Compression","text":"<p>Parquet files are automatically compressed. For CSV, consider gzipping:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv.gz\n  format: csv\n</code></pre>"},{"location":"user-guide/io/file-sources/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/io/file-sources/#file-not-found","title":"File Not Found","text":"<pre><code>Error: File not found: data/sales.parquet\n</code></pre> <ul> <li>Check the file path is correct</li> <li>Use absolute paths if relative paths don't work</li> <li>Ensure cloud credentials are configured</li> </ul>"},{"location":"user-guide/io/file-sources/#csv-parsing-errors","title":"CSV Parsing Errors","text":"<pre><code>Error: Could not parse CSV\n</code></pre> <ul> <li>Check the delimiter matches your file</li> <li>Verify <code>header</code> setting is correct</li> <li>Look for inconsistent row lengths</li> </ul>"},{"location":"user-guide/io/file-sources/#encoding-issues","title":"Encoding Issues","text":"<p>For files with non-UTF-8 encoding:</p> <pre><code>source:\n  type: file\n  path: data/sales.csv\n  format: csv\n  options:\n    encoding: \"latin-1\"\n</code></pre>"},{"location":"user-guide/io/file-sources/#related","title":"Related","text":"<ul> <li>Cloud Storage - S3, GCS, Azure setup</li> <li>File Sinks - Writing files</li> <li>Pipeline YAML - Full configuration reference</li> </ul>"},{"location":"user-guide/quality/","title":"Quality Checks","text":"<p>Quality checks validate your data after transforms are applied. ETLX provides 5 built-in check types.</p>"},{"location":"user-guide/quality/#overview","title":"Overview","text":"<p>Checks run after transforms, before writing to the sink:</p> <pre><code>graph LR\n    A[Source] --&gt; B[Transforms]\n    B --&gt; C[Quality Checks]\n    C --&gt; D[Sink]</code></pre> <p>If any check fails, the pipeline can either: - Fail (default) - Stop execution, don't write output - Continue - Log warning, write output anyway</p>"},{"location":"user-guide/quality/#check-types","title":"Check Types","text":"Check Purpose <code>not_null</code> Ensure no null values <code>unique</code> Verify uniqueness <code>row_count</code> Validate row count bounds <code>accepted_values</code> Check against whitelist <code>expression</code> Custom SQL validation"},{"location":"user-guide/quality/#basic-usage","title":"Basic Usage","text":"<pre><code>checks:\n  - type: not_null\n    columns: [id, name, amount]\n\n  - type: unique\n    columns: [id]\n\n  - type: row_count\n    min: 1\n    max: 1000000\n\n  - type: accepted_values\n    column: status\n    values: [pending, active, completed]\n\n  - type: expression\n    expr: amount &gt;= 0\n</code></pre>"},{"location":"user-guide/quality/#failure-behavior","title":"Failure Behavior","text":""},{"location":"user-guide/quality/#fail-on-check-failure-default","title":"Fail on Check Failure (Default)","text":"<pre><code>etlx run pipeline.yml --fail-on-checks\n</code></pre> <p>If any check fails, the pipeline stops and no output is written.</p>"},{"location":"user-guide/quality/#continue-on-check-failure","title":"Continue on Check Failure","text":"<pre><code>etlx run pipeline.yml --no-fail-on-checks\n</code></pre> <p>Checks are logged but pipeline continues.</p>"},{"location":"user-guide/quality/#python-api","title":"Python API","text":"<pre><code>result = pipeline.run(fail_on_check_failure=False)\n\nif not result.check_results[\"all_passed\"]:\n    print(\"Some checks failed:\", result.check_results)\n</code></pre>"},{"location":"user-guide/quality/#check-results","title":"Check Results","text":"<p>Pipeline results include check details:</p> <pre><code>result = pipeline.run()\n\n# Check overall status\nif result.check_results[\"all_passed\"]:\n    print(\"All checks passed!\")\nelse:\n    # Examine individual results\n    for check in result.check_results[\"results\"]:\n        print(f\"{check['check_type']}: {'PASS' if check['passed'] else 'FAIL'}\")\n        print(f\"  {check['message']}\")\n</code></pre>"},{"location":"user-guide/quality/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/#data-quality-gates","title":"Data Quality Gates","text":"<pre><code>checks:\n  # Mandatory columns\n  - type: not_null\n    columns: [id, customer_id, amount, created_at]\n\n  # Primary key\n  - type: unique\n    columns: [id]\n\n  # Business rules\n  - type: expression\n    expr: amount &gt; 0\n\n  - type: accepted_values\n    column: status\n    values: [pending, processing, completed, failed]\n</code></pre>"},{"location":"user-guide/quality/#anomaly-detection","title":"Anomaly Detection","text":"<pre><code>checks:\n  # Ensure reasonable data volume\n  - type: row_count\n    min: 1000    # At least 1000 rows expected\n    max: 100000  # No more than 100K\n\n  # Ensure no extreme values\n  - type: expression\n    expr: amount BETWEEN 0 AND 10000\n</code></pre>"},{"location":"user-guide/quality/#referential-checks","title":"Referential Checks","text":"<pre><code>checks:\n  # After join, ensure matches exist\n  - type: expression\n    expr: customer_name IS NOT NULL\n\n  # Check join success rate\n  - type: row_count\n    min: 1\n</code></pre>"},{"location":"user-guide/quality/#python-api_1","title":"Python API","text":"<pre><code>from etlx.config.checks import (\n    NotNullCheck,\n    UniqueCheck,\n    RowCountCheck,\n    AcceptedValuesCheck,\n    ExpressionCheck,\n)\n\npipeline = (\n    Pipeline(\"example\")\n    .source(source)\n    .transforms([...])\n    .check(NotNullCheck(columns=[\"id\", \"amount\"]))\n    .check(UniqueCheck(columns=[\"id\"]))\n    .check(RowCountCheck(min=1))\n    .check(ExpressionCheck(expr=\"amount &gt;= 0\"))\n    .sink(sink)\n)\n</code></pre>"},{"location":"user-guide/quality/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/quality/#check-critical-columns","title":"Check Critical Columns","text":"<p>Always check columns that downstream processes depend on:</p> <pre><code>checks:\n  - type: not_null\n    columns: [id, customer_id]  # Join keys\n</code></pre>"},{"location":"user-guide/quality/#set-reasonable-bounds","title":"Set Reasonable Bounds","text":"<pre><code>checks:\n  - type: row_count\n    min: 100      # Alert if too few rows\n    max: 10000000 # Alert if suspiciously many\n</code></pre>"},{"location":"user-guide/quality/#use-multiple-check-types","title":"Use Multiple Check Types","text":"<p>Combine checks for comprehensive validation:</p> <pre><code>checks:\n  - type: not_null\n    columns: [id]\n\n  - type: unique\n    columns: [id]\n\n  - type: expression\n    expr: id &gt; 0\n</code></pre>"},{"location":"user-guide/quality/#next-steps","title":"Next Steps","text":"<ul> <li>not_null - Check for null values</li> <li>unique - Verify uniqueness</li> <li>expression - Custom validations</li> </ul>"},{"location":"user-guide/quality/accepted-values/","title":"accepted_values","text":"<p>Verify that all values in a column are from an allowed list.</p>"},{"location":"user-guide/quality/accepted-values/#usage","title":"Usage","text":"<pre><code>- type: accepted_values\n  column: status\n  values: [pending, active, completed]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>column</code> Yes <code>str</code> Column to check <code>values</code> Yes <code>list[any]</code> Allowed values"},{"location":"user-guide/quality/accepted-values/#examples","title":"Examples","text":""},{"location":"user-guide/quality/accepted-values/#status-field","title":"Status Field","text":"<pre><code>- type: accepted_values\n  column: status\n  values: [pending, active, completed, cancelled]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#region-codes","title":"Region Codes","text":"<pre><code>- type: accepted_values\n  column: region\n  values: [north, south, east, west]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#numeric-values","title":"Numeric Values","text":"<pre><code>- type: accepted_values\n  column: priority\n  values: [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#passfail-behavior","title":"Pass/Fail Behavior","text":"<p>Pass: All values in the column are in the allowed list.</p> <p>Fail: Any value not in the allowed list.</p>"},{"location":"user-guide/quality/accepted-values/#failure-message","title":"Failure Message","text":"<pre><code>Check failed: accepted_values\n  Column 'status' contains invalid values: ['invalid', 'unknown']\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#python-api","title":"Python API","text":"<pre><code>from etlx.config.checks import AcceptedValuesCheck\n\ncheck = AcceptedValuesCheck(\n    column=\"status\",\n    values=[\"pending\", \"active\", \"completed\"]\n)\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/accepted-values/#enum-like-fields","title":"Enum-like Fields","text":"<pre><code>- type: accepted_values\n  column: order_type\n  values: [online, in_store, phone, partner]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#boolean-ish-fields","title":"Boolean-ish Fields","text":"<pre><code>- type: accepted_values\n  column: is_verified\n  values: [true, false, \"Y\", \"N\", 1, 0]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#category-validation","title":"Category Validation","text":"<pre><code>- type: accepted_values\n  column: category\n  values: [Electronics, Home, Office, Clothing, Food]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#tips","title":"Tips","text":""},{"location":"user-guide/quality/accepted-values/#null-handling","title":"Null Handling","text":"<p>By default, nulls are considered invalid. To allow nulls:</p> <pre><code>- type: accepted_values\n  column: status\n  values: [pending, active, null]\n</code></pre> <p>Or filter first:</p> <pre><code>transforms:\n  - op: filter\n    predicate: status IS NOT NULL\n\nchecks:\n  - type: accepted_values\n    column: status\n    values: [pending, active, completed]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#case-sensitivity","title":"Case Sensitivity","text":"<p>Values are case-sensitive:</p> <pre><code># \"ACTIVE\" won't match \"active\"\nvalues: [pending, active, completed]\n</code></pre> <p>Standardize case first:</p> <pre><code>transforms:\n  - op: derive_column\n    name: status_lower\n    expr: lower(status)\n\nchecks:\n  - type: accepted_values\n    column: status_lower\n    values: [pending, active, completed]\n</code></pre>"},{"location":"user-guide/quality/accepted-values/#related","title":"Related","text":"<ul> <li>expression check - More complex validations</li> <li>filter transform - Remove invalid values</li> </ul>"},{"location":"user-guide/quality/expression/","title":"expression","text":"<p>Validate data using a custom SQL expression.</p>"},{"location":"user-guide/quality/expression/#usage","title":"Usage","text":"<pre><code>- type: expression\n  expr: amount &gt;= 0\n</code></pre>"},{"location":"user-guide/quality/expression/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>expr</code> Yes <code>str</code> SQL expression that must be true for all rows"},{"location":"user-guide/quality/expression/#examples","title":"Examples","text":""},{"location":"user-guide/quality/expression/#positive-values","title":"Positive Values","text":"<pre><code>- type: expression\n  expr: amount &gt;= 0\n</code></pre>"},{"location":"user-guide/quality/expression/#range-check","title":"Range Check","text":"<pre><code>- type: expression\n  expr: amount BETWEEN 0 AND 10000\n</code></pre>"},{"location":"user-guide/quality/expression/#non-empty-string","title":"Non-Empty String","text":"<pre><code>- type: expression\n  expr: name IS NOT NULL AND length(name) &gt; 0\n</code></pre>"},{"location":"user-guide/quality/expression/#date-validation","title":"Date Validation","text":"<pre><code>- type: expression\n  expr: created_at &lt;= current_date()\n</code></pre>"},{"location":"user-guide/quality/expression/#multiple-conditions","title":"Multiple Conditions","text":"<pre><code>- type: expression\n  expr: amount &gt; 0 AND quantity &gt; 0 AND price &gt; 0\n</code></pre>"},{"location":"user-guide/quality/expression/#passfail-behavior","title":"Pass/Fail Behavior","text":"<p>Pass: Expression is true for all rows.</p> <p>Fail: Expression is false for any row.</p>"},{"location":"user-guide/quality/expression/#failure-message","title":"Failure Message","text":"<pre><code>Check failed: expression\n  Expression 'amount &gt;= 0' failed for 5 rows\n</code></pre>"},{"location":"user-guide/quality/expression/#python-api","title":"Python API","text":"<pre><code>from etlx.config.checks import ExpressionCheck\n\ncheck = ExpressionCheck(expr=\"amount &gt;= 0\")\n</code></pre>"},{"location":"user-guide/quality/expression/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/expression/#business-rules","title":"Business Rules","text":"<pre><code># Total must equal sum of parts\n- type: expression\n  expr: total = subtotal + tax + shipping\n\n# Discount can't exceed amount\n- type: expression\n  expr: discount &lt;= amount\n\n# End date after start date\n- type: expression\n  expr: end_date &gt;= start_date\n</code></pre>"},{"location":"user-guide/quality/expression/#data-quality-rules","title":"Data Quality Rules","text":"<pre><code># Valid email format (basic)\n- type: expression\n  expr: email LIKE '%@%.%'\n\n# Reasonable date range\n- type: expression\n  expr: order_date &gt;= '2020-01-01' AND order_date &lt;= current_date()\n\n# Positive quantities\n- type: expression\n  expr: quantity &gt; 0\n</code></pre>"},{"location":"user-guide/quality/expression/#consistency-checks","title":"Consistency Checks","text":"<pre><code># Status matches value\n- type: expression\n  expr: |\n    CASE\n      WHEN status = 'completed' THEN completed_at IS NOT NULL\n      ELSE true\n    END\n</code></pre>"},{"location":"user-guide/quality/expression/#aggregation-result-checks","title":"Aggregation Result Checks","text":"<pre><code># After aggregation\n- type: expression\n  expr: order_count &gt; 0 AND total_sales &gt;= 0\n</code></pre>"},{"location":"user-guide/quality/expression/#expression-syntax","title":"Expression Syntax","text":""},{"location":"user-guide/quality/expression/#operators","title":"Operators","text":"Operator Example <code>=</code>, <code>!=</code> <code>status = 'active'</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> <code>amount &gt;= 0</code> <code>AND</code>, <code>OR</code> <code>a &gt; 0 AND b &lt; 100</code> <code>IN</code> <code>status IN ('a', 'b')</code> <code>BETWEEN</code> <code>amount BETWEEN 0 AND 1000</code> <code>LIKE</code> <code>email LIKE '%@%'</code> <code>IS NULL</code> <code>discount IS NULL</code>"},{"location":"user-guide/quality/expression/#functions","title":"Functions","text":"Function Example <code>length()</code> <code>length(name) &gt; 0</code> <code>lower()</code> <code>lower(status) = 'active'</code> <code>COALESCE()</code> <code>COALESCE(amount, 0) &gt;= 0</code>"},{"location":"user-guide/quality/expression/#tips","title":"Tips","text":""},{"location":"user-guide/quality/expression/#multiline-expressions","title":"Multiline Expressions","text":"<pre><code>- type: expression\n  expr: |\n    amount &gt; 0\n    AND quantity &gt; 0\n    AND status IN ('pending', 'active', 'completed')\n</code></pre>"},{"location":"user-guide/quality/expression/#complex-logic","title":"Complex Logic","text":"<pre><code>- type: expression\n  expr: |\n    CASE\n      WHEN order_type = 'refund' THEN amount &lt; 0\n      ELSE amount &gt; 0\n    END\n</code></pre>"},{"location":"user-guide/quality/expression/#null-safe-comparisons","title":"Null-Safe Comparisons","text":"<pre><code># Handle nulls explicitly\n- type: expression\n  expr: COALESCE(discount, 0) &lt;= amount\n</code></pre>"},{"location":"user-guide/quality/expression/#errors","title":"Errors","text":""},{"location":"user-guide/quality/expression/#syntax-error","title":"Syntax Error","text":"<pre><code>Error: Could not parse expression\n</code></pre> <p>Check expression syntax.</p>"},{"location":"user-guide/quality/expression/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Verify column names.</p>"},{"location":"user-guide/quality/expression/#related","title":"Related","text":"<ul> <li>Expression Language - Full expression reference</li> <li>accepted_values check - Simpler value validation</li> <li>derive_column transform - Uses same expression syntax</li> </ul>"},{"location":"user-guide/quality/not-null/","title":"not_null","text":"<p>Verify that specified columns contain no null values.</p>"},{"location":"user-guide/quality/not-null/#usage","title":"Usage","text":"<pre><code>- type: not_null\n  columns: [id, name, amount]\n</code></pre>"},{"location":"user-guide/quality/not-null/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>columns</code> Yes <code>list[str]</code> Columns that must not contain nulls"},{"location":"user-guide/quality/not-null/#examples","title":"Examples","text":""},{"location":"user-guide/quality/not-null/#single-column","title":"Single Column","text":"<pre><code>- type: not_null\n  columns: [id]\n</code></pre>"},{"location":"user-guide/quality/not-null/#multiple-columns","title":"Multiple Columns","text":"<pre><code>- type: not_null\n  columns: [id, customer_id, amount, created_at]\n</code></pre>"},{"location":"user-guide/quality/not-null/#passfail-behavior","title":"Pass/Fail Behavior","text":"<p>Pass: All values in specified columns are non-null.</p> <p>Fail: Any null value found in specified columns.</p>"},{"location":"user-guide/quality/not-null/#failure-message","title":"Failure Message","text":"<pre><code>Check failed: not_null\n  Column 'customer_id' contains 15 null values\n</code></pre>"},{"location":"user-guide/quality/not-null/#python-api","title":"Python API","text":"<pre><code>from etlx.config.checks import NotNullCheck\n\ncheck = NotNullCheck(columns=[\"id\", \"name\", \"amount\"])\n</code></pre>"},{"location":"user-guide/quality/not-null/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/not-null/#check-primary-key","title":"Check Primary Key","text":"<pre><code>- type: not_null\n  columns: [id]\n</code></pre>"},{"location":"user-guide/quality/not-null/#check-foreign-keys","title":"Check Foreign Keys","text":"<pre><code>- type: not_null\n  columns: [customer_id, product_id, order_id]\n</code></pre>"},{"location":"user-guide/quality/not-null/#check-required-fields","title":"Check Required Fields","text":"<pre><code>- type: not_null\n  columns: [email, name, created_at]\n</code></pre>"},{"location":"user-guide/quality/not-null/#after-join","title":"After Join","text":"<pre><code>transforms:\n  - op: join\n    right: customers\n    on: [customer_id]\n    how: left\n\nchecks:\n  # Verify join found matches\n  - type: not_null\n    columns: [customer_name]\n</code></pre>"},{"location":"user-guide/quality/not-null/#tips","title":"Tips","text":""},{"location":"user-guide/quality/not-null/#fill-nulls-first","title":"Fill Nulls First","text":"<p>If nulls are expected, fill them before checking:</p> <pre><code>transforms:\n  - op: fill_null\n    columns:\n      status: \"unknown\"\n\nchecks:\n  - type: not_null\n    columns: [status]  # Now passes\n</code></pre>"},{"location":"user-guide/quality/not-null/#check-after-transforms","title":"Check After Transforms","text":"<p>Checks run after transforms, so derived columns can be checked:</p> <pre><code>transforms:\n  - op: derive_column\n    name: total\n    expr: quantity * price\n\nchecks:\n  - type: not_null\n    columns: [total]  # Checks derived column\n</code></pre>"},{"location":"user-guide/quality/not-null/#related","title":"Related","text":"<ul> <li>fill_null transform - Replace nulls</li> <li>filter transform - Remove null rows</li> <li>unique check - Also catches nulls in unique columns</li> </ul>"},{"location":"user-guide/quality/row-count/","title":"row_count","text":"<p>Verify that the row count is within expected bounds.</p>"},{"location":"user-guide/quality/row-count/#usage","title":"Usage","text":"<pre><code>- type: row_count\n  min: 1\n  max: 1000000\n</code></pre>"},{"location":"user-guide/quality/row-count/#parameters","title":"Parameters","text":"Parameter Required Default Description <code>min</code> No None Minimum row count <code>max</code> No None Maximum row count <p>At least one of <code>min</code> or <code>max</code> must be specified.</p>"},{"location":"user-guide/quality/row-count/#examples","title":"Examples","text":""},{"location":"user-guide/quality/row-count/#minimum-only","title":"Minimum Only","text":"<pre><code>- type: row_count\n  min: 1\n</code></pre>"},{"location":"user-guide/quality/row-count/#maximum-only","title":"Maximum Only","text":"<pre><code>- type: row_count\n  max: 1000000\n</code></pre>"},{"location":"user-guide/quality/row-count/#both-bounds","title":"Both Bounds","text":"<pre><code>- type: row_count\n  min: 100\n  max: 10000\n</code></pre>"},{"location":"user-guide/quality/row-count/#passfail-behavior","title":"Pass/Fail Behavior","text":"<p>Pass: Row count is within specified bounds.</p> <p>Fail: Row count is outside bounds.</p>"},{"location":"user-guide/quality/row-count/#failure-messages","title":"Failure Messages","text":"<pre><code>Check failed: row_count\n  Row count 0 is below minimum 1\n\nCheck failed: row_count\n  Row count 1500000 exceeds maximum 1000000\n</code></pre>"},{"location":"user-guide/quality/row-count/#python-api","title":"Python API","text":"<pre><code>from etlx.config.checks import RowCountCheck\n\n# At least 1 row\ncheck = RowCountCheck(min=1)\n\n# Between bounds\ncheck = RowCountCheck(min=100, max=10000)\n</code></pre>"},{"location":"user-guide/quality/row-count/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/row-count/#non-empty-output","title":"Non-Empty Output","text":"<pre><code>- type: row_count\n  min: 1\n</code></pre>"},{"location":"user-guide/quality/row-count/#expected-range","title":"Expected Range","text":"<pre><code># Daily sales should have 1K-10K rows\n- type: row_count\n  min: 1000\n  max: 10000\n</code></pre>"},{"location":"user-guide/quality/row-count/#detect-anomalies","title":"Detect Anomalies","text":"<pre><code># Alert if unusually high\n- type: row_count\n  max: 100000\n</code></pre>"},{"location":"user-guide/quality/row-count/#after-filter","title":"After Filter","text":"<pre><code>transforms:\n  - op: filter\n    predicate: status = 'active'\n\nchecks:\n  # Ensure filter didn't remove everything\n  - type: row_count\n    min: 1\n</code></pre>"},{"location":"user-guide/quality/row-count/#tips","title":"Tips","text":""},{"location":"user-guide/quality/row-count/#use-variables","title":"Use Variables","text":"<pre><code>- type: row_count\n  min: ${MIN_ROWS:-1}\n  max: ${MAX_ROWS:-1000000}\n</code></pre>"},{"location":"user-guide/quality/row-count/#document-expected-ranges","title":"Document Expected Ranges","text":"<pre><code># Daily pipeline: expect 5K-50K rows\n# Less than 5K suggests data source issue\n# More than 50K suggests duplicate ingestion\n- type: row_count\n  min: 5000\n  max: 50000\n</code></pre>"},{"location":"user-guide/quality/row-count/#related","title":"Related","text":"<ul> <li>filter transform - May affect row count</li> <li>dedup transform - May reduce row count</li> </ul>"},{"location":"user-guide/quality/unique/","title":"unique","text":"<p>Verify that column values are unique (no duplicates).</p>"},{"location":"user-guide/quality/unique/#usage","title":"Usage","text":"<pre><code>- type: unique\n  columns: [id]\n</code></pre>"},{"location":"user-guide/quality/unique/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>columns</code> Yes <code>list[str]</code> Columns that must be unique"},{"location":"user-guide/quality/unique/#examples","title":"Examples","text":""},{"location":"user-guide/quality/unique/#single-column","title":"Single Column","text":"<pre><code>- type: unique\n  columns: [id]\n</code></pre>"},{"location":"user-guide/quality/unique/#composite-key","title":"Composite Key","text":"<pre><code>- type: unique\n  columns: [customer_id, order_date]\n</code></pre>"},{"location":"user-guide/quality/unique/#passfail-behavior","title":"Pass/Fail Behavior","text":"<p>Pass: All combinations of specified columns are unique.</p> <p>Fail: Duplicate values found.</p>"},{"location":"user-guide/quality/unique/#failure-message","title":"Failure Message","text":"<pre><code>Check failed: unique\n  Found 23 duplicate values in columns [id]\n</code></pre>"},{"location":"user-guide/quality/unique/#python-api","title":"Python API","text":"<pre><code>from etlx.config.checks import UniqueCheck\n\ncheck = UniqueCheck(columns=[\"id\"])\ncheck = UniqueCheck(columns=[\"customer_id\", \"order_date\"])\n</code></pre>"},{"location":"user-guide/quality/unique/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quality/unique/#primary-key","title":"Primary Key","text":"<pre><code>- type: unique\n  columns: [id]\n</code></pre>"},{"location":"user-guide/quality/unique/#natural-key","title":"Natural Key","text":"<pre><code>- type: unique\n  columns: [customer_id, product_id, order_date]\n</code></pre>"},{"location":"user-guide/quality/unique/#after-dedup","title":"After Dedup","text":"<pre><code>transforms:\n  - op: dedup\n    columns: [id]\n\nchecks:\n  - type: unique\n    columns: [id]  # Verify dedup worked\n</code></pre>"},{"location":"user-guide/quality/unique/#tips","title":"Tips","text":""},{"location":"user-guide/quality/unique/#null-handling","title":"Null Handling","text":"<p>Nulls are treated as equal for uniqueness:</p> <ul> <li>Two rows with <code>NULL</code> in <code>id</code> \u2192 Duplicate</li> </ul> <p>To exclude nulls:</p> <pre><code>transforms:\n  - op: filter\n    predicate: id IS NOT NULL\n\nchecks:\n  - type: unique\n    columns: [id]\n</code></pre>"},{"location":"user-guide/quality/unique/#composite-uniqueness","title":"Composite Uniqueness","text":"<p>Check combination of columns:</p> <pre><code># Each customer can have one order per day\n- type: unique\n  columns: [customer_id, order_date]\n</code></pre>"},{"location":"user-guide/quality/unique/#related","title":"Related","text":"<ul> <li>dedup transform - Remove duplicates</li> <li>not_null check - Often combined with unique</li> </ul>"},{"location":"user-guide/transforms/","title":"Transforms","text":"<p>Transforms modify data as it flows through your pipeline. ETLX provides 12 built-in transform operations.</p>"},{"location":"user-guide/transforms/#overview","title":"Overview","text":"<p>Transforms are applied in sequence. Each transform takes the output of the previous step as input.</p> <pre><code>transforms:\n  - op: filter          # Step 1: Filter rows\n    predicate: amount &gt; 0\n\n  - op: derive_column   # Step 2: Add column (uses filtered data)\n    name: tax\n    expr: amount * 0.1\n\n  - op: aggregate       # Step 3: Aggregate (uses data with tax column)\n    group_by: [category]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/#transform-operations","title":"Transform Operations","text":""},{"location":"user-guide/transforms/#data-selection","title":"Data Selection","text":"Transform Purpose Example <code>select</code> Choose columns Keep only <code>id</code>, <code>name</code>, <code>amount</code> <code>rename</code> Rename columns Change <code>cust_id</code> to <code>customer_id</code> <code>limit</code> Limit rows Take first 1000 rows"},{"location":"user-guide/transforms/#data-filtering","title":"Data Filtering","text":"Transform Purpose Example <code>filter</code> Filter rows Keep rows where <code>amount &gt; 100</code> <code>dedup</code> Remove duplicates Keep unique <code>customer_id</code>"},{"location":"user-guide/transforms/#data-modification","title":"Data Modification","text":"Transform Purpose Example <code>derive_column</code> Add computed column Calculate <code>total = qty * price</code> <code>cast</code> Convert types Change <code>id</code> from int to string <code>fill_null</code> Replace nulls Set null <code>status</code> to <code>\"unknown\"</code>"},{"location":"user-guide/transforms/#data-organization","title":"Data Organization","text":"Transform Purpose Example <code>sort</code> Order rows Sort by <code>amount</code> descending <code>aggregate</code> Group and summarize Sum <code>amount</code> by <code>region</code>"},{"location":"user-guide/transforms/#data-combination","title":"Data Combination","text":"Transform Purpose Example <code>join</code> Join datasets Join orders with customers <code>union</code> Stack datasets Combine daily files"},{"location":"user-guide/transforms/#quick-reference","title":"Quick Reference","text":"<pre><code># Select columns\n- op: select\n  columns: [id, name, amount]\n\n# Rename columns\n- op: rename\n  mapping:\n    old_name: new_name\n\n# Filter rows\n- op: filter\n  predicate: amount &gt; 100 AND status = 'active'\n\n# Add computed column\n- op: derive_column\n  name: total_with_tax\n  expr: amount * 1.1\n\n# Convert types\n- op: cast\n  columns:\n    id: string\n    amount: float64\n\n# Replace nulls\n- op: fill_null\n  columns:\n    status: \"unknown\"\n    amount: 0\n\n# Remove duplicates\n- op: dedup\n  columns: [customer_id]\n\n# Sort rows\n- op: sort\n  by: [amount]\n  descending: true\n\n# Join datasets\n- op: join\n  right: customers\n  on: [customer_id]\n  how: left\n\n# Aggregate\n- op: aggregate\n  group_by: [region]\n  aggs:\n    total: sum(amount)\n    count: count(*)\n\n# Combine datasets\n- op: union\n  sources: [data1, data2]\n\n# Limit rows\n- op: limit\n  n: 1000\n</code></pre>"},{"location":"user-guide/transforms/#transform-order-best-practices","title":"Transform Order Best Practices","text":""},{"location":"user-guide/transforms/#1-filter-early","title":"1. Filter Early","text":"<p>Apply filters as early as possible to reduce data volume:</p> <pre><code>transforms:\n  - op: filter              # First: reduce rows\n    predicate: date &gt;= '2025-01-01'\n\n  - op: derive_column       # Then: compute on smaller dataset\n    name: metric\n    expr: complex_calculation\n</code></pre>"},{"location":"user-guide/transforms/#2-select-before-aggregate","title":"2. Select Before Aggregate","text":"<p>Remove unnecessary columns before aggregation:</p> <pre><code>transforms:\n  - op: select              # Remove unused columns\n    columns: [category, amount]\n\n  - op: aggregate           # Aggregate on smaller dataset\n    group_by: [category]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/#3-derive-before-aggregate","title":"3. Derive Before Aggregate","text":"<p>Create columns needed for aggregation:</p> <pre><code>transforms:\n  - op: derive_column       # Create column first\n    name: net_amount\n    expr: amount - discount\n\n  - op: aggregate           # Then aggregate\n    group_by: [region]\n    aggs:\n      total_net: sum(net_amount)\n</code></pre>"},{"location":"user-guide/transforms/#expression-language","title":"Expression Language","text":"<p>Many transforms use SQL-like expressions:</p>"},{"location":"user-guide/transforms/#operators","title":"Operators","text":"Type Operators Arithmetic <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code> Comparison <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Logical <code>AND</code>, <code>OR</code>, <code>NOT</code> Null <code>IS NULL</code>, <code>IS NOT NULL</code>"},{"location":"user-guide/transforms/#functions","title":"Functions","text":"Category Functions String <code>UPPER()</code>, <code>LOWER()</code>, <code>TRIM()</code>, <code>CONCAT()</code> Math <code>ABS()</code>, <code>ROUND()</code>, <code>FLOOR()</code>, <code>CEIL()</code> Date <code>EXTRACT()</code>, <code>DATE_TRUNC()</code> Null <code>COALESCE()</code>, <code>NULLIF()</code> Aggregate <code>SUM()</code>, <code>AVG()</code>, <code>MIN()</code>, <code>MAX()</code>, <code>COUNT()</code> <p>See Expression Language for full reference.</p>"},{"location":"user-guide/transforms/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import (\n    SelectTransform,\n    FilterTransform,\n    DeriveColumnTransform,\n    AggregateTransform,\n)\n\npipeline = (\n    Pipeline(\"example\")\n    .source(source)\n    .transform(FilterTransform(predicate=\"amount &gt; 0\"))\n    .transform(DeriveColumnTransform(name=\"tax\", expr=\"amount * 0.1\"))\n    .transform(AggregateTransform(\n        group_by=[\"category\"],\n        aggs={\"total\": \"sum(amount)\"}\n    ))\n    .sink(sink)\n)\n</code></pre>"},{"location":"user-guide/transforms/#next-steps","title":"Next Steps","text":"<p>Explore each transform in detail:</p> <ul> <li>select - Choose and reorder columns</li> <li>filter - Filter rows with predicates</li> <li>aggregate - Group and summarize data</li> </ul>"},{"location":"user-guide/transforms/aggregate/","title":"aggregate","text":"<p>Group data and compute summary statistics.</p>"},{"location":"user-guide/transforms/aggregate/#usage","title":"Usage","text":"<pre><code>- op: aggregate\n  group_by: [region]\n  aggs:\n    total_sales: sum(amount)\n    order_count: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>group_by</code> Yes <code>list[str]</code> Columns to group by <code>aggs</code> Yes <code>dict[str, str]</code> Output column \u2192 aggregation expression"},{"location":"user-guide/transforms/aggregate/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/aggregate/#basic-aggregation","title":"Basic Aggregation","text":"<pre><code>- op: aggregate\n  group_by: [category]\n  aggs:\n    total_sales: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#multiple-aggregations","title":"Multiple Aggregations","text":"<pre><code>- op: aggregate\n  group_by: [region]\n  aggs:\n    total_sales: sum(amount)\n    avg_order: avg(amount)\n    min_order: min(amount)\n    max_order: max(amount)\n    order_count: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#multiple-group-columns","title":"Multiple Group Columns","text":"<pre><code>- op: aggregate\n  group_by: [region, category, year]\n  aggs:\n    total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#count-distinct","title":"Count Distinct","text":"<pre><code>- op: aggregate\n  group_by: [region]\n  aggs:\n    unique_customers: count(customer_id)\n    total_orders: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id region category amount 1 North Electronics 100 2 North Electronics 200 3 North Home 50 4 South Electronics 150 5 South Home 75 <p>Transform:</p> <pre><code>- op: aggregate\n  group_by: [region, category]\n  aggs:\n    total_sales: sum(amount)\n    order_count: count(*)\n    avg_order: avg(amount)\n</code></pre> <p>Output Data:</p> region category total_sales order_count avg_order North Electronics 300 2 150.0 North Home 50 1 50.0 South Electronics 150 1 150.0 South Home 75 1 75.0"},{"location":"user-guide/transforms/aggregate/#aggregation-functions","title":"Aggregation Functions","text":"Function Description Example <code>sum(col)</code> Sum of values <code>sum(amount)</code> <code>avg(col)</code> Average (mean) <code>avg(amount)</code> <code>mean(col)</code> Same as avg <code>mean(amount)</code> <code>min(col)</code> Minimum value <code>min(amount)</code> <code>max(col)</code> Maximum value <code>max(amount)</code> <code>count(*)</code> Count all rows <code>count(*)</code> <code>count(col)</code> Count non-null <code>count(customer_id)</code>"},{"location":"user-guide/transforms/aggregate/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import AggregateTransform\n\ntransform = AggregateTransform(\n    group_by=[\"region\", \"category\"],\n    aggs={\n        \"total_sales\": \"sum(amount)\",\n        \"order_count\": \"count(*)\",\n        \"avg_order\": \"avg(amount)\",\n    }\n)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/aggregate/#daily-summary","title":"Daily Summary","text":"<pre><code>transforms:\n  - op: derive_column\n    name: date\n    expr: date_trunc('day', created_at)\n\n  - op: aggregate\n    group_by: [date]\n    aggs:\n      daily_sales: sum(amount)\n      daily_orders: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#regional-breakdown","title":"Regional Breakdown","text":"<pre><code>- op: aggregate\n  group_by: [region]\n  aggs:\n    total_sales: sum(amount)\n    avg_order_value: avg(amount)\n    customer_count: count(customer_id)\n    order_count: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#product-performance","title":"Product Performance","text":"<pre><code>- op: aggregate\n  group_by: [product_id, product_name]\n  aggs:\n    units_sold: sum(quantity)\n    revenue: sum(amount)\n    orders: count(*)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#time-series-aggregation","title":"Time Series Aggregation","text":"<pre><code>transforms:\n  # Extract time components\n  - op: derive_column\n    name: year\n    expr: extract(year from date)\n\n  - op: derive_column\n    name: month\n    expr: extract(month from date)\n\n  # Aggregate by time period\n  - op: aggregate\n    group_by: [year, month]\n    aggs:\n      monthly_sales: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#pre-aggregation-transforms","title":"Pre-Aggregation Transforms","text":"<p>Often you need to prepare data before aggregating:</p> <pre><code>transforms:\n  # 1. Clean data\n  - op: filter\n    predicate: amount &gt; 0 AND status != 'cancelled'\n\n  # 2. Create metrics\n  - op: derive_column\n    name: net_amount\n    expr: amount - COALESCE(discount, 0)\n\n  # 3. Aggregate\n  - op: aggregate\n    group_by: [region]\n    aggs:\n      gross_sales: sum(amount)\n      net_sales: sum(net_amount)\n      total_discount: sum(COALESCE(discount, 0))\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/aggregate/#aggregating-derived-columns","title":"Aggregating Derived Columns","text":"<p>Create columns before aggregating them:</p> <pre><code>transforms:\n  - op: derive_column\n    name: revenue\n    expr: quantity * unit_price\n\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total_revenue: sum(revenue)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#handling-nulls","title":"Handling Nulls","text":"<p>Aggregation functions ignore nulls by default:</p> <pre><code># If some amounts are null:\n# sum() ignores nulls\n# count(amount) counts non-null only\n# count(*) counts all rows\n</code></pre> <p>To include nulls:</p> <pre><code>transforms:\n  - op: fill_null\n    columns:\n      amount: 0\n\n  - op: aggregate\n    group_by: [region]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#output-column-names","title":"Output Column Names","text":"<p>Choose descriptive names:</p> <pre><code>aggs:\n  total_revenue_usd: sum(amount)      # Good\n  avg_order_value: avg(amount)        # Good\n  sum_amount: sum(amount)             # Less descriptive\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#performance","title":"Performance","text":""},{"location":"user-guide/transforms/aggregate/#reduce-data-first","title":"Reduce Data First","text":"<p>Filter and select before aggregating:</p> <pre><code>transforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  - op: select\n    columns: [region, amount]\n\n  - op: aggregate\n    group_by: [region]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/aggregate/#group-cardinality","title":"Group Cardinality","text":"<p>Be mindful of unique group combinations:</p> <ul> <li>Low cardinality (region: 5 values) \u2192 Fast</li> <li>High cardinality (customer_id: 1M values) \u2192 Slower, more memory</li> </ul>"},{"location":"user-guide/transforms/aggregate/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/aggregate/#invalid-aggregation-function","title":"Invalid Aggregation Function","text":"<pre><code>Error: Unknown aggregation function 'median'\n</code></pre> <p>Use supported functions: <code>sum</code>, <code>avg</code>, <code>mean</code>, <code>min</code>, <code>max</code>, <code>count</code>.</p>"},{"location":"user-guide/transforms/aggregate/#column-not-in-group-by","title":"Column Not in Group By","text":"<pre><code>Error: Column 'name' must appear in GROUP BY or be aggregated\n</code></pre> <p>Every non-aggregated column must be in <code>group_by</code>.</p>"},{"location":"user-guide/transforms/aggregate/#related","title":"Related","text":"<ul> <li>derive_column - Create columns before aggregating</li> <li>filter - Filter before aggregating</li> <li>sort - Sort aggregated results</li> </ul>"},{"location":"user-guide/transforms/cast/","title":"cast","text":"<p>Convert column data types.</p>"},{"location":"user-guide/transforms/cast/#usage","title":"Usage","text":"<pre><code>- op: cast\n  columns:\n    id: string\n    amount: float64\n</code></pre>"},{"location":"user-guide/transforms/cast/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>columns</code> Yes <code>dict[str, str]</code> Column \u2192 target type mapping"},{"location":"user-guide/transforms/cast/#supported-types","title":"Supported Types","text":"Type Aliases Description <code>string</code> <code>str</code> Text/string <code>int64</code> <code>int</code>, <code>integer</code> 64-bit integer <code>int32</code> 32-bit integer <code>float64</code> <code>float</code>, <code>double</code> 64-bit float <code>float32</code> 32-bit float <code>boolean</code> <code>bool</code> True/False <code>date</code> Date (no time) <code>datetime</code> <code>timestamp</code> Date with time"},{"location":"user-guide/transforms/cast/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/cast/#string-to-number","title":"String to Number","text":"<pre><code>- op: cast\n  columns:\n    quantity: int64\n    price: float64\n</code></pre>"},{"location":"user-guide/transforms/cast/#number-to-string","title":"Number to String","text":"<pre><code>- op: cast\n  columns:\n    zip_code: string\n    product_id: string\n</code></pre>"},{"location":"user-guide/transforms/cast/#string-to-date","title":"String to Date","text":"<pre><code>- op: cast\n  columns:\n    order_date: date\n    created_at: datetime\n</code></pre>"},{"location":"user-guide/transforms/cast/#multiple-casts","title":"Multiple Casts","text":"<pre><code>- op: cast\n  columns:\n    id: string\n    quantity: int64\n    amount: float64\n    is_active: boolean\n    created_at: datetime\n</code></pre>"},{"location":"user-guide/transforms/cast/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id amount is_active 1 \"99.99\" \"true\" 2 \"149.50\" \"false\" <p>Transform:</p> <pre><code>- op: cast\n  columns:\n    id: string\n    amount: float64\n    is_active: boolean\n</code></pre> <p>Output Data:</p> id amount is_active \"1\" 99.99 true \"2\" 149.50 false"},{"location":"user-guide/transforms/cast/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import CastTransform\n\ntransform = CastTransform(\n    columns={\n        \"id\": \"string\",\n        \"amount\": \"float64\",\n        \"is_active\": \"boolean\",\n    }\n)\n</code></pre>"},{"location":"user-guide/transforms/cast/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/cast/#prepare-for-calculations","title":"Prepare for Calculations","text":"<pre><code>transforms:\n  # Cast strings to numbers\n  - op: cast\n    columns:\n      quantity: int64\n      price: float64\n\n  # Now can calculate\n  - op: derive_column\n    name: total\n    expr: quantity * price\n</code></pre>"},{"location":"user-guide/transforms/cast/#standardize-ids","title":"Standardize IDs","text":"<pre><code>- op: cast\n  columns:\n    customer_id: string\n    order_id: string\n    product_id: string\n</code></pre>"},{"location":"user-guide/transforms/cast/#parse-dates","title":"Parse Dates","text":"<pre><code>transforms:\n  - op: cast\n    columns:\n      date_string: date\n\n  - op: derive_column\n    name: year\n    expr: extract(year from date_string)\n</code></pre>"},{"location":"user-guide/transforms/cast/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/cast/#cast-before-join","title":"Cast Before Join","text":"<p>Ensure join keys have matching types:</p> <pre><code>transforms:\n  # Orders: customer_id is integer\n  - op: cast\n    columns:\n      customer_id: string\n\n  # Now matches customers.customer_id (string)\n  - op: join\n    right: customers\n    on: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/cast/#handle-nulls","title":"Handle Nulls","text":"<p>Casting preserves nulls:</p> <pre><code># If amount is NULL, cast result is still NULL\n- op: cast\n  columns:\n    amount: float64\n</code></pre>"},{"location":"user-guide/transforms/cast/#boolean-casting","title":"Boolean Casting","text":"<p>Common boolean representations:</p> Input Cast to boolean <code>\"true\"</code>, <code>\"1\"</code>, <code>1</code> <code>true</code> <code>\"false\"</code>, <code>\"0\"</code>, <code>0</code> <code>false</code>"},{"location":"user-guide/transforms/cast/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/cast/#invalid-cast","title":"Invalid Cast","text":"<pre><code>Error: Cannot cast 'abc' to integer\n</code></pre> <p>Data must be convertible. Filter invalid values first:</p> <pre><code>transforms:\n  - op: filter\n    predicate: amount IS NOT NULL AND amount != ''\n\n  - op: cast\n    columns:\n      amount: float64\n</code></pre>"},{"location":"user-guide/transforms/cast/#unknown-type","title":"Unknown Type","text":"<pre><code>Error: Unknown type 'varchar'\n</code></pre> <p>Use supported type names. <code>string</code> instead of <code>varchar</code>.</p>"},{"location":"user-guide/transforms/cast/#related","title":"Related","text":"<ul> <li>derive_column - Calculate after casting</li> <li>fill_null - Handle nulls before casting</li> <li>Data Types - Type reference</li> </ul>"},{"location":"user-guide/transforms/dedup/","title":"dedup","text":"<p>Remove duplicate rows.</p>"},{"location":"user-guide/transforms/dedup/#usage","title":"Usage","text":"<pre><code>- op: dedup\n  columns: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#parameters","title":"Parameters","text":"Parameter Required Default Description <code>columns</code> No All columns Columns to consider for uniqueness"},{"location":"user-guide/transforms/dedup/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/dedup/#deduplicate-on-all-columns","title":"Deduplicate on All Columns","text":"<pre><code># Remove exact duplicate rows\n- op: dedup\n</code></pre>"},{"location":"user-guide/transforms/dedup/#deduplicate-on-specific-columns","title":"Deduplicate on Specific Columns","text":"<pre><code># Keep first row for each customer_id\n- op: dedup\n  columns: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#composite-key","title":"Composite Key","text":"<pre><code># Unique combination of customer and product\n- op: dedup\n  columns: [customer_id, product_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id customer_id product amount 1 C001 Widget 100 2 C001 Gadget 200 3 C002 Widget 150 4 C001 Other 50 <p>Transform:</p> <pre><code>- op: dedup\n  columns: [customer_id]\n</code></pre> <p>Output Data (keeps first occurrence):</p> id customer_id product amount 1 C001 Widget 100 3 C002 Widget 150"},{"location":"user-guide/transforms/dedup/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import DedupTransform\n\n# All columns\ntransform = DedupTransform()\n\n# Specific columns\ntransform = DedupTransform(columns=[\"customer_id\"])\n</code></pre>"},{"location":"user-guide/transforms/dedup/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/dedup/#remove-exact-duplicates","title":"Remove Exact Duplicates","text":"<pre><code># From data with duplicate rows\n- op: dedup\n</code></pre>"},{"location":"user-guide/transforms/dedup/#latest-record-per-entity","title":"Latest Record Per Entity","text":"<pre><code>transforms:\n  # Sort to get latest first\n  - op: sort\n    by: [updated_at]\n    descending: true\n\n  # Keep only first (latest) per customer\n  - op: dedup\n    columns: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#unique-combinations","title":"Unique Combinations","text":"<pre><code># Unique customer-product pairs\n- op: dedup\n  columns: [customer_id, product_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#clean-event-data","title":"Clean Event Data","text":"<pre><code>transforms:\n  # Remove duplicate events\n  - op: dedup\n    columns: [event_id]\n\n  # Or based on combination\n  - op: dedup\n    columns: [user_id, event_type, timestamp]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/dedup/#row-selection","title":"Row Selection","text":"<p>When duplicates exist, the first row encountered is kept. To control which row:</p> <pre><code>transforms:\n  # Sort first to define \"first\"\n  - op: sort\n    by: [priority, created_at]\n    descending: true\n\n  - op: dedup\n    columns: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/dedup/#count-duplicates","title":"Count Duplicates","text":"<p>To count duplicates before removing:</p> <pre><code>total_rows = engine.row_count(data)\ndeduped = engine.dedup(data, [\"customer_id\"])\ndeduped_rows = engine.row_count(deduped)\nduplicate_count = total_rows - deduped_rows\n</code></pre>"},{"location":"user-guide/transforms/dedup/#performance","title":"Performance","text":"<p>Deduplication requires comparing rows. For large datasets:</p> <ul> <li>Dedup on fewer columns is faster</li> <li>Filter first to reduce data volume</li> </ul>"},{"location":"user-guide/transforms/dedup/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/dedup/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Check that specified columns exist.</p>"},{"location":"user-guide/transforms/dedup/#related","title":"Related","text":"<ul> <li>sort - Sort before dedup to control which row is kept</li> <li>unique check - Verify uniqueness</li> <li>filter - Alternative for removing unwanted rows</li> </ul>"},{"location":"user-guide/transforms/derive-column/","title":"derive_column","text":"<p>Create a new computed column from an expression.</p>"},{"location":"user-guide/transforms/derive-column/#usage","title":"Usage","text":"<pre><code>- op: derive_column\n  name: total_with_tax\n  expr: amount * 1.1\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>name</code> Yes <code>str</code> Name for the new column <code>expr</code> Yes <code>str</code> SQL-like expression"},{"location":"user-guide/transforms/derive-column/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/derive-column/#arithmetic","title":"Arithmetic","text":"<pre><code># Multiplication\n- op: derive_column\n  name: total_with_tax\n  expr: amount * 1.1\n\n# Division\n- op: derive_column\n  name: unit_price\n  expr: total / quantity\n\n# Complex calculation\n- op: derive_column\n  name: profit_margin\n  expr: (revenue - cost) / revenue * 100\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#string-operations","title":"String Operations","text":"<pre><code># Uppercase\n- op: derive_column\n  name: name_upper\n  expr: upper(name)\n\n# Concatenation\n- op: derive_column\n  name: full_name\n  expr: concat(first_name, ' ', last_name)\n\n# Substring\n- op: derive_column\n  name: country_code\n  expr: substring(phone, 1, 2)\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#date-operations","title":"Date Operations","text":"<pre><code># Extract year\n- op: derive_column\n  name: year\n  expr: extract(year from created_at)\n\n# Extract month\n- op: derive_column\n  name: month\n  expr: extract(month from created_at)\n\n# Date truncation\n- op: derive_column\n  name: week_start\n  expr: date_trunc('week', created_at)\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#conditional-logic","title":"Conditional Logic","text":"<pre><code># CASE expression\n- op: derive_column\n  name: size_category\n  expr: |\n    CASE\n      WHEN amount &lt; 100 THEN 'small'\n      WHEN amount &lt; 1000 THEN 'medium'\n      ELSE 'large'\n    END\n\n# Simpler conditional\n- op: derive_column\n  name: is_high_value\n  expr: amount &gt;= 1000\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#null-handling","title":"Null Handling","text":"<pre><code># Default value for nulls\n- op: derive_column\n  name: discount_safe\n  expr: COALESCE(discount, 0)\n\n# Replace value with null\n- op: derive_column\n  name: amount_clean\n  expr: NULLIF(amount, 0)\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id quantity unit_price discount 1 5 10.00 2.00 2 3 25.00 NULL 3 10 5.00 5.00 <p>Transforms:</p> <pre><code>- op: derive_column\n  name: subtotal\n  expr: quantity * unit_price\n\n- op: derive_column\n  name: total\n  expr: subtotal - COALESCE(discount, 0)\n</code></pre> <p>Output Data:</p> id quantity unit_price discount subtotal total 1 5 10.00 2.00 50.00 48.00 2 3 25.00 NULL 75.00 75.00 3 10 5.00 5.00 50.00 45.00"},{"location":"user-guide/transforms/derive-column/#expression-reference","title":"Expression Reference","text":""},{"location":"user-guide/transforms/derive-column/#arithmetic-operators","title":"Arithmetic Operators","text":"Operator Description Example <code>+</code> Addition <code>price + tax</code> <code>-</code> Subtraction <code>total - discount</code> <code>*</code> Multiplication <code>qty * price</code> <code>/</code> Division <code>total / count</code>"},{"location":"user-guide/transforms/derive-column/#functions","title":"Functions","text":""},{"location":"user-guide/transforms/derive-column/#string-functions","title":"String Functions","text":"Function Description Example <code>upper(s)</code> Uppercase <code>upper(name)</code> <code>lower(s)</code> Lowercase <code>lower(email)</code> <code>trim(s)</code> Remove whitespace <code>trim(name)</code> <code>concat(...)</code> Concatenate <code>concat(a, b, c)</code> <code>substring(s, start, len)</code> Extract substring <code>substring(phone, 1, 3)</code> <code>length(s)</code> String length <code>length(name)</code>"},{"location":"user-guide/transforms/derive-column/#date-functions","title":"Date Functions","text":"Function Description Example <code>extract(part from d)</code> Extract component <code>extract(year from date)</code> <code>date_trunc(part, d)</code> Truncate date <code>date_trunc('month', date)</code>"},{"location":"user-guide/transforms/derive-column/#null-functions","title":"Null Functions","text":"Function Description Example <code>COALESCE(a, b, ...)</code> First non-null <code>COALESCE(discount, 0)</code> <code>NULLIF(a, b)</code> Null if equal <code>NULLIF(amount, 0)</code>"},{"location":"user-guide/transforms/derive-column/#math-functions","title":"Math Functions","text":"Function Description Example <code>abs(n)</code> Absolute value <code>abs(difference)</code> <code>round(n, d)</code> Round <code>round(price, 2)</code> <code>floor(n)</code> Round down <code>floor(score)</code> <code>ceil(n)</code> Round up <code>ceil(rating)</code>"},{"location":"user-guide/transforms/derive-column/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import DeriveColumnTransform\n\ntransform = DeriveColumnTransform(\n    name=\"total_with_tax\",\n    expr=\"amount * 1.1\"\n)\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/derive-column/#calculate-metrics","title":"Calculate Metrics","text":"<pre><code>transforms:\n  - op: derive_column\n    name: revenue\n    expr: quantity * unit_price\n\n  - op: derive_column\n    name: profit\n    expr: revenue - cost\n\n  - op: derive_column\n    name: margin_pct\n    expr: profit / revenue * 100\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#categorize-data","title":"Categorize Data","text":"<pre><code>- op: derive_column\n  name: customer_tier\n  expr: |\n    CASE\n      WHEN lifetime_value &gt;= 10000 THEN 'platinum'\n      WHEN lifetime_value &gt;= 5000 THEN 'gold'\n      WHEN lifetime_value &gt;= 1000 THEN 'silver'\n      ELSE 'bronze'\n    END\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#clean-data","title":"Clean Data","text":"<pre><code>transforms:\n  - op: derive_column\n    name: email_clean\n    expr: lower(trim(email))\n\n  - op: derive_column\n    name: phone_digits\n    expr: regexp_replace(phone, '[^0-9]', '')\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#date-dimensions","title":"Date Dimensions","text":"<pre><code>transforms:\n  - op: derive_column\n    name: year\n    expr: extract(year from order_date)\n\n  - op: derive_column\n    name: quarter\n    expr: extract(quarter from order_date)\n\n  - op: derive_column\n    name: month\n    expr: extract(month from order_date)\n\n  - op: derive_column\n    name: day_of_week\n    expr: extract(dow from order_date)\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/derive-column/#column-chaining","title":"Column Chaining","text":"<p>Derived columns can reference previously derived columns:</p> <pre><code>transforms:\n  - op: derive_column\n    name: subtotal\n    expr: quantity * price\n\n  - op: derive_column\n    name: tax\n    expr: subtotal * 0.1     # References subtotal\n\n  - op: derive_column\n    name: total\n    expr: subtotal + tax     # References both\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#readable-expressions","title":"Readable Expressions","text":"<p>For complex expressions, use YAML multiline:</p> <pre><code>- op: derive_column\n  name: score\n  expr: |\n    CASE\n      WHEN rating &gt;= 4.5 THEN 'excellent'\n      WHEN rating &gt;= 3.5 THEN 'good'\n      WHEN rating &gt;= 2.5 THEN 'average'\n      ELSE 'poor'\n    END\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#handle-division-by-zero","title":"Handle Division by Zero","text":"<pre><code>- op: derive_column\n  name: rate\n  expr: CASE WHEN count &gt; 0 THEN total / count ELSE 0 END\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/derive-column/#syntax-error","title":"Syntax Error","text":"<pre><code>Error: Could not parse expression\n</code></pre> <p>Check expression syntax. Common issues:</p> <ul> <li>Missing quotes around strings</li> <li>Incorrect function syntax</li> <li>Unbalanced parentheses</li> </ul>"},{"location":"user-guide/transforms/derive-column/#type-mismatch","title":"Type Mismatch","text":"<pre><code>Error: Cannot multiply string and integer\n</code></pre> <p>Cast columns if needed:</p> <pre><code>- op: cast\n  columns:\n    quantity: int64\n\n- op: derive_column\n  name: total\n  expr: quantity * price\n</code></pre>"},{"location":"user-guide/transforms/derive-column/#related","title":"Related","text":"<ul> <li>cast - Convert types before calculations</li> <li>fill_null - Handle nulls before calculations</li> <li>Expression Language - Full expression reference</li> </ul>"},{"location":"user-guide/transforms/fill-null/","title":"fill_null","text":"<p>Replace null values with specified defaults.</p>"},{"location":"user-guide/transforms/fill-null/#usage","title":"Usage","text":"<pre><code>- op: fill_null\n  columns:\n    amount: 0\n    status: \"unknown\"\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>columns</code> Yes <code>dict[str, any]</code> Column \u2192 replacement value"},{"location":"user-guide/transforms/fill-null/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/fill-null/#numeric-default","title":"Numeric Default","text":"<pre><code>- op: fill_null\n  columns:\n    amount: 0\n    discount: 0.0\n    quantity: 1\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#string-default","title":"String Default","text":"<pre><code>- op: fill_null\n  columns:\n    status: \"unknown\"\n    category: \"uncategorized\"\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#multiple-types","title":"Multiple Types","text":"<pre><code>- op: fill_null\n  columns:\n    amount: 0\n    discount: 0.0\n    status: \"pending\"\n    notes: \"\"\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id amount status 1 100 active 2 NULL NULL 3 200 NULL <p>Transform:</p> <pre><code>- op: fill_null\n  columns:\n    amount: 0\n    status: \"unknown\"\n</code></pre> <p>Output Data:</p> id amount status 1 100 active 2 0 unknown 3 200 unknown"},{"location":"user-guide/transforms/fill-null/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import FillNullTransform\n\ntransform = FillNullTransform(\n    columns={\n        \"amount\": 0,\n        \"status\": \"unknown\",\n    }\n)\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/fill-null/#prepare-for-calculations","title":"Prepare for Calculations","text":"<pre><code>transforms:\n  # Fill nulls before arithmetic\n  - op: fill_null\n    columns:\n      quantity: 1\n      discount: 0\n\n  - op: derive_column\n    name: total\n    expr: (quantity * price) - discount\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#clean-for-aggregation","title":"Clean for Aggregation","text":"<pre><code>transforms:\n  - op: fill_null\n    columns:\n      amount: 0\n\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total: sum(amount)  # Nulls would be ignored\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#default-categories","title":"Default Categories","text":"<pre><code>- op: fill_null\n  columns:\n    region: \"Other\"\n    category: \"Uncategorized\"\n    tier: \"Standard\"\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#boolean-defaults","title":"Boolean Defaults","text":"<pre><code>- op: fill_null\n  columns:\n    is_active: true\n    is_verified: false\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/fill-null/#type-matching","title":"Type Matching","text":"<p>Fill values should match column types:</p> <pre><code># If amount is integer\n- op: fill_null\n  columns:\n    amount: 0     # Integer, not \"0\" or 0.0\n\n# If amount is float\n- op: fill_null\n  columns:\n    amount: 0.0   # Float\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#alternative-coalesce-in-derive_column","title":"Alternative: COALESCE in derive_column","text":"<p>For more complex defaults:</p> <pre><code>- op: derive_column\n  name: amount_clean\n  expr: COALESCE(amount, default_amount, 0)\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#fill-all-string-columns","title":"Fill All String Columns","text":"<pre><code>- op: fill_null\n  columns:\n    name: \"\"\n    email: \"\"\n    phone: \"\"\n    address: \"\"\n</code></pre>"},{"location":"user-guide/transforms/fill-null/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/fill-null/#type-mismatch","title":"Type Mismatch","text":"<pre><code>Error: Cannot fill integer column with string value\n</code></pre> <p>Ensure fill value type matches column type.</p>"},{"location":"user-guide/transforms/fill-null/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Verify the column exists.</p>"},{"location":"user-guide/transforms/fill-null/#related","title":"Related","text":"<ul> <li>derive_column - Use COALESCE for conditional filling</li> <li>cast - Convert types after filling</li> <li>not_null check - Verify no nulls remain</li> </ul>"},{"location":"user-guide/transforms/filter/","title":"filter","text":"<p>Filter rows based on a SQL-like predicate.</p>"},{"location":"user-guide/transforms/filter/#usage","title":"Usage","text":"<pre><code>- op: filter\n  predicate: amount &gt; 100\n</code></pre>"},{"location":"user-guide/transforms/filter/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>predicate</code> Yes <code>str</code> SQL-like boolean expression"},{"location":"user-guide/transforms/filter/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/filter/#simple-comparison","title":"Simple Comparison","text":"<pre><code>- op: filter\n  predicate: amount &gt; 100\n</code></pre>"},{"location":"user-guide/transforms/filter/#equality","title":"Equality","text":"<pre><code>- op: filter\n  predicate: status = 'active'\n</code></pre>"},{"location":"user-guide/transforms/filter/#multiple-conditions","title":"Multiple Conditions","text":"<pre><code># AND\n- op: filter\n  predicate: amount &gt; 100 AND status = 'active'\n\n# OR\n- op: filter\n  predicate: region = 'north' OR region = 'south'\n</code></pre>"},{"location":"user-guide/transforms/filter/#null-handling","title":"Null Handling","text":"<pre><code># Keep non-null values\n- op: filter\n  predicate: email IS NOT NULL\n\n# Keep null values\n- op: filter\n  predicate: discount IS NULL\n</code></pre>"},{"location":"user-guide/transforms/filter/#date-filtering","title":"Date Filtering","text":"<pre><code>- op: filter\n  predicate: created_at &gt;= '2025-01-01'\n\n- op: filter\n  predicate: created_at &gt;= '${START_DATE}' AND created_at &lt; '${END_DATE}'\n</code></pre>"},{"location":"user-guide/transforms/filter/#in-operator","title":"IN Operator","text":"<pre><code>- op: filter\n  predicate: category IN ('Electronics', 'Home', 'Office')\n</code></pre>"},{"location":"user-guide/transforms/filter/#string-matching","title":"String Matching","text":"<pre><code># Starts with\n- op: filter\n  predicate: name LIKE 'Widget%'\n\n# Contains\n- op: filter\n  predicate: description LIKE '%sale%'\n</code></pre>"},{"location":"user-guide/transforms/filter/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id name amount status 1 Widget A 50 active 2 Widget B 150 active 3 Widget C 200 inactive 4 Widget D 75 active <p>Transform:</p> <pre><code>- op: filter\n  predicate: amount &gt; 100 AND status = 'active'\n</code></pre> <p>Output Data:</p> id name amount status 2 Widget B 150 active"},{"location":"user-guide/transforms/filter/#operators-reference","title":"Operators Reference","text":""},{"location":"user-guide/transforms/filter/#comparison","title":"Comparison","text":"Operator Description Example <code>=</code> Equal <code>status = 'active'</code> <code>!=</code> or <code>&lt;&gt;</code> Not equal <code>status != 'cancelled'</code> <code>&gt;</code> Greater than <code>amount &gt; 100</code> <code>&lt;</code> Less than <code>amount &lt; 1000</code> <code>&gt;=</code> Greater or equal <code>amount &gt;= 100</code> <code>&lt;=</code> Less or equal <code>amount &lt;= 1000</code>"},{"location":"user-guide/transforms/filter/#logical","title":"Logical","text":"Operator Description Example <code>AND</code> Both conditions <code>a &gt; 1 AND b &lt; 10</code> <code>OR</code> Either condition <code>a = 1 OR b = 2</code> <code>NOT</code> Negate <code>NOT status = 'cancelled'</code>"},{"location":"user-guide/transforms/filter/#null","title":"Null","text":"Operator Description Example <code>IS NULL</code> Is null <code>discount IS NULL</code> <code>IS NOT NULL</code> Is not null <code>email IS NOT NULL</code>"},{"location":"user-guide/transforms/filter/#other","title":"Other","text":"Operator Description Example <code>IN</code> In list <code>region IN ('north', 'south')</code> <code>NOT IN</code> Not in list <code>status NOT IN ('cancelled', 'refunded')</code> <code>BETWEEN</code> In range <code>amount BETWEEN 100 AND 500</code> <code>LIKE</code> Pattern match <code>name LIKE 'Widget%'</code>"},{"location":"user-guide/transforms/filter/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import FilterTransform\n\n# Simple\ntransform = FilterTransform(predicate=\"amount &gt; 100\")\n\n# Complex\ntransform = FilterTransform(\n    predicate=\"amount &gt; 100 AND status = 'active' AND region IN ('north', 'south')\"\n)\n</code></pre>"},{"location":"user-guide/transforms/filter/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/filter/#clean-invalid-data","title":"Clean Invalid Data","text":"<pre><code>transforms:\n  - op: filter\n    predicate: amount &gt; 0 AND amount IS NOT NULL\n\n  - op: filter\n    predicate: customer_id IS NOT NULL\n</code></pre>"},{"location":"user-guide/transforms/filter/#date-range","title":"Date Range","text":"<pre><code>- op: filter\n  predicate: |\n    created_at &gt;= '${START_DATE}'\n    AND created_at &lt; '${END_DATE}'\n</code></pre>"},{"location":"user-guide/transforms/filter/#exclude-test-data","title":"Exclude Test Data","text":"<pre><code>- op: filter\n  predicate: email NOT LIKE '%@test.com' AND name NOT LIKE 'Test %'\n</code></pre>"},{"location":"user-guide/transforms/filter/#active-records-only","title":"Active Records Only","text":"<pre><code>- op: filter\n  predicate: status = 'active' AND deleted_at IS NULL\n</code></pre>"},{"location":"user-guide/transforms/filter/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/transforms/filter/#filter-early","title":"Filter Early","text":"<p>Apply filters as early as possible:</p> <pre><code>transforms:\n  # Good: filter first, reduces data for subsequent operations\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n\n  - op: derive_column\n    name: complex_metric\n    expr: expensive_calculation\n</code></pre>"},{"location":"user-guide/transforms/filter/#use-database-filters","title":"Use Database Filters","text":"<p>For database sources, filter in the query:</p> <pre><code># Better: filter in database\nsource:\n  type: database\n  query: SELECT * FROM sales WHERE date &gt;= '2025-01-01'\n\n# Less efficient: filter after loading\nsource:\n  type: database\n  table: sales\ntransforms:\n  - op: filter\n    predicate: date &gt;= '2025-01-01'\n</code></pre>"},{"location":"user-guide/transforms/filter/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/filter/#quoting-strings","title":"Quoting Strings","text":"<p>Use single quotes for string values:</p> <pre><code>predicate: status = 'active'   # Correct\npredicate: status = \"active\"   # May not work\n</code></pre>"},{"location":"user-guide/transforms/filter/#escaping-quotes","title":"Escaping Quotes","text":"<p>For values containing quotes:</p> <pre><code>predicate: name = 'O''Brien'   # Single quote escape\n</code></pre>"},{"location":"user-guide/transforms/filter/#complex-predicates","title":"Complex Predicates","text":"<p>Use parentheses for clarity:</p> <pre><code>predicate: (region = 'north' OR region = 'south') AND amount &gt; 100\n</code></pre>"},{"location":"user-guide/transforms/filter/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/filter/#syntax-error","title":"Syntax Error","text":"<pre><code>Error: Could not parse predicate\n</code></pre> <p>Check your predicate syntax. Common issues:</p> <ul> <li>Missing quotes around strings</li> <li>Typo in column name</li> <li>Invalid operator</li> </ul>"},{"location":"user-guide/transforms/filter/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Verify the column exists in your data.</p>"},{"location":"user-guide/transforms/filter/#related","title":"Related","text":"<ul> <li>derive_column - Create columns for filtering</li> <li>dedup - Another way to reduce rows</li> <li>Expression Language - Full expression syntax</li> </ul>"},{"location":"user-guide/transforms/join/","title":"join","text":"<p>Join two datasets on one or more columns.</p>"},{"location":"user-guide/transforms/join/#usage","title":"Usage","text":"<pre><code>- op: join\n  right: customers\n  on: [customer_id]\n  how: left\n</code></pre>"},{"location":"user-guide/transforms/join/#parameters","title":"Parameters","text":"Parameter Required Default Description <code>right</code> Yes - Reference to right dataset <code>on</code> Yes - Join key columns <code>how</code> No <code>inner</code> Join type: <code>inner</code>, <code>left</code>, <code>right</code>, <code>outer</code>"},{"location":"user-guide/transforms/join/#join-types","title":"Join Types","text":"Type Description <code>inner</code> Only matching rows from both sides <code>left</code> All rows from left, matching from right <code>right</code> All rows from right, matching from left <code>outer</code> All rows from both sides"},{"location":"user-guide/transforms/join/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/join/#inner-join","title":"Inner Join","text":"<pre><code>- op: join\n  right: customers\n  on: [customer_id]\n  how: inner\n</code></pre>"},{"location":"user-guide/transforms/join/#left-join","title":"Left Join","text":"<pre><code>- op: join\n  right: customers\n  on: [customer_id]\n  how: left\n</code></pre>"},{"location":"user-guide/transforms/join/#multiple-join-keys","title":"Multiple Join Keys","text":"<pre><code>- op: join\n  right: products\n  on: [product_id, region]\n  how: left\n</code></pre>"},{"location":"user-guide/transforms/join/#inputoutput-example","title":"Input/Output Example","text":"<p>Orders (left):</p> order_id customer_id amount 1 C001 100 2 C002 200 3 C003 150 <p>Customers (right):</p> customer_id name tier C001 Acme Gold C002 Globex Silver <p>Left Join:</p> <pre><code>- op: join\n  right: customers\n  on: [customer_id]\n  how: left\n</code></pre> <p>Output:</p> order_id customer_id amount name tier 1 C001 100 Acme Gold 2 C002 200 Globex Silver 3 C003 150 NULL NULL"},{"location":"user-guide/transforms/join/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import JoinTransform\n\ntransform = JoinTransform(\n    right=\"customers\",\n    on=[\"customer_id\"],\n    how=\"left\"\n)\n</code></pre>"},{"location":"user-guide/transforms/join/#multi-source-pipeline","title":"Multi-Source Pipeline","text":"<p>For joins, you need multiple sources. Use Python API:</p> <pre><code>from etlx import Pipeline, ETLXEngine\nfrom etlx.config.models import FileSource\n\nengine = ETLXEngine(backend=\"duckdb\")\n\n# Read both datasets\norders = engine.read_file(\"orders.parquet\", \"parquet\")\ncustomers = engine.read_file(\"customers.parquet\", \"parquet\")\n\n# Join\nresult = engine.join(\n    left=orders,\n    right=customers,\n    on=[\"customer_id\"],\n    how=\"left\"\n)\n</code></pre>"},{"location":"user-guide/transforms/join/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/join/#enrich-with-dimensions","title":"Enrich with Dimensions","text":"<pre><code>transforms:\n  - op: join\n    right: customers\n    on: [customer_id]\n    how: left\n\n  - op: join\n    right: products\n    on: [product_id]\n    how: left\n</code></pre>"},{"location":"user-guide/transforms/join/#validate-referential-integrity","title":"Validate Referential Integrity","text":"<p>Use inner join to find orphans:</p> <pre><code># Find orders without matching customers\ninner_count = engine.row_count(\n    engine.join(orders, customers, [\"customer_id\"], \"inner\")\n)\nleft_count = engine.row_count(orders)\n\norphans = left_count - inner_count\n</code></pre>"},{"location":"user-guide/transforms/join/#star-schema-join","title":"Star Schema Join","text":"<pre><code># Join fact table with dimensions\ntransforms:\n  - op: join\n    right: dim_customer\n    on: [customer_id]\n    how: left\n\n  - op: join\n    right: dim_product\n    on: [product_id]\n    how: left\n\n  - op: join\n    right: dim_date\n    on: [date_id]\n    how: left\n</code></pre>"},{"location":"user-guide/transforms/join/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/join/#column-naming","title":"Column Naming","text":"<p>After join, columns from both tables are included. Watch for name conflicts:</p> <pre><code>transforms:\n  - op: join\n    right: customers\n    on: [customer_id]\n\n  # Rename to avoid confusion\n  - op: rename\n    mapping:\n      name: customer_name\n</code></pre>"},{"location":"user-guide/transforms/join/#join-key-types","title":"Join Key Types","text":"<p>Ensure join keys have matching types:</p> <pre><code>transforms:\n  # Cast to match types\n  - op: cast\n    columns:\n      customer_id: string\n\n  - op: join\n    right: customers\n    on: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/join/#performance","title":"Performance","text":"<ul> <li>Join on indexed columns when possible</li> <li>Filter before joining to reduce data</li> <li>Consider join order (smaller table as right)</li> </ul>"},{"location":"user-guide/transforms/join/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/join/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Join column 'cust_id' not found\n</code></pre> <p>Verify the join column exists in both datasets.</p>"},{"location":"user-guide/transforms/join/#type-mismatch","title":"Type Mismatch","text":"<pre><code>Error: Cannot join on columns with different types\n</code></pre> <p>Cast columns to matching types before joining.</p>"},{"location":"user-guide/transforms/join/#related","title":"Related","text":"<ul> <li>cast - Match types for join</li> <li>rename - Rename after join</li> <li>select - Select columns after join</li> </ul>"},{"location":"user-guide/transforms/limit/","title":"limit","text":"<p>Limit output to the first N rows.</p>"},{"location":"user-guide/transforms/limit/#usage","title":"Usage","text":"<pre><code>- op: limit\n  n: 1000\n</code></pre>"},{"location":"user-guide/transforms/limit/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>n</code> Yes <code>int</code> Maximum number of rows (must be &gt; 0)"},{"location":"user-guide/transforms/limit/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/limit/#basic-limit","title":"Basic Limit","text":"<pre><code>- op: limit\n  n: 100\n</code></pre>"},{"location":"user-guide/transforms/limit/#top-n-pattern","title":"Top N Pattern","text":"<pre><code>transforms:\n  - op: sort\n    by: [sales]\n    descending: true\n\n  - op: limit\n    n: 10\n</code></pre>"},{"location":"user-guide/transforms/limit/#sample-data","title":"Sample Data","text":"<pre><code># Get sample for testing\n- op: limit\n  n: 1000\n</code></pre>"},{"location":"user-guide/transforms/limit/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data (1000 rows):</p> id name amount 1 Widget A 500 2 Widget B 400 3 Widget C 300 ... ... ... <p>Transform:</p> <pre><code>- op: limit\n  n: 3\n</code></pre> <p>Output Data:</p> id name amount 1 Widget A 500 2 Widget B 400 3 Widget C 300"},{"location":"user-guide/transforms/limit/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import LimitTransform\n\ntransform = LimitTransform(n=1000)\n</code></pre>"},{"location":"user-guide/transforms/limit/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/limit/#top-n-by-value","title":"Top N by Value","text":"<pre><code>transforms:\n  - op: sort\n    by: [revenue]\n    descending: true\n\n  - op: limit\n    n: 10\n</code></pre>"},{"location":"user-guide/transforms/limit/#top-n-per-group","title":"Top N per Group","text":"<p>For top N per group, use Python:</p> <pre><code># Top 5 products per category\nresult = (\n    engine.sort(data, [\"amount\"], descending=True)\n    .over(group_by=[\"category\"])\n    .head(5)\n)\n</code></pre>"},{"location":"user-guide/transforms/limit/#development-sampling","title":"Development Sampling","text":"<pre><code># Limit data for faster development\ntransforms:\n  - op: limit\n    n: ${SAMPLE_SIZE:-10000}\n</code></pre> <pre><code># Development\netlx run pipeline.yml --var SAMPLE_SIZE=1000\n\n# Production\netlx run pipeline.yml --var SAMPLE_SIZE=1000000\n</code></pre>"},{"location":"user-guide/transforms/limit/#preview-data","title":"Preview Data","text":"<pre><code>transforms:\n  # Quick preview\n  - op: limit\n    n: 5\n</code></pre>"},{"location":"user-guide/transforms/limit/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/limit/#order-matters","title":"Order Matters","text":"<p>Without sorting, limit returns arbitrary rows:</p> <pre><code># Arbitrary 10 rows\n- op: limit\n  n: 10\n\n# Definite top 10 by amount\ntransforms:\n  - op: sort\n    by: [amount]\n    descending: true\n  - op: limit\n    n: 10\n</code></pre>"},{"location":"user-guide/transforms/limit/#use-for-testing","title":"Use for Testing","text":"<p>Limit data during development:</p> <pre><code>transforms:\n  # ... your transforms ...\n\n  # Remove in production\n  - op: limit\n    n: 100\n</code></pre>"},{"location":"user-guide/transforms/limit/#memory-management","title":"Memory Management","text":"<p>For very large datasets, limit early:</p> <pre><code>transforms:\n  - op: limit\n    n: 1000000  # Process at most 1M rows\n\n  # ... expensive transforms ...\n</code></pre>"},{"location":"user-guide/transforms/limit/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/limit/#invalid-n","title":"Invalid N","text":"<pre><code>Error: 'n' must be greater than 0\n</code></pre> <p>Provide a positive integer.</p>"},{"location":"user-guide/transforms/limit/#related","title":"Related","text":"<ul> <li>sort - Sort before limit for Top N</li> <li>filter - Filter instead of arbitrary limit</li> </ul>"},{"location":"user-guide/transforms/rename/","title":"rename","text":"<p>Rename columns using a mapping.</p>"},{"location":"user-guide/transforms/rename/#usage","title":"Usage","text":"<pre><code>- op: rename\n  mapping:\n    old_name: new_name\n</code></pre>"},{"location":"user-guide/transforms/rename/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>mapping</code> Yes <code>dict[str, str]</code> Old name \u2192 new name mapping"},{"location":"user-guide/transforms/rename/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/rename/#single-column","title":"Single Column","text":"<pre><code>- op: rename\n  mapping:\n    cust_id: customer_id\n</code></pre>"},{"location":"user-guide/transforms/rename/#multiple-columns","title":"Multiple Columns","text":"<pre><code>- op: rename\n  mapping:\n    cust_id: customer_id\n    order_amt: amount\n    created: created_at\n</code></pre>"},{"location":"user-guide/transforms/rename/#standardize-naming","title":"Standardize Naming","text":"<pre><code>- op: rename\n  mapping:\n    # Database columns to snake_case\n    CustomerID: customer_id\n    OrderAmount: order_amount\n    CreatedAt: created_at\n</code></pre>"},{"location":"user-guide/transforms/rename/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> cust_id order_amt dt C001 99.99 2025-01-15 <p>Transform:</p> <pre><code>- op: rename\n  mapping:\n    cust_id: customer_id\n    order_amt: amount\n    dt: order_date\n</code></pre> <p>Output Data:</p> customer_id amount order_date C001 99.99 2025-01-15"},{"location":"user-guide/transforms/rename/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import RenameTransform\n\ntransform = RenameTransform(\n    mapping={\n        \"cust_id\": \"customer_id\",\n        \"order_amt\": \"amount\",\n    }\n)\n</code></pre>"},{"location":"user-guide/transforms/rename/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/rename/#clean-source-column-names","title":"Clean Source Column Names","text":"<pre><code># From external API with messy names\n- op: rename\n  mapping:\n    \"Customer ID\": customer_id\n    \"Order Amount (USD)\": amount_usd\n    \"Date Created\": created_at\n</code></pre>"},{"location":"user-guide/transforms/rename/#prepare-for-join","title":"Prepare for Join","text":"<p>Rename to match join keys:</p> <pre><code># Orders table\n- op: rename\n  mapping:\n    cid: customer_id\n\n# Now can join with customers on customer_id\n</code></pre>"},{"location":"user-guide/transforms/rename/#add-prefixes-for-joins","title":"Add Prefixes for Joins","text":"<pre><code># After join, disambiguate columns\n- op: rename\n  mapping:\n    name: customer_name\n    amount: order_amount\n</code></pre>"},{"location":"user-guide/transforms/rename/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/rename/#order-doesnt-matter","title":"Order Doesn't Matter","text":"<p>Renames are applied simultaneously:</p> <pre><code># This works (swap names)\n- op: rename\n  mapping:\n    a: b\n    b: a\n</code></pre>"},{"location":"user-guide/transforms/rename/#combine-with-select","title":"Combine with Select","text":"<p>Use select to keep only renamed columns:</p> <pre><code>transforms:\n  - op: rename\n    mapping:\n      cust_id: customer_id\n      order_amt: amount\n\n  - op: select\n    columns: [customer_id, amount]\n</code></pre>"},{"location":"user-guide/transforms/rename/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/rename/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Check that the source column exists.</p>"},{"location":"user-guide/transforms/rename/#duplicate-target-names","title":"Duplicate Target Names","text":"<pre><code>Error: Duplicate column name 'amount'\n</code></pre> <p>Ensure all target names are unique.</p>"},{"location":"user-guide/transforms/rename/#related","title":"Related","text":"<ul> <li>select - Reorder and filter columns</li> <li>derive_column - Add new columns</li> </ul>"},{"location":"user-guide/transforms/select/","title":"select","text":"<p>Choose and reorder columns in your data.</p>"},{"location":"user-guide/transforms/select/#usage","title":"Usage","text":"<pre><code>- op: select\n  columns: [id, name, amount]\n</code></pre>"},{"location":"user-guide/transforms/select/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>columns</code> Yes <code>list[str]</code> Columns to keep, in order"},{"location":"user-guide/transforms/select/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/select/#basic-selection","title":"Basic Selection","text":"<p>Keep only specific columns:</p> <pre><code># Input: id, name, email, amount, created_at, updated_at\n- op: select\n  columns: [id, name, amount]\n# Output: id, name, amount\n</code></pre>"},{"location":"user-guide/transforms/select/#reorder-columns","title":"Reorder Columns","text":"<p>Change column order:</p> <pre><code># Input: amount, id, name\n- op: select\n  columns: [id, name, amount]\n# Output: id, name, amount\n</code></pre>"},{"location":"user-guide/transforms/select/#select-after-transforms","title":"Select After Transforms","text":"<p>Select final output columns:</p> <pre><code>transforms:\n  - op: derive_column\n    name: total_with_tax\n    expr: amount * 1.1\n\n  - op: derive_column\n    name: profit\n    expr: amount - cost\n\n  - op: select\n    columns: [id, amount, total_with_tax, profit]\n</code></pre>"},{"location":"user-guide/transforms/select/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id name email amount status 1 Widget A a@example.com 99.99 active 2 Widget B b@example.com 149.99 active <p>Transform:</p> <pre><code>- op: select\n  columns: [id, name, amount]\n</code></pre> <p>Output Data:</p> id name amount 1 Widget A 99.99 2 Widget B 149.99"},{"location":"user-guide/transforms/select/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import SelectTransform\n\ntransform = SelectTransform(columns=[\"id\", \"name\", \"amount\"])\n</code></pre>"},{"location":"user-guide/transforms/select/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/select/#remove-sensitive-columns","title":"Remove Sensitive Columns","text":"<pre><code># Remove PII before output\n- op: select\n  columns: [id, product, amount, date]\n  # Excludes: email, phone, address\n</code></pre>"},{"location":"user-guide/transforms/select/#prepare-for-join","title":"Prepare for Join","text":"<p>Select only columns needed for join:</p> <pre><code>transforms:\n  - op: select\n    columns: [order_id, customer_id, amount]\n\n  - op: join\n    right: customers\n    on: [customer_id]\n</code></pre>"},{"location":"user-guide/transforms/select/#final-output-schema","title":"Final Output Schema","text":"<p>Ensure consistent output:</p> <pre><code>transforms:\n  # ... other transforms ...\n\n  # Last transform: define output schema\n  - op: select\n    columns: [region, category, total_sales, order_count, avg_order_value]\n</code></pre>"},{"location":"user-guide/transforms/select/#performance","title":"Performance","text":"<p><code>select</code> is very efficient:</p> <ul> <li>Columnar formats (Parquet) only read selected columns</li> <li>Reduces memory usage</li> <li>Apply early in pipeline for best performance</li> </ul> <pre><code>transforms:\n  # Good: select early, reduces data for subsequent transforms\n  - op: select\n    columns: [id, amount, category]\n  - op: filter\n    predicate: amount &gt; 100\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total: sum(amount)\n</code></pre>"},{"location":"user-guide/transforms/select/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/select/#use-with-parquet-sources","title":"Use with Parquet Sources","text":"<p>With Parquet files, unselected columns aren't read from disk:</p> <pre><code>source:\n  type: file\n  path: large_file.parquet  # 100 columns\n\ntransforms:\n  - op: select\n    columns: [id, amount]   # Only reads 2 columns\n</code></pre>"},{"location":"user-guide/transforms/select/#column-order-matters","title":"Column Order Matters","text":"<p>Columns appear in the order specified:</p> <pre><code>- op: select\n  columns: [z_col, a_col, m_col]\n# Output columns: z_col, a_col, m_col\n</code></pre>"},{"location":"user-guide/transforms/select/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/select/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found in table\n</code></pre> <p>Check that all columns in <code>columns</code> exist in the input data.</p>"},{"location":"user-guide/transforms/select/#empty-column-list","title":"Empty Column List","text":"<pre><code>Error: 'columns' must not be empty\n</code></pre> <p>Provide at least one column.</p>"},{"location":"user-guide/transforms/select/#related","title":"Related","text":"<ul> <li>rename - Rename columns</li> <li>derive_column - Add new columns</li> <li>aggregate - Select is often used before aggregation</li> </ul>"},{"location":"user-guide/transforms/sort/","title":"sort","text":"<p>Order rows by one or more columns.</p>"},{"location":"user-guide/transforms/sort/#usage","title":"Usage","text":"<pre><code>- op: sort\n  by: [amount]\n  descending: true\n</code></pre>"},{"location":"user-guide/transforms/sort/#parameters","title":"Parameters","text":"Parameter Required Default Description <code>by</code> Yes - Columns to sort by <code>descending</code> No <code>false</code> Sort in descending order"},{"location":"user-guide/transforms/sort/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/sort/#single-column-ascending","title":"Single Column Ascending","text":"<pre><code>- op: sort\n  by: [name]\n</code></pre>"},{"location":"user-guide/transforms/sort/#single-column-descending","title":"Single Column Descending","text":"<pre><code>- op: sort\n  by: [amount]\n  descending: true\n</code></pre>"},{"location":"user-guide/transforms/sort/#multiple-columns","title":"Multiple Columns","text":"<pre><code>- op: sort\n  by: [category, amount]\n  descending: true\n</code></pre>"},{"location":"user-guide/transforms/sort/#inputoutput-example","title":"Input/Output Example","text":"<p>Input Data:</p> id name amount 1 Widget C 50 2 Widget A 150 3 Widget B 100 <p>Transform (ascending):</p> <pre><code>- op: sort\n  by: [amount]\n</code></pre> <p>Output:</p> id name amount 1 Widget C 50 3 Widget B 100 2 Widget A 150 <p>Transform (descending):</p> <pre><code>- op: sort\n  by: [amount]\n  descending: true\n</code></pre> <p>Output:</p> id name amount 2 Widget A 150 3 Widget B 100 1 Widget C 50"},{"location":"user-guide/transforms/sort/#python-api","title":"Python API","text":"<pre><code>from etlx.config.transforms import SortTransform\n\n# Ascending\ntransform = SortTransform(by=[\"amount\"])\n\n# Descending\ntransform = SortTransform(by=[\"amount\"], descending=True)\n\n# Multiple columns\ntransform = SortTransform(by=[\"category\", \"amount\"], descending=True)\n</code></pre>"},{"location":"user-guide/transforms/sort/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/sort/#top-n-pattern","title":"Top N Pattern","text":"<pre><code>transforms:\n  - op: sort\n    by: [sales]\n    descending: true\n\n  - op: limit\n    n: 10\n</code></pre>"},{"location":"user-guide/transforms/sort/#sort-after-aggregation","title":"Sort After Aggregation","text":"<pre><code>transforms:\n  - op: aggregate\n    group_by: [category]\n    aggs:\n      total: sum(amount)\n\n  - op: sort\n    by: [total]\n    descending: true\n</code></pre>"},{"location":"user-guide/transforms/sort/#sort-for-reporting","title":"Sort for Reporting","text":"<pre><code>transforms:\n  # Sort by region, then by sales within region\n  - op: sort\n    by: [region, total_sales]\n    descending: true\n</code></pre>"},{"location":"user-guide/transforms/sort/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/sort/#nulls","title":"Nulls","text":"<p>Null values typically sort last (ascending) or first (descending).</p>"},{"location":"user-guide/transforms/sort/#performance","title":"Performance","text":"<p>Sorting large datasets is expensive. Consider:</p> <ul> <li>Sort after filtering to reduce rows</li> <li>Sort after aggregation when possible</li> <li>May not be needed if output order doesn't matter</li> </ul>"},{"location":"user-guide/transforms/sort/#multiple-sort-directions","title":"Multiple Sort Directions","text":"<p>Currently, all columns use the same direction. For mixed directions, use multiple sorts or SQL in the source query.</p>"},{"location":"user-guide/transforms/sort/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/sort/#column-not-found","title":"Column Not Found","text":"<pre><code>Error: Column 'nonexistent' not found\n</code></pre> <p>Verify the column exists.</p>"},{"location":"user-guide/transforms/sort/#related","title":"Related","text":"<ul> <li>limit - Often used after sort for Top N</li> <li>aggregate - Sort aggregation results</li> </ul>"},{"location":"user-guide/transforms/union/","title":"union","text":"<p>Vertically combine multiple datasets.</p>"},{"location":"user-guide/transforms/union/#usage","title":"Usage","text":"<pre><code>- op: union\n  sources: [data1, data2]\n</code></pre>"},{"location":"user-guide/transforms/union/#parameters","title":"Parameters","text":"Parameter Required Type Description <code>sources</code> Yes <code>list[str]</code> References to datasets to combine"},{"location":"user-guide/transforms/union/#examples","title":"Examples","text":""},{"location":"user-guide/transforms/union/#combine-two-datasets","title":"Combine Two Datasets","text":"<pre><code>- op: union\n  sources: [north_sales, south_sales]\n</code></pre>"},{"location":"user-guide/transforms/union/#combine-multiple-datasets","title":"Combine Multiple Datasets","text":"<pre><code>- op: union\n  sources: [q1_data, q2_data, q3_data, q4_data]\n</code></pre>"},{"location":"user-guide/transforms/union/#inputoutput-example","title":"Input/Output Example","text":"<p>Dataset 1 (north_sales):</p> id region amount 1 North 100 2 North 200 <p>Dataset 2 (south_sales):</p> id region amount 3 South 150 4 South 250 <p>Transform:</p> <pre><code>- op: union\n  sources: [north_sales, south_sales]\n</code></pre> <p>Output:</p> id region amount 1 North 100 2 North 200 3 South 150 4 South 250"},{"location":"user-guide/transforms/union/#python-api","title":"Python API","text":"<pre><code>from etlx import ETLXEngine\n\nengine = ETLXEngine(backend=\"duckdb\")\n\n# Read datasets\nnorth = engine.read_file(\"north_sales.parquet\", \"parquet\")\nsouth = engine.read_file(\"south_sales.parquet\", \"parquet\")\n\n# Union\ncombined = engine.union([north, south])\n</code></pre>"},{"location":"user-guide/transforms/union/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/transforms/union/#combine-daily-files","title":"Combine Daily Files","text":"<pre><code>engine = ETLXEngine(backend=\"duckdb\")\n\n# Read daily files\nfiles = [\n    engine.read_file(f\"data/{date}/sales.parquet\", \"parquet\")\n    for date in [\"2025-01-01\", \"2025-01-02\", \"2025-01-03\"]\n]\n\n# Combine\ncombined = engine.union(files)\n</code></pre>"},{"location":"user-guide/transforms/union/#combine-regional-data","title":"Combine Regional Data","text":"<pre><code># Read regional files\nregions = [\"north\", \"south\", \"east\", \"west\"]\ndatasets = [\n    engine.read_file(f\"data/{region}/sales.parquet\", \"parquet\")\n    for region in regions\n]\n\ncombined = engine.union(datasets)\n</code></pre>"},{"location":"user-guide/transforms/union/#add-source-column","title":"Add Source Column","text":"<p>Track which dataset each row came from:</p> <pre><code>north = engine.read_file(\"north.parquet\", \"parquet\")\nsouth = engine.read_file(\"south.parquet\", \"parquet\")\n\n# Add source identifier\nnorth = engine.derive_column(north, \"source\", \"'north'\")\nsouth = engine.derive_column(south, \"source\", \"'south'\")\n\ncombined = engine.union([north, south])\n</code></pre>"},{"location":"user-guide/transforms/union/#requirements","title":"Requirements","text":""},{"location":"user-guide/transforms/union/#matching-schemas","title":"Matching Schemas","text":"<p>All datasets must have the same columns:</p> <pre><code># Both must have: id, name, amount\nnorth = engine.read_file(\"north.parquet\", \"parquet\")  # id, name, amount\nsouth = engine.read_file(\"south.parquet\", \"parquet\")  # id, name, amount\n\ncombined = engine.union([north, south])  # Works\n</code></pre>"},{"location":"user-guide/transforms/union/#column-order","title":"Column Order","text":"<p>Columns should be in the same order. Use <code>select</code> to align:</p> <pre><code># Align columns before union\nnorth = engine.select(north, [\"id\", \"name\", \"amount\"])\nsouth = engine.select(south, [\"id\", \"name\", \"amount\"])\n\ncombined = engine.union([north, south])\n</code></pre>"},{"location":"user-guide/transforms/union/#tips","title":"Tips","text":""},{"location":"user-guide/transforms/union/#dedup-after-union","title":"Dedup After Union","text":"<p>Remove duplicates that might exist across sources:</p> <pre><code>combined = engine.union([data1, data2])\ndeduped = engine.dedup(combined, [\"id\"])\n</code></pre>"},{"location":"user-guide/transforms/union/#filter-after-union","title":"Filter After Union","text":"<p>Apply consistent filters:</p> <pre><code>combined = engine.union([q1, q2, q3, q4])\nfiltered = engine.filter(combined, \"amount &gt; 0\")\n</code></pre>"},{"location":"user-guide/transforms/union/#add-metadata","title":"Add Metadata","text":"<p>Track source information:</p> <pre><code>transforms:\n  # After union\n  - op: derive_column\n    name: loaded_at\n    expr: current_timestamp()\n</code></pre>"},{"location":"user-guide/transforms/union/#errors","title":"Errors","text":""},{"location":"user-guide/transforms/union/#schema-mismatch","title":"Schema Mismatch","text":"<pre><code>Error: Cannot union tables with different schemas\n</code></pre> <p>Ensure all datasets have identical column names and types.</p>"},{"location":"user-guide/transforms/union/#empty-sources","title":"Empty Sources","text":"<pre><code>Error: Union requires at least two datasets\n</code></pre> <p>Provide at least two datasets to combine.</p>"},{"location":"user-guide/transforms/union/#related","title":"Related","text":"<ul> <li>dedup - Remove duplicates after union</li> <li>select - Align columns before union</li> <li>derive_column - Add source tracking</li> </ul>"}]}